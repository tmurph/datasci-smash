{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "plt.rcParams['figure.figsize'] = [9, 8]\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 74)\n",
    "pd.set_option('display.max_columns', 15)\n",
    "pd.set_option('display.max_rows', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "DATADIR = \"data/keras\"\n",
    "NUM_CHARACTERS = len(os.listdir(os.path.join(DATADIR, \"train\", \"images\")))\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "IMAGE_ROW_SIZE = 584\n",
    "IMAGE_COLUMN_SIZE = 480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "def make_generator(folder=\"train\",\n",
    "                   data_gen_args={\"fill_mode\": \"constant\",\n",
    "                                  \"cval\": 0,\n",
    "                                  \"width_shift_range\": 0.05,\n",
    "                                  \"height_shift_range\": 0.05,\n",
    "                                  \"zoom_range\": 0.1,\n",
    "                                  \"horizontal_flip\": True,\n",
    "                                  \"rescale\": 1.0 / 255,\n",
    "                                  \"preprocessing_function\": preprocess_input},\n",
    "                   data_flow_args={\"seed\": 1,\n",
    "                                   \"batch_size\": BATCH_SIZE}):\n",
    "\n",
    "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        directory=os.path.join(DATADIR, folder, \"images\"),\n",
    "        target_size=(IMAGE_ROW_SIZE, IMAGE_COLUMN_SIZE),\n",
    "        color_mode='rgb',\n",
    "        **data_flow_args)\n",
    "\n",
    "    return image_generator\n",
    "\n",
    "\n",
    "def count_images(folder=\"train\"):\n",
    "    image_directory = os.path.join(DATADIR, folder, \"images\")\n",
    "    data_size = 0\n",
    "\n",
    "    for char_name in os.listdir(image_directory):\n",
    "        char_directory = os.path.join(image_directory, char_name)\n",
    "        data_size += len(os.listdir(char_directory))\n",
    "\n",
    "    return data_size\n",
    "\n",
    "\n",
    "def steps_per_epoch(folder=\"train\", batch_size=BATCH_SIZE):\n",
    "    return count_images(folder) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features from my Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should not run this every time, instead just need to run it once to generate VGG16 features.\n",
    "\n",
    "It takes a very long time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "model = VGG16(include_top=False, weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pull out all the images for feeding through VGG16 we use `class_mode=None` and `shuffled=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 93638 images belonging to 8 classes.\n",
      "Found 93638 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "image_count_train = count_images(\"train\")\n",
    "generator_train = make_generator(\n",
    "    \"train\",\n",
    "    data_gen_args={\"preprocessing_function\": preprocess_input},\n",
    "    data_flow_args={\"batch_size\": BATCH_SIZE,\n",
    "                    \"class_mode\": None,\n",
    "                    \"shuffle\": False})\n",
    "generator_train_labels = make_generator(\n",
    "    \"train\",\n",
    "    data_gen_args={},\n",
    "    data_flow_args={\"batch_size\": BATCH_SIZE,\n",
    "                    \"shuffle\": False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step takes super long.  I wish I could figure out a faster way to pull out just the labels in generator order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = []\n",
    "image_count_train_cutoff = image_count_train / BATCH_SIZE + 1\n",
    "for i, (_, labels) in enumerate(generator_train_labels):\n",
    "    if i >= image_count_train_cutoff:\n",
    "        break\n",
    "    else:\n",
    "        labels_train.extend(labels)\n",
    "labels_train = np.array(labels_train[:image_count_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then this one takes a while too, but at least it makes sense why.  I wish I could figure out a faster way to do this as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features_train = model.predict_generator(generator_train,\n",
    "                                                    image_count_train_cutoff)\n",
    "bottleneck_features_train = bottleneck_features_train[:image_count_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATADIR, 'bottleneck_features_train.npy'), 'bw') as train:\n",
    "    np.save(train, bottleneck_features_train)\n",
    "with open(os.path.join(DATADIR, 'bottleneck_feature_labels_train.npy'), 'bw') as train:\n",
    "    np.save(train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same with our validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_count_valid = count_images(\"valid\")\n",
    "generator_valid = make_generator(\n",
    "    \"valid\",\n",
    "    data_gen_args={\"preprocessing_function\": preprocess_input},\n",
    "    data_flow_args={\"batch_size\": BATCH_SIZE,\n",
    "                    \"class_mode\": None,\n",
    "                    \"shuffle\": False})\n",
    "generator_valid_labels = make_generator(\n",
    "    \"valid\",\n",
    "    data_gen_args={},\n",
    "    data_flow_args={\"batch_size\": BATCH_SIZE,\n",
    "                    \"shuffle\": False})\n",
    "\n",
    "labels_valid = []\n",
    "image_count_valid_cutoff = image_count_valid / BATCH_SIZE + 1\n",
    "for i, (_, labels) in enumerate(generator_valid_labels):\n",
    "    if i >= image_count_valid_cutoff:\n",
    "        break\n",
    "    else:\n",
    "        labels_valid.extend(labels)\n",
    "labels_valid = np.array(labels_valid[:image_count_valid])\n",
    "\n",
    "bottleneck_features_valid = model.predict_generator(generator_valid,\n",
    "                                                    image_count_valid_cutoff)\n",
    "bottleneck_features_valid = bottleneck_features_valid[:image_count_valid]\n",
    "\n",
    "with open(os.path.join(DATADIR, 'bottleneck_features_valid.npy'), 'bw') as valid:\n",
    "    np.save(valid, bottleneck_features_valid)\n",
    "with open(os.path.join(DATADIR, 'bottleneck_feature_labels_valid.npy'), 'bw') as valid:\n",
    "    np.save(valid, labels_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Small Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up our features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATADIR, 'bottleneck_features_train.npy'), 'rb') as train:\n",
    "    train_data = np.load(train)\n",
    "with open(os.path.join(DATADIR, 'bottleneck_feature_labels_train.npy'), 'rb') as train:\n",
    "    train_labels = np.load(train)\n",
    "\n",
    "with open(os.path.join(DATADIR, 'bottleneck_features_valid.npy'), 'rb') as valid:\n",
    "    valid_data = np.load(valid)\n",
    "with open(os.path.join(DATADIR, 'bottleneck_feature_labels_valid.npy'), 'rb') as valid:\n",
    "    valid_labels = np.load(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're gonna need these callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "stop_on_val_loss = EarlyStopping(monitor='val_loss', min_delta=0.005, \n",
    "                                 patience=2, verbose=0, mode='auto')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, \n",
    "                              patience=2, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, finally, start training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "\n",
    "def init_model(input_shape, target_num=NUM_CHARACTERS,\n",
    "               learning_rate=LEARNING_RATE):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=input_shape))\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='lecun_uniform'))\n",
    "    model.add(Dense(target_num, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=learning_rate),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_model(train_data.shape[1:], learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history = model.fit(train_data, train_labels,\n",
    "                             epochs=50,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             validation_data=(valid_data, valid_labels),\n",
    "                             callbacks=[stop_on_val_loss, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(DATADIR, datetime.datetime.today().strftime('%Y-%m-%d-%H-%M') + \"-FIXED-DATA.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_generator(folder=\"test\",\n",
    "                        data_flow_args={\"seed\": 1,\n",
    "                                        \"batch_size\": BATCH_SIZE}):\n",
    "\n",
    "    return ImageDataGenerator().flow_from_directory(\n",
    "        directory=os.path.join(DATADIR, folder, \"images\"),\n",
    "        target_size=(IMAGE_ROW_SIZE, IMAGE_COLUMN_SIZE),\n",
    "        color_mode='rgb',\n",
    "        **data_flow_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches = make_test_generator()\n",
    "class_dict = test_batches.class_indices\n",
    "index_dict = {i: c for c, i in class_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(matrix, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(matrix, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = matrix.max() / 2.\n",
    "    for i, j in itertools.product(range(matrix.shape[0]), range(matrix.shape[1])):\n",
    "        plt.text(j, i, format(matrix[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, vectors = next(test_batches)\n",
    "true_indices = vectors.argmax(1)\n",
    "predictions = model.predict_on_batch(images)\n",
    "prediction_indices = predictions.argmax(1)\n",
    "class_names = [c for i, c in sorted(index_dict.items())]\n",
    "\n",
    "cnf_matrix = confusion_matrix(true_indices, prediction_indices)\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
