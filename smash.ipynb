{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: **I am a data scientist in the Bay Area!  Feel free to",
        " contact me with questions, concerns, offers for work ;)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- [trevor.m.murphy@gmail.com](mailto:trevor.m.murphy@gmail.c",
        "om)\n",
        "- [LinkedIn](https://www.linkedin.com/in/trevor-murphy-49ba1",
        "421)\n",
        "- [GitHub](https://github.com/tmurph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classification of Player Characters in Super Smash Bros. Melee"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook contains the code and resources to recognize t",
        "he most common player avatars (hereafter, characters) used b",
        "y professional players of the competitive fighting game Supe",
        "r Smash Bros. Melee."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A neural network is given images captured from scripted game",
        " play and trained via transfer learning from VGG16.  In test",
        "s the network achieves ~96% classification accuracy after ~1",
        "5 training epochs.  Here is a representative confusion matri",
        "x obtained after one such test:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](jupyter_images/confusion_matrix.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is Transfer Learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following contains merely the briefest description of tr",
        "ansfer learning.  [Many](https://machinelearningmastery.com/",
        "transfer-learning-for-deep-learning/) [more](http://ruder.io",
        "/transfer-learning/) [thorough](https://www.analyticsvidhya.",
        "com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-",
        "pre-trained-model/) [explanations](http://ftp.cs.wisc.edu/ma",
        "chine-learning/shavlik-group/torrey.handbook09.pdf) exist an",
        "d can be found with a simple Google search.  I assume the re",
        "ader is already familiar with convolutional neural networks ",
        "(CNN)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With this project I aimed to train a neural network to class",
        "ify custom-generated data with greater than 90% classificati",
        "on accuracy.  One might construct a neural network from scra",
        "tch for this purpose, but such an approach would also raise ",
        "significant challenges and force difficult tradeoffs between",
        " depth and cost.  The desired accuracy would require a suffi",
        "ciently deep network, while the cost to train such a network",
        " would increase rapidly with depth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A common alternative to from-scratch training is transfer le",
        "arning, where pre-trained, state-of-the-art image classifica",
        "tion models are repurposed to suit a data set that may diffe",
        "r significantly from the original training data.  When this ",
        "approach works it can significantly reduce the amount of tra",
        "ining (and therefore cost) required to achieve the desired c",
        "lassification accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first step of a transfer learning solution is selecting ",
        "a model.  The [ImageNet Large Scale Visual Recognition Chall",
        "enge](http://image-net.org/challenges/LSVRC/) attracts the a",
        "ttention of brilliant researchers and the resources of unive",
        "rsities and large internet companies.  As such, winning mode",
        "ls make excellent starting points for transfer learning.  No",
        "t all models are equally easy to repurpose for any given dat",
        "a set, however.  \n",
        "  \n",
        "Following [this blog post from the Keras developers](https:/",
        "/blog.keras.io/building-powerful-image-classification-models",
        "-using-very-little-data.html) I selected VGG16 for this proj",
        "ect.  My custom-generated images differ in size from the Ima",
        "geNet training data, but VGG16’s simple convolutional archit",
        "ecture makes it uniquely capable of adapting to this differe",
        "nce.  \n",
        "  \n",
        "The last step in transfer learning is called “fine-tuning” a",
        "nd involves retraining or replacing the final layers of the ",
        "model to increase the accuracy of predictions on the new dat",
        "a set.  The Keras library makes this particularly easy to do",
        "."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transfer Learning Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, let’s have a look at the images used to train the CNN",
        ".  Over 100,000 images were used, so we’ll draw a random sam",
        "ple and use [Matplotlib](https://matplotlib.org/) to display",
        " them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [16, 12]\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "DATA_DIR = \"data/keras\"\n",
        "\n",
        "all_images = []\n",
        "for root, dirs, files in os.walk(os.path.join(DATA_DIR, 'tra",
        "in', 'images')):\n",
        "    all_images.extend(files)\n",
        "\n",
        "img_re = re.compile('(?:[^/]*/)*'\n",
        "                    '(?P<character>[^_]*)_'\n",
        "                    '(?P<color>[^_]*)_'\n",
        "                    '(?P<stage>[^_]*)_'\n",
        "                    '(?P<orientation>[^_]*)_'\n",
        "                    'bg_on_'\n",
        "                    '(?P<img_number>[0-9]{3})'\n",
        "                    '.jpg')\n",
        "\n",
        "def get_image(character, color, stage, orientation, img_numb",
        "er):\n",
        "    image = os.path.join(DATA_DIR, 'train', 'images', charac",
        "ter,\n",
        "                         '{character}_{color}_{stage}_{orien",
        "tation}_bg_on_{img_number}.jpg'.format(character=character,\n",
        "                                                            ",
        "                                       color=color,\n",
        "                                                            ",
        "                                       stage=stage,\n",
        "                                                            ",
        "                                       orientation=orientati",
        "on,\n",
        "                                                            ",
        "                                       img_number=img_number",
        "))\n",
        "\n",
        "    rgb_image = plt.imread(image)\n",
        "\n",
        "    return rgb_image\n",
        "\n",
        "def get_image_by_name(name):\n",
        "    image_groups = re.search(img_re, name)\n",
        "    return get_image(**image_groups.groupdict())\n",
        "\n",
        "IMAGE_ROW_SIZE, IMAGE_COLUMN_SIZE, IMAGE_DEPTH = get_image_b",
        "y_name(all_images[0]).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "NUM_ROWS = 4\n",
        "NUM_COLS = 4\n",
        "\n",
        "fig, axarr = plt.subplots(NUM_ROWS, NUM_COLS, sharex=True, s",
        "harey=True, squeeze=False)\n",
        "\n",
        "for i, image_name in enumerate(random.sample(all_images, NUM",
        "_ROWS * NUM_COLS)):\n",
        "    ax = axarr[divmod(i, NUM_COLS)]\n",
        "    ax.tick_params(axis='both', which='both', bottom='off', ",
        "top='off', labelbottom='off', right='off', left='off', label",
        "left='off')\n",
        "    ax.imshow(get_image_by_name(image_name))\n",
        "\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These images were all captured from scripted gameplay.  The ",
        "[Data Generation](smash.org) section includes technical deta",
        "ils on the capturing process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s compare these images against the images used in the Im",
        "ageNet competition to train VGG16.  E.g. [these images from ",
        "the persian cat category](http://imagenet.stanford.edu/synse",
        "t?wnid=n02123394)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Without additional metadata about the captured images, such ",
        "as character bounding boxes or pose annotation points, we ca",
        "nnot easily compute rich statistics about the images.  Howev",
        "er, visual inspection suggests the following differences to ",
        "me:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Size within frame**\n",
        "  - Characters in captured images rarely fill more than 10% ",
        "of the frame\n",
        "  - Objects in ImageNet images often fill 25% or more of the",
        " frame\n",
        "- **Variation of pose / orientation**\n",
        "  - Characters in captured images, coming from a video game,",
        " contort into an extremely wide variety of poses\n",
        "  - Objects in ImageNet images are constrained by real biolo",
        "gy\n",
        "- **Composition within frame**\n",
        "  - Captured images frequently include visual effects like s",
        "moke and explosions, which can occlude parts of the characte",
        "rs\n",
        "  - In the video game characters frequently fall off the sid",
        "e of the frame, resulting in capture images that cut off par",
        "t of the character\n",
        "  - ImageNet images are comparatively well composed, placing",
        " the object in the front of the frame and fairly close to th",
        "e center"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Despite these differences, VGG16 is able to extract useful f",
        "eatures that lead to accurate image classification.  This is",
        " the power of award-winning neural networks and transfer lea",
        "rning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running image data through VGG16 is computationally expensiv",
        "e, so we would like to do it just once and cache the results",
        " for the later fine-tuning work.  These results are typicall",
        "y thought of as “important features” that have been “extract",
        "ed” from the image data, realized as a very large floating p",
        "oint array.  \n",
        "  \n",
        "This caching technique trades off increased memory requireme",
        "nts against decreased computational time.  Fortunately, memo",
        "ry is cheap relative to GPU compute, so this trade off makes",
        " sense for our problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code blocks set up the VGG model.  \n",
        "  \n",
        "We want to take advantage of the GPUs available to us, which",
        " we can do with the function `keras.utils.multi_gpu_model`. ",
        " This function accepts a template model (typically compiled ",
        "for a CPU-only architecture) and creates an identical model ",
        "compiled for a multi-GPU architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.utils import multi_gpu_model\n",
        "from keras.models import clone_model\n",
        "from GPUtil import getGPUs\n",
        "\n",
        "try:\n",
        "    NUM_GPU = len(getGPUs())\n",
        "except:\n",
        "    NUM_GPU = 0\n",
        "\n",
        "def gpuify_model_maybe(model):\n",
        "    result = None\n",
        "    if NUM_GPU >= 2:\n",
        "        result = multi_gpu_model(model, gpus=NUM_GPU)\n",
        "    elif NUM_GPU == 1:\n",
        "        with tf.device(\"/gpu:0\"):\n",
        "            result = clone_model(model)\n",
        "    else:\n",
        "        result = model\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with tf.device(\"/cpu:0\"):\n",
        "    template_vgg_model = VGG16(include_top=False, weights=\"i",
        "magenet\", input_shape=(IMAGE_ROW_SIZE, IMAGE_COLUMN_SIZE, IM",
        "AGE_DEPTH))\n",
        "gpu_vgg_model = gpuify_model_maybe(template_vgg_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code blocks set up the data flow into the mode",
        "l.  \n",
        "  \n",
        "As with most transfer learning work, we must preprocess our ",
        "images to match key statistical features of the original dat",
        "a set.  The function `keras.applications.imagenet_utils.prep",
        "rocess_input` does just this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.imagenet_utils import preprocess_inp",
        "ut\n",
        "\n",
        "BATCH_SIZE = 16 * max(NUM_GPU, 1)\n",
        "\n",
        "def make_generator(folder=\"train\",\n",
        "                   data_gen_args={\"fill_mode\": \"constant\",\n",
        "                                  \"cval\": 0,\n",
        "                                  \"width_shift_range\": 0.05,",
        "\n",
        "                                  \"height_shift_range\": 0.05",
        ",\n",
        "                                  \"zoom_range\": 0.1,\n",
        "                                  \"horizontal_flip\": True,\n",
        "                                  \"preprocessing_function\": ",
        "preprocess_input},\n",
        "                   data_flow_args={\"seed\": 1,\n",
        "                                   \"batch_size\": BATCH_SIZE}",
        "):\n",
        "\n",
        "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
        "\n",
        "    image_generator = image_datagen.flow_from_directory(\n",
        "        directory=os.path.join(DATA_DIR, folder, \"images\"),\n",
        "        target_size=(IMAGE_ROW_SIZE, IMAGE_COLUMN_SIZE),\n",
        "        color_mode='rgb',\n",
        "        **data_flow_args)\n",
        "\n",
        "    return image_generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Due to the design of the Keras library, we can’t easily extr",
        "act features from our images and simultaneously retain the i",
        "mage labels.  To work around this, we’re going to iterate ov",
        "er our images twice, in the same deterministic order both ti",
        "mes, pulling out features one time and labels the other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For feature extraction, we will use the `model.predict_gener",
        "ator` API that requires a generator that emits solely arrays",
        " of images (as opposed to pairs like `(image_array, label_ar",
        "ray)`).  To get this required generator we use `class_mode: ",
        "None` and `shuffle: False`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_generator_train = make_generator(\"train\", data_gen_arg",
        "s={\"preprocessing_function\": preprocess_input},\n",
        "                                       data_flow_args={\"batc",
        "h_size\": BATCH_SIZE, \"class_mode\": None, \"shuffle\": False})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For label extraction, we will use `shuffle: False` again to ",
        "iterate in the same order as above, but we fall back to the ",
        "default `class_mode` in order to retain the labels.="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "label_generator_train = make_generator(\"train\", data_gen_arg",
        "s={}, data_flow_args={\"batch_size\": BATCH_SIZE, \"shuffle\": F",
        "alse})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After we define some helper functions, we’ll finally be read",
        "y to extract our two arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_images(folder=\"train\"):\n",
        "    image_directory = os.path.join(DATA_DIR, folder, \"images",
        "\")\n",
        "    data_size = 0\n",
        "\n",
        "    for char_name in os.listdir(image_directory):\n",
        "        char_directory = os.path.join(image_directory, char_",
        "name)\n",
        "        data_size += len(os.listdir(char_directory))\n",
        "\n",
        "    return data_size\n",
        "\n",
        "def truncate_generator(gen, limit=None):\n",
        "    if limit is None:\n",
        "        return\n",
        "\n",
        "    for i, elt in enumerate(gen):\n",
        "        if i < limit:\n",
        "            yield elt\n",
        "        else:\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the following code can take a significant amount o",
        "f time to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_count_train = count_images(\"train\")\n",
        "image_count_train_generator_cutoff = image_count_train / BAT",
        "CH_SIZE + 1\n",
        "\n",
        "train_labels = []\n",
        "for _, labels in truncate_generator(label_generator_train, i",
        "mage_count_train_generator_cutoff):\n",
        "    train_labels.extend(labels)\n",
        "train_labels = np.array(train_labels[:image_count_train])\n",
        "\n",
        "train_data = gpu_vgg_model.predict_generator(image_generator",
        "_train, steps=image_count_train_generator_cutoff)\n",
        "train_data = train_data[:image_count_train]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let’s extract the analogous pair of arrays for our valid",
        "ation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_count_valid = count_images(\"valid\")\n",
        "image_count_valid_generator_cutoff = image_count_valid / BAT",
        "CH_SIZE + 1\n",
        "\n",
        "image_generator_valid = make_generator(\"valid\", data_gen_arg",
        "s={\"preprocessing_function\": preprocess_input},\n",
        "                                       data_flow_args={\"batc",
        "h_size\": BATCH_SIZE, \"class_mode\": None, \"shuffle\": False})\n",
        "label_generator_valid = make_generator(\"valid\", data_gen_arg",
        "s={}, data_flow_args={\"batch_size\": BATCH_SIZE, \"shuffle\": F",
        "alse})\n",
        "\n",
        "valid_labels = []\n",
        "for _, labels in truncate_generator(label_generator_valid, i",
        "mage_count_valid_generator_cutoff):\n",
        "    valid_labels.extend(labels)\n",
        "valid_labels = np.array(valid_labels[:image_count_valid])\n",
        "\n",
        "valid_data = gpu_vgg_model.predict_generator(image_generator",
        "_valid, steps=image_count_valid_generator_cutoff)\n",
        "valid_data = valid_data[:image_count_valid]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we have our training and validation arrays, but they are",
        " in non-random order.  Before we can train our final neural ",
        "network we must randomize them, being careful to preserve th",
        "e pairwise correlation between features and labels ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_random_index = np.arange(train_data.shape[0])\n",
        "np.random.shuffle(train_random_index)\n",
        "train_data = train_data[train_random_index]\n",
        "train_labels = train_labels[train_random_index]\n",
        "\n",
        "valid_random_index = np.arange(valid_data.shape[0])\n",
        "np.random.shuffle(valid_random_index)\n",
        "valid_data = valid_data[valid_random_index]\n",
        "valid_labels = valid_labels[valid_random_index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build and train the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We’ll build our model in a CPU environment but compile it fo",
        "r a GPU environment.  Because our classes are relatively bal",
        "anced in terms of number of images, we’ll use categorical cr",
        "ossentropy as our loss and classification accuracy as our me",
        "tric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dropout, Flatten, Dense\n",
        "\n",
        "LEARNING_RATE = 0.0001\n",
        "NUM_CHARACTERS = len(os.listdir(os.path.join(DATA_DIR, \"trai",
        "n\", \"images\")))\n",
        "\n",
        "def build_model(input_shape, target_num=NUM_CHARACTERS, hidd",
        "en_width=128, hidden_depth=1, dropout_rate=0.2):\n",
        "    with tf.device(\"/cpu:0\"):\n",
        "        model = Sequential()\n",
        "        model.add(Flatten(input_shape=input_shape))\n",
        "        for _ in range(hidden_depth):\n",
        "            model.add(Dense(hidden_width, activation='relu',",
        " kernel_initializer='lecun_uniform'))\n",
        "            model.add(Dropout(dropout_rate))\n",
        "        model.add(Dense(target_num, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "def compile_model(model, learning_rate=LEARNING_RATE):\n",
        "    model = gpuify_model_maybe(model)\n",
        "    model.compile(optimizer=Adam(lr=learning_rate), loss='ca",
        "tegorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following callbacks will, first, ensure early stopping i",
        "n case of a bad model, and, second, improve learning as the ",
        "model reaches diminishing returns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau",
        "\n",
        "\n",
        "stop_on_val_loss = EarlyStopping(monitor='val_loss', min_del",
        "ta=0.005, patience=3, verbose=0, mode='auto')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2",
        ", patience=2, min_lr=0.00001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s visualize our model as a quick sanity check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "____________________________________________________________",
            "_____\n",
            "Layer (type)                 Output Shape              Param",
            " #   \n",
            "============================================================",
            "=====\n",
            "flatten_3 (Flatten)          (None, 138240)            0    ",
            "     \n",
            "____________________________________________________________",
            "_____\n",
            "dense_9 (Dense)              (None, 512)               70779",
            "392  \n",
            "____________________________________________________________",
            "_____\n",
            "dropout_7 (Dropout)          (None, 512)               0    ",
            "     \n",
            "____________________________________________________________",
            "_____\n",
            "dense_10 (Dense)             (None, 512)               26265",
            "6    \n",
            "____________________________________________________________",
            "_____\n",
            "dropout_8 (Dropout)          (None, 512)               0    ",
            "     \n",
            "____________________________________________________________",
            "_____\n",
            "dense_11 (Dense)             (None, 512)               26265",
            "6    \n",
            "____________________________________________________________",
            "_____\n",
            "dropout_9 (Dropout)          (None, 512)               0    ",
            "     \n",
            "____________________________________________________________",
            "_____\n",
            "dense_12 (Dense)             (None, 8)                 4104 ",
            "     \n",
            "============================================================",
            "=====\n",
            "Total params: 71,308,808\n",
            "Trainable params: 71,308,808\n",
            "Non-trainable params: 0\n",
            "____________________________________________________________",
            "_____\n"
          ]
        }
      ],
      "source": [
        "template_model = build_model(train_data.shape[1:], hidden_wi",
        "dth=512, hidden_depth=3, dropout_rate=0.2)\n",
        "template_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally!  Time to train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compiled_model = compile_model(template_model)\n",
        "\n",
        "training_history = compiled_model.fit(train_data, train_labe",
        "ls,\n",
        "                                      epochs=15,\n",
        "                                      batch_size=BATCH_SIZE,",
        "\n",
        "                                      validation_data=(valid",
        "_data, valid_labels),\n",
        "                                      callbacks=[stop_on_val",
        "_loss, reduce_lr])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Due to a bug in Keras, we cannot directly save the weights a",
        "nd structure of a model compiled for multiple GPUs.  However",
        ", the multi-GPU model object shares both structure **and** w",
        "eights with the template model object, which we can save to ",
        "disk."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we save the `template_model`, however, we must rememb",
        "er that it is not yet useable for testing / production.  It ",
        "expects an array of VGG16 features, but in testing we will p",
        "rovide new images.  So now we’ll stack the `template_model` ",
        "on top of the `template_vgg_model` and save the result to di",
        "sk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "import datetime\n",
        "\n",
        "output_layer = template_vgg_model.output\n",
        "for layer in template_model.layers:\n",
        "    output_layer = layer(output_layer)\n",
        "full_model = Model(inputs=template_vgg_model.input, outputs=",
        "output_layer)\n",
        "full_model.save(os.path.join(DATA_DIR, datetime.datetime.tod",
        "ay().strftime('%Y-%m-%d-%H-%M') + \"-model.h5\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To compare true classification labels with the predictions o",
        "ur model emits, we’ll need to iterate twice over our test im",
        "ages in deterministic order again.  One pass will run the im",
        "ages through the model to produce predicted labels, and the ",
        "other will collect the true labels for the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_count_test = count_images(\"test\")\n",
        "image_count_test_generator_cutoff = image_count_test / BATCH",
        "_SIZE + 1\n",
        "\n",
        "image_generator_test = make_generator(\"test\", data_gen_args=",
        "{\"preprocessing_function\": preprocess_input},\n",
        "    data_flow_args={\"batch_size\": BATCH_SIZE, \"class_mode\": ",
        "None, \"shuffle\": False})\n",
        "label_generator_test = make_generator(\"test\", data_gen_args=",
        "{}, data_flow_args={\"batch_size\": BATCH_SIZE, \"shuffle\": Fal",
        "se})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "true_labels = []\n",
        "for _, labels in truncate_generator(label_generator_test, im",
        "age_count_test_generator_cutoff):\n",
        "    true_labels.extend(labels)\n",
        "true_labels = np.array(true_labels[:image_count_test])\n",
        "true_classes = true_labels.argmax(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_labels = full_model.predict_generator(image_generator_t",
        "est, steps=image_count_test_generator_cutoff)\n",
        "pred_labels = pred_labels[:image_count_test]\n",
        "pred_classes = pred_labels.argmax(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "c_matrix = confusion_matrix(true_classes, pred_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(matrix, classes, normalize=False, ",
        "title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`",
        ".\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        matrix = matrix.astype('float') / matrix.sum(axis=1)",
        "[:, np.newaxis]\n",
        "\n",
        "    plt.imshow(matrix, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = matrix.max() / 2.\n",
        "    for i, j in itertools.product(range(matrix.shape[0]), ra",
        "nge(matrix.shape[1])):\n",
        "        plt.text(j, i, format(matrix[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if matrix[i, j] > thresh else",
        " \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By default our class labels are encoded as integers, but we ",
        "can actually find the actual class names as taken from the f",
        "older hierarchy if we know where to look."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_names = [c for c, i in sorted(label_generator_test.cla",
        "ss_indices.items(), key=lambda pair: pair[1])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can visualize our accuracy!  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_confusion_matrix(c_matrix, class_names, normalize=True)"
      ]
    }
  ],
  "metadata": {
    "git": {
      "suppress_output": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
