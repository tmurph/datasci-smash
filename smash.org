#+PROPERTY: header-args:jupyter  :session smash
#+PROPERTY: header-args:jupyter+ :kernel datasci-smash
#+PROPERTY: header-args:jupyter+ :output-dir ./jupyter_images

* DONE tweak the makefile a bit
CLOSED: [2018-03-30 Fri 17:18]
- Note taken on [2018-03-30 Fri 17:13] \\
  okay, sweet.  don’t have any duplication of image files or anything, but it’s still relatively easy to control where `make hist' looks for the images, and the same for `make masks'.  renaming folders is a *bitch* but I think that’s just a fact of life … people shouldn’t do that.
  
  it’s actually not that bad to handle, just nuke the various _images and _masks files and remake the overall /images and /masks summary files.  of course, that took a while to get to the point where it wouldn’t nuke images and try to rerecord them, but yay!  it’s there!
  
  I feel like I’m happy enough with this guy.  time to move on to running the keras network.  
  
  if we need to tweak the mask files later, or if we bump into issues using docker to parallelize, then we’ll cross that bridge when we come to it
- Note taken on [2018-03-30 Fri 16:43] \\
  the default target is a usage message, which I like
- Note taken on [2018-03-30 Fri 15:44] \\
  yup this took a good bit of work, but I’m fairly happy with it now.
  
  histograms depend on the nobg folder, but it’s more of a temporary workspace
it needs cleaner separation between the full images (with bg on) and the reference images (with bg off)

that probably needs a new variable for reference moves (and rename “moves list” to “full moves list”)

and maybe think harder about how to reuse the existing rules just with different targets.  I’m sure make can handle it, somehow.

* DONE parallelize the record avi step
CLOSED: [2018-03-31 Sat 22:08]
- Note taken on [2018-03-31 Sat 22:06] \\
  omg that was a lot easier than I expected.  also, the fucking dolphin recording is the brittlest shit known to man.  I have some stuff in my personal config that’s required, and the record_avi shell script hard-codes these three assumptions:
  
  1. the location of the dolphin executable
  2. the location of my user directory
  3. the stupid way dolphin spits out two framedump files (as of 5.0-361, sure, that’s the behavior … but 5.0 release didn’t do it that way, and who the fuck knows what it’s doing now)
this is the only thing that stops me from running make with just a bajillion cores.  I’m sure there’s some sort of chroot trickery or something I can do

* Setup

#+BEGIN_SRC jupyter
  %matplotlib inline
  import matplotlib
  import matplotlib.pyplot as plt
  from mpl_toolkits.mplot3d import Axes3D
  plt.rcParams['figure.figsize'] = [9, 8]
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter
  import cv2
  import numpy as np
  import pandas as pd
  pd.set_option('display.width', 74)
  pd.set_option('display.max_columns', 15)
  pd.set_option('display.max_rows', 20)
#+END_SRC

#+RESULTS:

* One Image Prototype
** Prior Art

#+BEGIN_SRC jupyter
  character = 'samus'
  color = 4
  stage = 'battlefield'
  orientation = 'right'
  img_number = 3
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter
  image_dir = './images/'
  base = f'{character}_{color}_{stage}_{orientation}_bg_on_{img_number:03d}.jpg'
  nobg = f'{character}_{color}_{stage}_{orientation}_bg_off_{img_number:03d}.jpg'
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter
  bgr_base = cv2.imread(image_dir + base)
  bgr_nobg = cv2.imread(image_dir + nobg)

  rgb_base = cv2.cvtColor(bgr_base, cv2.COLOR_BGR2RGB)
  rgb_nobg = cv2.cvtColor(bgr_nobg, cv2.COLOR_BGR2RGB)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter :results file
  fig, (ax1, ax2) = plt.subplots(1, 2)
  ax1.imshow(rgb_base)
  ax2.imshow(rgb_nobg)
#+END_SRC

#+RESULTS:
[[file:./jupyter_images/93298Tod.png]]

** Convenience Function

#+BEGIN_SRC jupyter
  def plot_background_images(character, color, stage, orientation,
                             img_number):
      "Create two side-by-side plots of a scenario, with bg ON and bg OFF."
      image_dir = './images/'
      base = f'{character}_{color}_{stage}_{orientation}_bg_on_{img_number:03d}.jpg'
      nobg = f'{character}_{color}_{stage}_{orientation}_bg_off_{img_number:03d}.jpg'

      bgr_base = cv2.imread(image_dir + base)
      bgr_nobg = cv2.imread(image_dir + nobg)

      rgb_base = cv2.cvtColor(bgr_base, cv2.COLOR_BGR2RGB)
      rgb_nobg = cv2.cvtColor(bgr_nobg, cv2.COLOR_BGR2RGB)

      fig, (ax1, ax2) = plt.subplots(1, 2)
      ax1.imshow(rgb_base)
      ax2.imshow(rgb_nobg)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter :results file
  plot_background_images('samus', 2, 'fountain', 'right', 305)
#+END_SRC

#+RESULTS:
[[file:./jupyter_images/93298gyj.png]]

* Localization by Histogram

** DONE why do we make reference histograms instead of using all the histogram data?
CLOSED: [2018-03-05 Mon 14:29]
I’m pretty sure this is because there’s so much noise in the full data.  It was easier to make reference data (even though I couldn’t make that work in the makefile) and just use that for the segmentation / localization.

** DONE add makefile for reference data
CLOSED: [2018-03-17 Sat 19:21]
- Note taken on [2018-03-17 Sat 19:21] \\
  this was actually easy, just set up a MOVES_LIST variable in the Makefile and away we go
we need to generate this stuff for all the characters and all the colors

there’s some issue where it’s so close to the basic data, but just different enough

** Reference Histograms
We’ve got a lot of reference data.
#+BEGIN_SRC jupyter
  df = pd.read_csv('./reference_data/hist.csv')
  cha_col_df = df.drop(['stage', 'orientation', 'number'], axis=1)
  h_cols = cha_col_df.columns.str.contains('H')
  h_total = cha_col_df.iloc[:, h_cols].sum(axis=1)

  agg_df = cha_col_df.loc[h_total > 0]
  agg_total = h_total.loc[h_total > 0]

  agg_df.iloc[:, h_cols] = agg_df.iloc[:, h_cols].divide(agg_total, axis='index')
  agg_df = agg_df.groupby(['character', 'color']).agg(np.mean)
#+END_SRC

#+RESULTS:

*** Exploratory Analysis
:PROPERTIES:
:header-args:jupyter+: :exports none
:END:
Need to find a way to cut off the histograms for backpropagation purposes.

We can plot the summary histograms.
#+BEGIN_SRC jupyter :results file
  indices = agg_df.index.levels
  char_index = indices[0]
  color_index = indices[1]

  fig, axarr = plt.subplots(len(char_index), len(color_index),
                            sharey=True)
  char = 'samus'
  # for i, char in enumerate(char_index):
  for j, color in enumerate(color_index):
      if j == 0:
          axarr[j].set_ylabel(char, rotation=0, size='large')
      float_hist = agg_df.loc[char, color].values.reshape(180, 1)
      norm_hist = cv2.normalize(float_hist, float_hist, 0, 255,
                                  cv2.NORM_MINMAX, cv2.CV_8U)
      # axarr[i, j].plot(norm_hist)
      axarr[j].plot(norm_hist)

  fig.tight_layout()
#+END_SRC

#+RESULTS:
[[file:./jupyter_images/58906J8V.png]]

Just from visual inspection, we can see spikes that mark the most character-defining hues.  So let’s pick thresholding values that cut off just the tops of those peaks.

These are the thresholds I found by eyeballing.
#+BEGIN_SRC jupyter
  eyeball_thresh_dict = {0: 27, 1: 11, 2: 12, 3: 17, 4: 18}
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter :results file
  indices = agg_df.index.levels
  char_index = indices[0]
  color_index = indices[1]

  fig, axarr = plt.subplots(len(char_index), len(color_index),
                          sharey=True)
  char = 'samus'
  # for i, char in enumerate(char_index):
  for j, color in enumerate(color_index):
      if j == 0:
          axarr[j].set_ylabel(char, rotation=0, size='large')
      axarr[j].set_title(color)

      float_hist = agg_df.loc[char, color].values.reshape(180, 1)
      norm_hist = cv2.normalize(float_hist, float_hist, 0, 255,
                                  cv2.NORM_MINMAX, cv2.CV_8U)
      # axarr[i, j].plot(norm_hist)
      axarr[j].plot(norm_hist)
      axarr[j].axhline(eyeball_thresh_dict[color])

  fig.tight_layout()
#+END_SRC

#+RESULTS:
[[file:./jupyter_images/932986Gw.png]]

Ugh, this works, but constantly reshaping np arrays is just so annoying.
#+BEGIN_SRC jupyter
  def select_hist(character, color):
      float_hist = agg_df.loc[character, color].values.reshape(180, 1)
      norm_hist = cv2.normalize(float_hist, float_hist, 0, 255,
                                cv2.NORM_MINMAX, cv2.CV_8U)
      return norm_hist
#+END_SRC

#+RESULTS:

This feels like the worst hack, but eh, it works.
#+BEGIN_SRC jupyter
  def pick_peaks(hist):
      list_hist = hist.reshape(180)
      first = [list_hist[0] > list_hist[1]]
      middle = [list_hist[i] < e > list_hist[i+2] for i, e
                in enumerate(list_hist[1:-1])]
      last = [list_hist[-2] < list_hist[-1]]
      return first + middle + last
#+END_SRC

#+RESULTS:

This is easy to explain, I think, but the graph is really hard to interpret.  It’s like watching for when the noise drops out of my cutoff and I’m left with just the big peaks.
#+BEGIN_SRC jupyter
  def count_peaks(hist):
      arr = np.zeros(255, dtype=np.uint8)
      peaks = pick_peaks(hist)
      for x in range(hist.max()):
          arr[x] = sum(abovep and peakp for abovep, peakp in zip(hist > x, peaks))
      return arr
#+END_SRC

#+RESULTS:

Kinda makes me wonder … if I could write code that sliced off the top 25% of the area under the histogram, would that work?  But JFC I don’t want to write that code.

And this is the hackiest way to find a transition from “noisy peaks” to “big peaks”.  When does my cutoff line get past the noise of closely bunched together peaks and into the signal of distantly removed peaks?
#+BEGIN_SRC jupyter
  from itertools import groupby

  def first_peak_run(hist, min_length=4):
      peaks = count_peaks(hist)
      for k, g in groupby(peaks):
          if len(list(g)) >= min_length:
              peak_count = k
              break
      for i, e in enumerate(peaks):
          if e == peak_count:
              first_index = i
              break
      return first_index
#+END_SRC

#+RESULTS:

And these are the thresholds from my code.

#+BEGIN_SRC jupyter :results verbatim
  calc_thresh_dict = {n: first_peak_run(select_hist('samus', n))
                      for n in range(5)}
  calc_thresh_dict
#+END_SRC

#+RESULTS:
: {0: 10, 1: 12, 2: 11, 3: 14, 4: 27}

Here’s what the thresholds look like, side by side.
#+BEGIN_SRC jupyter :results file
  indices = agg_df.index.levels
  char_index = indices[0]
  color_index = indices[1]

  fig, axarr = plt.subplots(len(char_index), len(color_index),
                          sharey=True)
  char = 'samus'
  # for i, char in enumerate(char_index):
  for j, color in enumerate(color_index):
      if j == 0:
          axarr[j].set_ylabel(char, rotation=0, size='large')
      axarr[j].set_title(color)

      float_hist = agg_df.loc[char, color].values.reshape(180, 1)
      norm_hist = cv2.normalize(float_hist, float_hist, 0, 255,
                                  cv2.NORM_MINMAX, cv2.CV_8U)
      axarr[j].plot(norm_hist)
      axarr[j].axhline(eyeball_thresh_dict[color], color='b')
      axarr[j].axhline(calc_thresh_dict[color], color='r')

  fig.tight_layout()
#+END_SRC

#+RESULTS:
[[file:./jupyter_images/58906HUj.png]]

*** God Dammit the Peak Height Heuristic kinda fails

Check this out.

#+BEGIN_SRC jupyter
  df = pd.read_csv('reference_data/samus_2_fountain_left_bg_off_hist.csv')

  cha_col_df = df.drop(['stage', 'orientation', 'number'], axis=1)
  h_cols = cha_col_df.columns.str.contains('H')
  h_total = cha_col_df.iloc[:, h_cols].sum(axis=1)

  cha_col_df = cha_col_df.loc[h_total > 0]
  h_total = h_total.loc[h_total > 0]

  cha_col_df.iloc[:, h_cols] = cha_col_df.iloc[:, h_cols].divide(h_total, axis='index')
  agg_df = cha_col_df.groupby(['character', 'color']).agg(np.mean)

  float_hist = agg_df.loc['samus', 2].values
  float_hist = float_hist.reshape(len(float_hist), 1)
  back_proj_hist = cv2.normalize(float_hist, float_hist,
                                 alpha=0, beta=int8_max,
                                 norm_type=cv2.NORM_MINMAX,
                                 dtype=cv2.CV_8U)
  back_proj_hist = back_proj_hist.reshape(len(back_proj_hist))
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter :results file
  plt.plot(back_proj_hist)
#+END_SRC

#+RESULTS:
[[file:./jupyter_images/589061ID.png]]

Tough to eyeball the problem buuuuut the first peak here is actually 4 units high.  Our heuristic of “climb up until you get to a peak 4 higher than the last one” doesn’t really work.  We could probably tweak it, like “ignore the first peak” but seriously this is starting to smell like real hack work.

Let’s try cutting off the top X% of the histogram.

#+BEGIN_SRC jupyter
  def cutoff_thresh(hist, percentage=0.25):
      "Determine a threshold to select the top percentage of HIST."
      total_area = hist.sum()
      result = 0
      for x in range(hist.max(), 0, -1):
          cutoff_area = sum(map(lambda e: max(e - x, 0), hist))
          if cutoff_area > total_area * percentage:
              result = x
              break
      return result
#+END_SRC

#+RESULTS:

Nope, that’s shitty, there’s no real good correspondence with the stuff I found earlier.
#+NAME: perc_thresh_code
#+BEGIN_SRC jupyter :var perc=0.55 :results verbatim
  perc_thresh_dict = {n: cutoff_thresh(select_hist('samus', n).reshape(180),
                                       percentage=perc) for n in range(5)}
  perc_thresh_dict
#+END_SRC

#+RESULTS:
: {0: 22, 1: 14, 2: 11, 3: 16, 4: 15}

#+BEGIN_SRC jupyter :results file
  indices = agg_df.index.levels
  char_index = indices[0]
  color_index = indices[1]

  fig, axarr = plt.subplots(len(char_index), len(color_index),
                          sharey=True)
  char = 'samus'
  # for i, char in enumerate(char_index):
  for j, color in enumerate(color_index):
      if j == 0:
          axarr[j].set_ylabel(char, rotation=0, size='large')
      axarr[j].set_title(color)

      float_hist = agg_df.loc[char, color].values.reshape(180, 1)
      norm_hist = cv2.normalize(float_hist, float_hist, 0, 255,
                                  cv2.NORM_MINMAX, cv2.CV_8U)
      axarr[j].plot(norm_hist)
      axarr[j].axhline(perc_thresh_dict[color], color='b')
      axarr[j].axhline(calc_thresh_dict[color], color='r')

  fig.tight_layout()
#+END_SRC

#+RESULTS:
[[file:./jupyter_images/58906GEz.png]]

Hmm but actually maybe that’s not so bad?

** Object Segmentation and Localization

So let’s now see how we can do with object localization / segmentation.  That is to say, how well can we come up with bounding boxes / object masks.  There are two ways to segment: just straight masking of the images; and hue backprojection plus thresholding.  Let’s take a look at some random examples, find some edge cases, and move on.

#+BEGIN_SRC jupyter
  def four_images(character, color, stage, orientation, img_number,
                  agg_df, thresh_dict):

      img_name_on = f'./images/{character}_{color}_{stage}_{orientation}_bg_on_{img_number:03d}.jpg'
      rgb_img = cv2.cvtColor(cv2.imread(img_name_on), cv2.COLOR_BGR2RGB)

      img_name_off = f'./images/{character}_{color}_{stage}_{orientation}_bg_off_{img_number:03d}.jpg'
      hsv_img = cv2.cvtColor(cv2.imread(img_name_off), cv2.COLOR_BGR2HSV)
      hsv_mask = cv2.inRange(hsv_img, np.array([0, 50, 50]),
                          np.array([179, 255, 255]))
      h_img = hsv_img[:, :, 0]
      h_hist = cv2.calcHist([h_img], [0], hsv_mask, [180], [0, 180])

      target_hist = agg_df.loc[character, color].values.reshape(180, 1)
      norm_hist = cv2.normalize(target_hist, target_hist, 0, 255,
                              cv2.NORM_MINMAX, cv2.CV_8U)
      back_proj = norm_hist.flatten()[h_img.ravel()].reshape(h_img.shape)
      back_proj &= hsv_mask

      _, thresh_img = cv2.threshold(back_proj, thresh_dict[color],
                                    255, cv2.THRESH_BINARY)

      blur_img = cv2.GaussianBlur(thresh_img, (11, 11), 0)
      _, contours, _ = cv2.findContours(blur_img, cv2.RETR_EXTERNAL,
                                      cv2.CHAIN_APPROX_SIMPLE)
      contours = sorted(contours, key=cv2.contourArea, reverse=True)
      if contours:
          large_contour = contours[0]
          x, y, w, h = cv2.boundingRect(large_contour)
          final_img = cv2.rectangle(rgb_img.copy(), (x, y),
                                    (x + w, y + h), 255, 3)
          final_mask = cv2.rectangle(np.zeros(blur_img.shape,
                                              dtype=np.uint8),
                                     (x, y), (x + w, y + h), 255, -1)
          final_mask &= blur_img
      else:
          final_img = rgb_img
          final_mask = np.zeros(blur_img.shape)

      return hsv_mask, thresh_img, final_img, final_mask
#+END_SRC

#+RESULTS:

I think I actually may get better results from including just the tip of the last noisy peak?
#+NAME: calc_thresh_code
#+BEGIN_SRC jupyter :results verbatim
  calc_thresh_dict = {n: first_peak_run(select_hist('samus', n)) - 1
                      for n in range(5)}
  calc_thresh_dict
#+END_SRC

#+RESULTS:
: {0: 9, 1: 11, 2: 10, 3: 13, 4: 26}

#+BEGIN_SRC jupyter :results file
  indices = agg_df.index.levels
  char_index = indices[0]
  color_index = indices[1]

  nrows = 4
  ncols = 5

  fig, axarr = plt.subplots(nrows, ncols, sharey=True)

  for i in range(ncols):
      char = 'samus'
      color = np.random.choice(5)
      stage = np.random.choice(['final', 'fountain', 'stadium', 'story',
                              'battlefield', 'dreamland'])
      orientation = np.random.choice(['left', 'right'])
      img_number = np.random.randint(300)

      mask, thresh, final, segment = four_images(char, color, stage,
                                                 orientation,
                                                 img_number,
                                                 agg_df,
                                                 perc_thresh_dict)

      axarr[0, i].imshow(mask, cmap='gray')
      axarr[1, i].imshow(thresh, cmap='gray')
      axarr[2, i].imshow(final)
      axarr[3, i].imshow(segment, cmap='gray')
#+END_SRC

#+RESULTS:
[[file:./jupyter_images/58906GSb.png]]

This produces some really decent localization annotations.  And when we intersect the hue mask with the bounding box we get some decent segmentation.

There are definitely some bullshit edge cases.  Samus has a lot of yellow and red in her costume.  She also makes a lot of fire (yellow/red/orange) that gets found as a false positive.  In extreme cases, like when she’s curled in a ball above an exploding bomb, the bomb explosion is found but nothing of her.  Additionally, Yoshi’s Story has these red shy guys that are frequent false-positives.

Part of me says, go ahead and train the fucking Keras model.  If / when it gets shitty results, come back and clean the data some more.

*** Does it work with the reference data?
We got in trouble earlier trying to use the thresholding algorithm with a different data set.  Is the percentile algorithm better?

#+BEGIN_SRC jupyter
  def read_agg_df(path):
      df = pd.read_csv(path)

      cha_col_df = df.drop(['stage', 'orientation', 'number'], axis=1)
      h_cols = cha_col_df.columns.str.contains('H')
      h_total = cha_col_df.iloc[:, h_cols].sum(axis=1)

      cha_col_df = cha_col_df.loc[h_total > 0]
      h_total = h_total.loc[h_total > 0]

      cha_col_df.iloc[:, h_cols] = cha_col_df.iloc[:, h_cols].divide(h_total, axis='index')
      agg_df = cha_col_df.groupby(['character', 'color']).agg(np.mean)
      return agg_df
#+END_SRC

#+RESULTS:

#+NAME: set_agg_df
#+HEADER: :var path="reference_data/samus_2_fountain_left_bg_off_hist.csv"
#+BEGIN_SRC jupyter
  agg_df = read_agg_df(path)
#+END_SRC

#+RESULTS: set_agg_df

And let’s redefine that histogram function to be more functional, less global variable.

#+BEGIN_SRC jupyter
  def select_hist(agg_df, character, color):
      float_hist = agg_df.loc[character, color].values.reshape(180, 1)
      norm_hist = cv2.normalize(float_hist, float_hist, 0, 255,
                                cv2.NORM_MINMAX, cv2.CV_8U)
      return norm_hist.reshape(180)
#+END_SRC

#+RESULTS:

Check it works.

#+BEGIN_SRC jupyter
  cutoff_thresh(select_hist(agg_df, 'samus', 2))
#+END_SRC

#+RESULTS:
: 40

Okay so that’s a number, but where is it on the histogram?

#+BEGIN_SRC jupyter :results file
  plt.plot(select_hist(agg_df, 'samus', 2))
  plt.axhline(cutoff_thresh(select_hist(agg_df, 'samus', 2)))
#+END_SRC

#+RESULTS:
[[file:./jupyter_images/589060Tv.png]]

Eh?  Looks okay?  How does it perform on my data set?

Gotta get all the reference histograms.

#+CALL: set_agg_df(path="reference_data/hist.csv")

#+RESULTS:

#+NAME: set_perc_thresh
#+BEGIN_SRC jupyter :var perc=0.55 :results verbatim
  perc_thresh_dict = {n: cutoff_thresh(select_hist(agg_df, 'samus', n),
                                       percentage=perc) for n in range(5)}
  perc_thresh_dict
#+END_SRC

#+RESULTS: set_perc_thresh
: {0: 22, 1: 14, 2: 11, 3: 16, 4: 15}

Let’s make that image code reusable.

#+NAME: make_segmented_images
#+HEADER: :var dictionary="perc_thresh_dict"
#+BEGIN_SRC jupyter :results file
  indices = agg_df.index.levels
  char_index = indices[0]
  color_index = indices[1]

  nrows = 4
  ncols = 5

  fig, axarr = plt.subplots(nrows, ncols, sharey=True)

  for i in range(ncols):
      char = 'samus'
      color = np.random.choice(5)
      stage = np.random.choice(['final', 'fountain', 'stadium', 'story',
                              'battlefield', 'dreamland'])
      orientation = np.random.choice(['left', 'right'])
      img_number = np.random.randint(300)

      mask, thresh, final, segment = four_images(char, color, stage,
                                                 orientation,
                                                 img_number,
                                                 agg_df,
                                                 eval(dictionary))

      axarr[0, i].imshow(mask, cmap='gray')
      axarr[1, i].imshow(thresh, cmap='gray')
      axarr[2, i].imshow(final)
      axarr[3, i].imshow(segment, cmap='gray')
#+END_SRC

#+CALL: make_segmented_images(dictionary="perc_thresh_dict")

#+RESULTS:
[[file:./jupyter_images/58906bAq.png]]

Not bad.  I like how it’s better at picking up Samus’ gun.  Still got those shy guy and fire problems.

What about different thresholds?

#+CALL: set_perc_thresh(perc=0.55)

#+RESULTS:
: {0: 22, 1: 14, 2: 11, 3: 16, 4: 15}

#+CALL: make_segmented_images(dictionary="perc_thresh_dict")

#+RESULTS:
[[file:./jupyter_images/58906Pw2.png]]

I can’t really tell a difference from eyeballing.  Obvi a lower percentage / higher cutoff produces grainier / more disjointed masks.  Let’s just arbitrarily pick 55% as our threshold.

Yeah, fine, works with ref data.

* Neural Network Training
The hard part!  The fun part!  The part where I need a mentor!  Let’s do it!

** Follow a Keras tutorial
I like https://www.kaggle.com/xingyang/show-me-the-fishes-object-localization-with-cnn
 for the code, though it references [[file:~/Code/deepsense-whales][file:~/Code/deepsense-whales]] too.

#+NAME: fishes-localization
#+BEGIN_SRC jupyter :eval never
  """
      In order to run this script, you need to download the annotation files from https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring/discussion/25902 and modify the DATASET_FOLDER_PATH variable. The script has been tested on Python 3.6 with latest packages. You might need to modify the script because of the possible compatibility issues.
      The localization algorithm implemented here could achieve satisfactory results on the testing dataset. To further improve the performance, you may find the following links useful.
      https://deepsense.io/deep-learning-right-whale-recognition-kaggle/
      http://felixlaumon.github.io/2015/01/08/kaggle-right-whale.html
      https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html
  """
  assert False

  import matplotlib
  matplotlib.use("Agg")

  import os
  import glob
  import shutil
  import json
  import pylab
  import numpy as np
  from keras.applications.vgg16 import VGG16
  from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint
  from keras.layers import Dense, Dropout, Flatten, Input
  from keras.layers.normalization import BatchNormalization
  from keras.models import Model
  from keras.optimizers import Adam
  from keras.preprocessing.image import ImageDataGenerator
  from keras.utils.visualize_util import plot
  from scipy.misc import imread, imsave, imresize
  from sklearn.cluster import DBSCAN
  from sklearn.model_selection import GroupShuffleSplit

  # Dataset
  DATASET_FOLDER_PATH = os.path.join(os.path.expanduser("~"), "Documents/Dataset/The Nature Conservancy Fisheries Monitoring")
  TRAIN_FOLDER_PATH = os.path.join(DATASET_FOLDER_PATH, "train")
  TEST_FOLDER_PATH = os.path.join(DATASET_FOLDER_PATH, "test_stg1")
  LOCALIZATION_FOLDER_PATH = os.path.join(DATASET_FOLDER_PATH, "localization")
  ANNOTATION_FOLDER_PATH = os.path.join(DATASET_FOLDER_PATH, "annotations")
  CLUSTERING_RESULT_FILE_PATH = os.path.join(DATASET_FOLDER_PATH, "clustering_result.npy")

  # Workspace
  WORKSPACE_FOLDER_PATH = os.path.join("/tmp", os.path.basename(DATASET_FOLDER_PATH))
  CLUSTERING_FOLDER_PATH = os.path.join(WORKSPACE_FOLDER_PATH, "clustering")
  ACTUAL_DATASET_FOLDER_PATH = os.path.join(WORKSPACE_FOLDER_PATH, "actual_dataset")
  ACTUAL_TRAIN_ORIGINAL_FOLDER_PATH = os.path.join(ACTUAL_DATASET_FOLDER_PATH, "train_original")
  ACTUAL_VALID_ORIGINAL_FOLDER_PATH = os.path.join(ACTUAL_DATASET_FOLDER_PATH, "valid_original")
  ACTUAL_TRAIN_LOCALIZATION_FOLDER_PATH = os.path.join(ACTUAL_DATASET_FOLDER_PATH, "train_localization")
  ACTUAL_VALID_LOCALIZATION_FOLDER_PATH = os.path.join(ACTUAL_DATASET_FOLDER_PATH, "valid_localization")

  # Output
  OUTPUT_FOLDER_PATH = os.path.join(DATASET_FOLDER_PATH, "{}_output".format(os.path.basename(__file__).split(".")[0]))
  VISUALIZATION_FOLDER_PATH = os.path.join(OUTPUT_FOLDER_PATH, "Visualization")
  OPTIMAL_WEIGHTS_FOLDER_PATH = os.path.join(OUTPUT_FOLDER_PATH, "Optimal Weights")
  OPTIMAL_WEIGHTS_FILE_RULE = os.path.join(OPTIMAL_WEIGHTS_FOLDER_PATH, "epoch_{epoch:03d}-loss_{loss:.5f}-val_loss_{val_loss:.5f}.h5")

  # Image processing
  IMAGE_ROW_SIZE = 256
  IMAGE_COLUMN_SIZE = 256

  # Training and Testing procedure
  MAXIMUM_EPOCH_NUM = 1000
  PATIENCE = 100
  BATCH_SIZE = 32
  INSPECT_SIZE = 4

  def reformat_testing_dataset():
      # Create a dummy folder
      dummy_test_folder_path = os.path.join(TEST_FOLDER_PATH, "dummy")
      os.makedirs(dummy_test_folder_path, exist_ok=True)

      # Move files to the dummy folder if needed
      file_path_list = glob.glob(os.path.join(TEST_FOLDER_PATH, "*"))
      for file_path in file_path_list:
          if os.path.isfile(file_path):
              shutil.move(file_path, os.path.join(dummy_test_folder_path, os.path.basename(file_path)))

  def load_annotation():
      annotation_dict = {}
      annotation_file_path_list = glob.glob(os.path.join(ANNOTATION_FOLDER_PATH, "*.json"))
      for annotation_file_path in annotation_file_path_list:
          with open(annotation_file_path) as annotation_file:
              annotation_file_content = json.load(annotation_file)
              for item in annotation_file_content:
                  key = os.path.basename(item["filename"])
                  if key in annotation_dict:
                      assert False, "Found existing key {}!!!".format(key)
                  value = []
                  for annotation in item["annotations"]:
                      value.append(np.clip((annotation["x"], annotation["width"], annotation["y"], annotation["height"]), 0, np.inf).astype(np.int))
                  annotation_dict[key] = value
      return annotation_dict

  def reformat_localization():
      print("Creating the localization folder ...")
      os.makedirs(LOCALIZATION_FOLDER_PATH, exist_ok=True)

      print("Loading annotation ...")
      annotation_dict = load_annotation()

      original_image_path_list = glob.glob(os.path.join(TRAIN_FOLDER_PATH, "*/*"))
      for original_image_path in original_image_path_list:
          localization_image_path = LOCALIZATION_FOLDER_PATH + original_image_path[len(TRAIN_FOLDER_PATH):]
          if os.path.isfile(localization_image_path):
              continue

          localization_image_content = np.zeros(imread(original_image_path).shape[:2], dtype=np.uint8)
          for annotation_x, annotation_width, annotation_y, annotation_height in annotation_dict.get(os.path.basename(original_image_path), []):
              localization_image_content[annotation_y:annotation_y + annotation_height, annotation_x:annotation_x + annotation_width] = 255

          os.makedirs(os.path.abspath(os.path.join(localization_image_path, os.pardir)), exist_ok=True)
          imsave(localization_image_path, localization_image_content)

  def perform_CV(image_path_list, resized_image_row_size=64, resized_image_column_size=64):
      if os.path.isfile(CLUSTERING_RESULT_FILE_PATH):
          print("Loading clustering result ...")
          image_name_to_cluster_ID_array = np.load(CLUSTERING_RESULT_FILE_PATH)
          image_name_to_cluster_ID_dict = dict(image_name_to_cluster_ID_array)
          cluster_ID_array = np.array([image_name_to_cluster_ID_dict[os.path.basename(image_path)] for image_path in image_path_list], dtype=np.int)
      else:
          print("Reading image content ...")
          image_content_array = np.array([imresize(imread(image_path), (resized_image_row_size, resized_image_column_size)) for image_path in image_path_list])
          image_content_array = np.reshape(image_content_array, (len(image_content_array), -1))
          image_content_array = np.array([(image_content - image_content.mean()) / image_content.std() for image_content in image_content_array], dtype=np.float32)

          print("Apply clustering ...")
          cluster_ID_array = DBSCAN(eps=1.5 * resized_image_row_size * resized_image_column_size, min_samples=20, metric="l1", n_jobs=-1).fit_predict(image_content_array)

          print("Saving clustering result ...")
          image_name_to_cluster_ID_array = np.transpose(np.vstack(([os.path.basename(image_path) for image_path in image_path_list], cluster_ID_array)))
          np.save(CLUSTERING_RESULT_FILE_PATH, image_name_to_cluster_ID_array)

      print("The ID value and count are as follows:")
      cluster_ID_values, cluster_ID_counts = np.unique(cluster_ID_array, return_counts=True)
      for cluster_ID_value, cluster_ID_count in zip(cluster_ID_values, cluster_ID_counts):
          print("{}\t{}".format(cluster_ID_value, cluster_ID_count))

      print("Visualizing clustering result ...")
      shutil.rmtree(CLUSTERING_FOLDER_PATH, ignore_errors=True)
      for image_path, cluster_ID in zip(image_path_list, cluster_ID_array):
          sub_clustering_folder_path = os.path.join(CLUSTERING_FOLDER_PATH, str(cluster_ID))
          if not os.path.isdir(sub_clustering_folder_path):
              os.makedirs(sub_clustering_folder_path)
          os.symlink(image_path, os.path.join(sub_clustering_folder_path, os.path.basename(image_path)))

      cv_object = GroupShuffleSplit(n_splits=100, test_size=0.2, random_state=0)
      for cv_index, (train_index_array, valid_index_array) in enumerate(cv_object.split(X=np.zeros((len(cluster_ID_array), 1)), groups=cluster_ID_array), start=1):
          print("Checking cv {} ...".format(cv_index))
          valid_sample_ratio = len(valid_index_array) / (len(train_index_array) + len(valid_index_array))
          if -1 in np.unique(cluster_ID_array[train_index_array]) and valid_sample_ratio > 0.15 and valid_sample_ratio < 0.25:
              train_unique_label, train_unique_counts = np.unique([image_path.split("/")[-2] for image_path in np.array(image_path_list)[train_index_array]], return_counts=True)
              valid_unique_label, valid_unique_counts = np.unique([image_path.split("/")[-2] for image_path in np.array(image_path_list)[valid_index_array]], return_counts=True)
              if np.array_equal(train_unique_label, valid_unique_label):
                  train_unique_ratio = train_unique_counts / np.sum(train_unique_counts)
                  valid_unique_ratio = valid_unique_counts / np.sum(valid_unique_counts)
                  print("Using {:.2f}% original training samples as validation samples ...".format(valid_sample_ratio * 100))
                  print("For training samples: {}".format(train_unique_ratio))
                  print("For validation samples: {}".format(valid_unique_ratio))
                  return train_index_array, valid_index_array

      assert False

  def reorganize_dataset():
      # Get list of files
      original_image_path_list = sorted(glob.glob(os.path.join(TRAIN_FOLDER_PATH, "*/*")))
      localization_image_path_list = sorted(glob.glob(os.path.join(LOCALIZATION_FOLDER_PATH, "*/*")))

      # Sanity check
      original_image_name_list = [os.path.basename(image_path) for image_path in original_image_path_list]
      localization_image_name_list = [os.path.basename(image_path) for image_path in localization_image_path_list]
      assert np.array_equal(original_image_name_list, localization_image_name_list)

      # Perform Cross Validation
      train_index_array, valid_index_array = perform_CV(original_image_path_list)

      # Create symbolic links
      shutil.rmtree(ACTUAL_DATASET_FOLDER_PATH, ignore_errors=True)
      for (actual_original_folder_path, actual_localization_folder_path), index_array in zip(
              ((ACTUAL_TRAIN_ORIGINAL_FOLDER_PATH, ACTUAL_TRAIN_LOCALIZATION_FOLDER_PATH),
              (ACTUAL_VALID_ORIGINAL_FOLDER_PATH, ACTUAL_VALID_LOCALIZATION_FOLDER_PATH)),
              (train_index_array, valid_index_array)):
          for index_value in index_array:
              original_image_path = original_image_path_list[index_value]
              localization_image_path = localization_image_path_list[index_value]

              path_suffix = original_image_path[len(TRAIN_FOLDER_PATH):]
              assert path_suffix == localization_image_path[len(LOCALIZATION_FOLDER_PATH):]

              if path_suffix[1:].startswith("NoF"):
                  continue

              actual_original_image_path = actual_original_folder_path + path_suffix
              actual_localization_image_path = actual_localization_folder_path + path_suffix

              os.makedirs(os.path.abspath(os.path.join(actual_original_image_path, os.pardir)), exist_ok=True)
              os.makedirs(os.path.abspath(os.path.join(actual_localization_image_path, os.pardir)), exist_ok=True)

              os.symlink(original_image_path, actual_original_image_path)
              os.symlink(localization_image_path, actual_localization_image_path)

      return len(glob.glob(os.path.join(ACTUAL_TRAIN_ORIGINAL_FOLDER_PATH, "*/*"))), len(glob.glob(os.path.join(ACTUAL_VALID_ORIGINAL_FOLDER_PATH, "*/*")))

  def init_model(target_num=4, FC_block_num=2, FC_feature_dim=512, dropout_ratio=0.5, learning_rate=0.0001):
      # Get the input tensor
      input_tensor = Input(shape=(3, IMAGE_ROW_SIZE, IMAGE_COLUMN_SIZE))

      # Convolutional blocks
      pretrained_model = VGG16(include_top=False, weights="imagenet")
      for layer in pretrained_model.layers:
          layer.trainable = False
      output_tensor = pretrained_model(input_tensor)

      # FullyConnected blocks
      output_tensor = Flatten()(output_tensor)
      for _ in range(FC_block_num):
          output_tensor = Dense(FC_feature_dim, activation="relu")(output_tensor)
          output_tensor = BatchNormalization()(output_tensor)
          output_tensor = Dropout(dropout_ratio)(output_tensor)
      output_tensor = Dense(target_num, activation="sigmoid")(output_tensor)

      # Define and compile the model
      model = Model(input_tensor, output_tensor)
      model.compile(optimizer=Adam(lr=learning_rate), loss="mse")
      plot(model, to_file=os.path.join(OPTIMAL_WEIGHTS_FOLDER_PATH, "model.png"), show_shapes=True, show_layer_names=True)

      return model

  def convert_localization_to_annotation(localization_array, row_size=IMAGE_ROW_SIZE, column_size=IMAGE_COLUMN_SIZE):
      annotation_list = []
      for localization in localization_array:
          localization = localization[0]

          mask_along_row = np.max(localization, axis=1) > 0.5
          row_start_index = np.argmax(mask_along_row)
          row_end_index = len(mask_along_row) - np.argmax(np.flipud(mask_along_row)) - 1

          mask_along_column = np.max(localization, axis=0) > 0.5
          column_start_index = np.argmax(mask_along_column)
          column_end_index = len(mask_along_column) - np.argmax(np.flipud(mask_along_column)) - 1

          annotation = (row_start_index / row_size, (row_end_index - row_start_index) / row_size, column_start_index / column_size, (column_end_index - column_start_index) / column_size)
          annotation_list.append(annotation)

      return np.array(annotation_list).astype(np.float32)

  def convert_annotation_to_localization(annotation_array, row_size=IMAGE_ROW_SIZE, column_size=IMAGE_COLUMN_SIZE):
      localization_list = []
      for annotation in annotation_array:
          localization = np.zeros((row_size, column_size))

          row_start_index = np.max((0, int(annotation[0] * row_size)))
          row_end_index = np.min((row_start_index + int(annotation[1] * row_size), row_size - 1))

          column_start_index = np.max((0, int(annotation[2] * column_size)))
          column_end_index = np.min((column_start_index + int(annotation[3] * column_size), column_size - 1))

          localization[row_start_index:row_end_index + 1, column_start_index:column_end_index + 1] = 1
          localization_list.append(np.expand_dims(localization, axis=0))

      return np.array(localization_list).astype(np.float32)

  def load_dataset(folder_path_list, color_mode_list, batch_size, classes=None, class_mode=None, shuffle=True, seed=None, apply_conversion=False):
      # Get the generator of the dataset
      data_generator_list = []
      for folder_path, color_mode in zip(folder_path_list, color_mode_list):
          data_generator_object = ImageDataGenerator(
              rotation_range=10,
              width_shift_range=0.05,
              height_shift_range=0.05,
              shear_range=0.05,
              zoom_range=0.2,
              horizontal_flip=True,
              rescale=1.0 / 255)
          data_generator = data_generator_object.flow_from_directory(
              directory=folder_path,
              target_size=(IMAGE_ROW_SIZE, IMAGE_COLUMN_SIZE),
              color_mode=color_mode,
              classes=classes,
              class_mode=class_mode,
              batch_size=batch_size,
              shuffle=shuffle,
              seed=seed)
          data_generator_list.append(data_generator)

      # Sanity check
      filenames_list = [data_generator.filenames for data_generator in data_generator_list]
      assert all(filenames == filenames_list[0] for filenames in filenames_list)

      if apply_conversion:
          assert len(data_generator_list) == 2
          for X_array, Y_array in zip(*data_generator_list):
              yield (X_array, convert_localization_to_annotation(Y_array))
      else:
          for array_tuple in zip(*data_generator_list):
              yield array_tuple

  class InspectPrediction(Callback):
      def __init__(self, data_generator_list):
          super(InspectPrediction, self).__init__()

          self.data_generator_list = data_generator_list

      def on_epoch_end(self, epoch, logs=None):
          for data_generator_index, data_generator in enumerate(self.data_generator_list, start=1):
              X_array, GT_Y_array = next(data_generator)
              P_Y_array = convert_annotation_to_localization(self.model.predict_on_batch(X_array))

              for sample_index, (X, GT_Y, P_Y) in enumerate(zip(X_array, GT_Y_array, P_Y_array), start=1):
                  pylab.figure()
                  pylab.subplot(1, 3, 1)
                  pylab.imshow(np.rollaxis(X, 0, 3))
                  pylab.title("X")
                  pylab.axis("off")
                  pylab.subplot(1, 3, 2)
                  pylab.imshow(GT_Y[0], cmap="gray")
                  pylab.title("GT_Y")
                  pylab.axis("off")
                  pylab.subplot(1, 3, 3)
                  pylab.imshow(P_Y[0], cmap="gray")
                  pylab.title("P_Y")
                  pylab.axis("off")
                  pylab.savefig(os.path.join(VISUALIZATION_FOLDER_PATH, "Epoch_{}_Split_{}_Sample_{}.png".format(epoch + 1, data_generator_index, sample_index)))
                  pylab.close()

  class InspectLoss(Callback):
      def __init__(self):
          super(InspectLoss, self).__init__()

          self.train_loss_list = []
          self.valid_loss_list = []

      def on_epoch_end(self, epoch, logs=None):
          train_loss = logs.get("loss")
          valid_loss = logs.get("val_loss")
          self.train_loss_list.append(train_loss)
          self.valid_loss_list.append(valid_loss)
          epoch_index_array = np.arange(len(self.train_loss_list)) + 1

          pylab.figure()
          pylab.plot(epoch_index_array, self.train_loss_list, "yellowgreen", label="train_loss")
          pylab.plot(epoch_index_array, self.valid_loss_list, "lightskyblue", label="valid_loss")
          pylab.grid()
          pylab.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=2, ncol=2, mode="expand", borderaxespad=0.)
          pylab.savefig(os.path.join(OUTPUT_FOLDER_PATH, "Loss Curve.png"))
          pylab.close()

  def run():
      print("Creating folders ...")
      os.makedirs(VISUALIZATION_FOLDER_PATH, exist_ok=True)
      os.makedirs(OPTIMAL_WEIGHTS_FOLDER_PATH, exist_ok=True)

      print("Reformatting testing dataset ...")
      reformat_testing_dataset()

      print("Reformatting localization ...")
      reformat_localization()

      print("Reorganizing dataset ...")
      train_sample_num, valid_sample_num = reorganize_dataset()

      print("Initializing model ...")
      model = init_model()

      weights_file_path_list = sorted(glob.glob(os.path.join(OPTIMAL_WEIGHTS_FOLDER_PATH, "*.h5")))
      if len(weights_file_path_list) == 0:
          print("Performing the training procedure ...")
          train_generator = load_dataset(folder_path_list=[ACTUAL_TRAIN_ORIGINAL_FOLDER_PATH, ACTUAL_TRAIN_LOCALIZATION_FOLDER_PATH], color_mode_list=["rgb", "grayscale"], batch_size=BATCH_SIZE, seed=0, apply_conversion=True)
          valid_generator = load_dataset(folder_path_list=[ACTUAL_VALID_ORIGINAL_FOLDER_PATH, ACTUAL_VALID_LOCALIZATION_FOLDER_PATH], color_mode_list=["rgb", "grayscale"], batch_size=BATCH_SIZE, seed=0, apply_conversion=True)
          train_generator_for_inspection = load_dataset(folder_path_list=[ACTUAL_TRAIN_ORIGINAL_FOLDER_PATH, ACTUAL_TRAIN_LOCALIZATION_FOLDER_PATH], color_mode_list=["rgb", "grayscale"], batch_size=INSPECT_SIZE, seed=1)
          valid_generator_for_inspection = load_dataset(folder_path_list=[ACTUAL_VALID_ORIGINAL_FOLDER_PATH, ACTUAL_VALID_LOCALIZATION_FOLDER_PATH], color_mode_list=["rgb", "grayscale"], batch_size=INSPECT_SIZE, seed=1)
          earlystopping_callback = EarlyStopping(monitor="val_loss", patience=PATIENCE)
          modelcheckpoint_callback = ModelCheckpoint(OPTIMAL_WEIGHTS_FILE_RULE, monitor="val_loss", save_best_only=True, save_weights_only=True)
          inspectprediction_callback = InspectPrediction([train_generator_for_inspection, valid_generator_for_inspection])
          inspectloss_callback = InspectLoss()
          model.fit_generator(generator=train_generator,
                              samples_per_epoch=train_sample_num,
                              validation_data=valid_generator,
                              nb_val_samples=valid_sample_num,
                              callbacks=[earlystopping_callback, modelcheckpoint_callback, inspectprediction_callback, inspectloss_callback],
                              nb_epoch=MAXIMUM_EPOCH_NUM, verbose=2)
          weights_file_path_list = sorted(glob.glob(os.path.join(OPTIMAL_WEIGHTS_FOLDER_PATH, "*.h5")))

      print("All done!")

  if __name__ == "__main__":
      run()
#+END_SRC

There’s this thing with the input tensor below.  The shape parameter varies with the Keras backend.  Ugh.  Couldn’t ask for two parameters and handle that logic yourself?  Or I dunno.  Everything is terrible.

Note also that the final loss is measured by mean squared error and we’re targeting four neurons.  Makes sense in my base case.  Eventually may have to learn how to model two different losses at the same time, to do category and regression.

#+BEGIN_SRC jupyter
  from keras.applications.vgg16 import VGG16
  from keras.layers import Dense, Dropout, Flatten, Input
  from keras.layers.normalization import BatchNormalization
  from keras.models import Model
  from keras.optimizers import Adam

  # Image processing
  IMAGE_ROW_SIZE = 584
  IMAGE_COLUMN_SIZE = 480

  def init_model(target_num=4, FC_block_num=2, FC_feature_dim=512, dropout_ratio=0.5, learning_rate=0.0001):
      # Get the input tensor
      input_tensor = Input(shape=(IMAGE_ROW_SIZE, IMAGE_COLUMN_SIZE, 3))

      # Convolutional blocks
      pretrained_model = VGG16(include_top=False, weights="imagenet")
      for layer in pretrained_model.layers:
          layer.trainable = False
      output_tensor = pretrained_model(input_tensor)

      # FullyConnected blocks
      output_tensor = Flatten()(output_tensor)
      for _ in range(FC_block_num):
          output_tensor = Dense(FC_feature_dim, activation="relu")(output_tensor)
          output_tensor = BatchNormalization()(output_tensor)
          output_tensor = Dropout(dropout_ratio)(output_tensor)
      output_tensor = Dense(target_num, activation="sigmoid")(output_tensor)

      # Define and compile the model
      model = Model(input_tensor, output_tensor)
      model.compile(optimizer=Adam(lr=learning_rate), loss="mse")

      return model
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter :results replace :file model.png :output-dir
  model = init_model()

  from keras.utils.vis_utils import plot_model
  plot_model(model, show_shapes=True, show_layer_names=True)
#+END_SRC

#+RESULTS:
[[file:model.png]]

Time to generate some training and validation sets.

Here’s a snag, though.  If we wanna do data augmentation (and I guess we do?) then we can’t easily pull out =(image, annotation)= pairs.  It’s simple to randomly transform the images with ~ImageDataGenerator.flow_from_directory~ but *good luck* trying to apply the same set of random transformations to the annotations.

I kinda wonder if there’s an element here from Gelman’s modeling advice.  We could try to model the bounding box of the character, try to predict four numbers =(x, y, l, w)= … or we could try to model the segmentation of the character, all pixel-by-pixel probabilities of being the character or not, and *then* reduce that to a bounding box at the end if you want.

So, quick digression, we’ve gotta process the images to their masks.

** Mask Script

#+BEGIN_SRC jupyter :tangle scripts/process_masks.py :eval no
  import os
  import sys
  import argparse
  import re
  from itertools import groupby

  import numpy as np
  import pandas as pd
  import cv2

  int8_max = 255

  hue_chan = 0
  hue_min = 0
  hue_max = 180

  img_re = re.compile('(?:[^/]*/)*'
                      '(?P<character>[^_]*)_'
                      '(?P<color>[^_]*)_'
                      '(?P<stage>[^_]*)_'
                      '(?P<orientation>[^_]*)_'
                      'bg_(?P<background>[^_]*)_'
                      '(?P<number>[0-9]{3})'
                      '.jpg')


  def writeable_dir(prospective_dir):
      if not os.path.isdir(prospective_dir):
          raise Exception(f"{prospective_dir} is not a valid path")
      if not os.access(prospective_dir, os.W_OK):
          raise Exception(f"{prospective_dir} is not a writeable dir")
      return prospective_dir


  class CharacterColorHistogram():


      def __init__(self, master_hist):
          # Reduce the master hue histogram to character/color averages
          cha_col_df = master_hist.drop(
              ['stage', 'orientation', 'number'], axis=1)
          h_cols = cha_col_df.columns.str.contains('H')
          h_total = cha_col_df.iloc[:, h_cols].sum(axis=1)

          cha_col_df = cha_col_df.loc[h_total > 0]
          h_total = h_total.loc[h_total > 0]

          cha_col_df.iloc[:, h_cols] = cha_col_df.iloc[:, h_cols].divide(
              h_total, axis='index')
          agg_df = cha_col_df.groupby(['character', 'color']).agg(np.mean)

          self._aggregate_df = agg_df


      def histogram(self, character, color):
          float_hist = self._aggregate_df.loc[character, color].values
          float_hist = float_hist.reshape(len(float_hist), 1)
          back_proj_hist = cv2.normalize(float_hist, float_hist,
                                         alpha=0, beta=int8_max,
                                         norm_type=cv2.NORM_MINMAX,
                                         dtype=cv2.CV_8U)
          back_proj_hist = back_proj_hist.reshape(len(back_proj_hist))
          return back_proj_hist


  def cutoff_thresh(hist, min_peak_height=4):
      "Determine a threshold to select tall peaks from HIST."
      # where are the peaks?
      first = hist[0] > hist[1]
      middle = [hist[i] < e > hist[i+2]
                for i, e in enumerate(hist[1:-1])]
      last = hist[-2] < hist[-1]
      peak_p = [first] + middle + [last]

      # how are the peaks spaced out?
      peaks_above_x = np.zeros(int8_max, dtype=np.uint8)
      for x in range(hist.max()):
          peaks_above_x[x] = sum(abovep and peakp
                                 for abovep, peakp
                                 in zip(hist > x, peak_p))

      # when's the first big peak?
      for k, g in groupby(peaks_above_x):
          peak_height = len(list(g))
          if peak_height >= min_peak_height:
              peak_key = k
              break
      for i, e in enumerate(peaks_above_x):
          if e == peak_key:
              first_index = i
              break

      return first_index


  def mask_from_image(hsv_img, back_proj_hist, thresh,
                      hsv_min=np.array([0, 50, 50]),
                      hsv_max=np.array([179, 255, 255]),
                      blur_size=11):
      """Calculate a greyscale ROI mask from HSV_IMG.

  Backproject the hues of HSV_IMG according to BACK_PROJ_HIST and cut off
  the backprojection at THRESH.  In case multiple regions survive the
  cutoff, select the region with the largest area.

      """
      # backproject hues, but mask out pixels with little saturation/value
      hsv_mask = cv2.inRange(hsv_img, hsv_min, hsv_max)
      h_img = hsv_img[:, :, hue_chan]
      back_proj = back_proj_hist[h_img.ravel()].reshape(h_img.shape)
      back_proj &= hsv_mask

      # theshold and select largest region
      _, thresh_img = cv2.threshold(back_proj, thresh, int8_max,
                                    cv2.THRESH_BINARY)
      blur_img = cv2.GaussianBlur(thresh_img, (blur_size, blur_size),
                                  cv2.BORDER_CONSTANT)
      _, contours, _ = cv2.findContours(blur_img, cv2.RETR_EXTERNAL,
                                        cv2.CHAIN_APPROX_SIMPLE)
      contours = sorted(contours, key=cv2.contourArea, reverse=True)

      # calculate greyscale mask
      if contours:
          large_contour = contours[0]
          final_mask = cv2.drawContours(np.zeros(blur_img.shape,
                                                 dtype=np.uint8),
                                        contours,
                                        contourIdx=0,
                                        color=int8_max,
                                        thickness=cv2.FILLED)
          final_mask &= blur_img
      else:
          final_mask = np.zeros(blur_img.shape)

      return final_mask


  def main(argv=None):
      if argv is None:
          argv = sys.argv

      description = ('Process images to masks.')
      parser = argparse.ArgumentParser(description=description,
                                       fromfile_prefix_chars='@')
      parser.add_argument('outdir', type=writeable_dir,
                          help='directory to place mask images')
      parser.add_argument('hist', type=argparse.FileType(),
                          help='CSV of backprojection hue histograms')
      parser.add_argument('image', nargs='+', help='images to process')

      args = parser.parse_args(argv[1:])

      image_name_lst = args.image
      outdir = args.outdir
      master_hist_df = pd.read_csv(args.hist)

      character_colors = CharacterColorHistogram(master_hist_df)

      for image_name in image_name_lst:
          match = re.search(img_re, image_name)
          if match:
              character = match.group('character')
              color = int(match.group('color'))

              hsv_img = cv2.cvtColor(cv2.imread(image_name),
                                     cv2.COLOR_BGR2HSV)
              back_proj_hist = character_colors.histogram(character, color)
              # couldn't tell you why, but for some reason I get better
              # results including just the tippy top of the cutoff peaks
              optimal_thresh = cutoff_thresh(back_proj_hist) - 1

              mask_image = mask_from_image(hsv_img,
                                           back_proj_hist,
                                           optimal_thresh)
              mask_image_name = ('{character}_'
                                 '{color}_'
                                 '{stage}_'
                                 '{orientation}_'
                                 'bg_{background}_'
                                 '{number}_'
                                 'mask.jpg').format(**match.groupdict())
              cv2.imwrite(os.path.join(outdir, mask_image_name), mask_image)


  if __name__ == '__main__':
      sys.exit(main())
#+END_SRC

#+RESULTS:

** DONE make masks
CLOSED: [2018-03-28 Wed 15:00]
- Note taken on [2018-03-28 Wed 15:00] \\
  I’m happy with top 55%.  makefile away!
- Note taken on [2018-03-17 Sat 19:13] \\
  god dammit, my approach of finding large peaks is too brittle.  let’s try cutting off the top x% of the hist
need a makefile target, and need to finish the process_masks script

oh god, okay, now we need to actually figure out how to calculate the threshold from the histogram

so that needs to happen in the masking function itself

** Back to Keras

#+BEGIN_SRC jupyter
  def generate_input_output_pairs(seed=None):
      image_generator_object = ImageDataGenerator(
          rotation_range=10,
          width_shift_range=0.05,
          height_shift_range=0.05,
          shear_range=0.05,
          zoom_range=0.2,
          horizontal_flip=True,
          rescale=1.0 / 255)
      image_generator = image_generator_object.flow_from_directory(
          directory=IMAGEDIR,
          target_size=(IMAGE_ROW_SIZE, IMAGE_COLUMN_SIZE),
          color_mode='rgb',
          shuffle=True,
          seed=seed)
#+END_SRC
