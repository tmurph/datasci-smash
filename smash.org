#+PROPERTY: header-args:jupyter  :session smash
#+PROPERTY: header-args:jupyter+ :kernel datasci-smash
#+PROPERTY: header-args:jupyter+ :output-dir ./jupyter_images

* DONE tweak the makefile a bit
CLOSED: [2018-03-30 Fri 17:18]
- Note taken on [2018-03-30 Fri 17:13] \\
  okay, sweet.  don’t have any duplication of image files or anything, but it’s still relatively easy to control where `make hist' looks for the images, and the same for `make masks'.  renaming folders is a *bitch* but I think that’s just a fact of life … people shouldn’t do that.
  
  it’s actually not that bad to handle, just nuke the various _images and _masks files and remake the overall /images and /masks summary files.  of course, that took a while to get to the point where it wouldn’t nuke images and try to rerecord them, but yay!  it’s there!
  
  I feel like I’m happy enough with this guy.  time to move on to running the keras network.  
  
  if we need to tweak the mask files later, or if we bump into issues using docker to parallelize, then we’ll cross that bridge when we come to it
- Note taken on [2018-03-30 Fri 16:43] \\
  the default target is a usage message, which I like
- Note taken on [2018-03-30 Fri 15:44] \\
  yup this took a good bit of work, but I’m fairly happy with it now.
  
  histograms depend on the nobg folder, but it’s more of a temporary workspace
it needs cleaner separation between the full images (with bg on) and the reference images (with bg off)

that probably needs a new variable for reference moves (and rename “moves list” to “full moves list”)

and maybe think harder about how to reuse the existing rules just with different targets.  I’m sure make can handle it, somehow.

* DONE parallelize the record avi step
CLOSED: [2018-03-31 Sat 22:08]
- Note taken on [2018-03-31 Sat 22:06] \\
  omg that was a lot easier than I expected.  also, the fucking dolphin recording is the brittlest shit known to man.  I have some stuff in my personal config that’s required, and the record_avi shell script hard-codes these three assumptions:
  
  1. the location of the dolphin executable
  2. the location of my user directory
  3. the stupid way dolphin spits out two framedump files (as of 5.0-361, sure, that’s the behavior … but 5.0 release didn’t do it that way, and who the fuck knows what it’s doing now)
this is the only thing that stops me from running make with just a bajillion cores.  I’m sure there’s some sort of chroot trickery or something I can do

* Setup

** AWS Ubuntu
This comes in four parts, because we have to drop back home in the middle.

Update the property values when the target remote changes.

*** Quick remote setup
:PROPERTIES:
:header-args:shell: :dir /ssh:amazon-west:
:END:

#+BEGIN_SRC shell
  mkdir bin
  mkdir -p .emacs.d/server
#+END_SRC

#+RESULTS:

*** Quick local push
:PROPERTIES:
:header-args:shell: :var remote="amazon-west"
:END:

#+BEGIN_SRC shell
  cat ~/bin/ec | sed -e "s/amazon/${remote}/" | ssh ${remote} "cat >~/bin/ec"
  scp ~/.emacs.d/server/server ${remote}:.emacs.d/server
  scp ~/.ssh/id_rsa ${remote}:.ssh
  scp ~/.ssh/id_rsa.pub ${remote}:.ssh
  scp ~/.gitconfig ${remote}:
#+END_SRC

#+RESULTS:

*** Final remote setup
:PROPERTIES:
:header-args:shell: :dir /ssh:amazon-west:
:END:

#+BEGIN_SRC shell :results silent
  chmod +x ~/bin/ec

  sudo add-apt-repository ppa:dolphin-emu/ppa -y >/dev/null
  sudo apt update >/dev/null

  git clone https://github.com/tmurph/datasci-smash
  cd datasci-smash
  xargs -a ubuntu-packages.txt sudo apt install -y >/dev/null
  pip3 install -r requirements.txt --user >/dev/null
#+END_SRC

#+RESULTS:

then this is for convenience

#+BEGIN_SRC shell :results silent
  sudo apt install emacs24-nox -y >/dev/null

  cd ~
  rm -rf datasci-smash

  echo 'github.com ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAq2A7hRGmdnm9tUDbO9IDSwBK6TbQa+PXYPCPy6rbTrTtw7PHkccKrpp0yVhp5HdEIcKr6pLlVDBfOLX9QUsyCOV0wzfjIJNlGEYsdlLJizHhbn2mUjvSAHQqZETYP81eFzLQNnPHt4EVVUh7VfDESU84KezmD5QlWpXLmvU31/yMf+Se8xhHTvKSCZIFImWwoG6mbUoWf9nzpIoaSjB+weqqUUmpaaasXVal72J+UX2B+2RPW3RcT0eOzQgqlJL3RKrTJvdsjE3JEAvGq3lGHSZXy28G3skua2SmVi/w4yCE6gbODqnTWlg7+wC604ydGXA8VJiS5ap43JXiUFFAaQ==' >>~/.ssh/known_hosts 
  git clone git@github.com:tmurph/datasci-smash.git

  cd datasci-smash
  git config filter.clean_ipynb.clean scripts/ipynb_drop_output.py
  git config filter.clean_ipynb.smudge cat
  git config branch.master.rebase true

  cat >~/.bash_profile <<EOF
  export EDITOR="$HOME/bin/ec"
  export PATH="$HOME/bin:$HOME/.local/bin:$PATH"
  export DISPLAY=:0

  if [ -f "$HOME/.bashrc" ]; then
     . "$HOME/.bashrc"
  fi
  EOF
#+END_SRC

#+RESULTS:

and this is to work around copyright stuff

#+BEGIN_SRC shell
  touch ~/datasci-smash/scripts/record_avi/Super_Smash_Bros._Melee_\(v1.02\).iso
#+END_SRC

#+RESULTS:

*** Setup data
:PROPERTIES:
:header-args:shell: :dir /ssh:amazon-west:datasci-smash
:END:

#+BEGIN_SRC shell :results silent
  make dirs
#+END_SRC

#+RESULTS:

This download can take forever.

#+BEGIN_SRC shell :eval never
  aws s3 sync s3://datasci-smash/images data/images --exclude='*' --include='*jpg' &
  aws s3 sync s3://datasci-smash/masks data/masks --exclude='*' --include='*mask.jpg' &
#+END_SRC

#+RESULTS:

We can also do this faster version.

#+BEGIN_SRC shell
  cd ~/datasci-smash && aws s3 cp s3://datasci-smash/jpg-data.tar.gz - | tar -xzf - && find data -name '*jpg' -execdir touch {} +
#+END_SRC

#+RESULTS:

** Notebook LOB

#+NAME: setup_libraries
#+BEGIN_SRC jupyter
  %matplotlib inline
  import matplotlib
  import matplotlib.pyplot as plt
  from mpl_toolkits.mplot3d import Axes3D
  plt.rcParams['figure.figsize'] = [9, 8]

  import cv2
  import numpy as np
  import pandas as pd
  pd.set_option('display.width', 74)
  pd.set_option('display.max_columns', 15)
  pd.set_option('display.max_rows', 20)
#+END_SRC

#+RESULTS: setup_libraries

#+RESULTS:

** Remote Notebook Setup
:PROPERTIES:
:header-args:jupyter+: :existing kernel-b424b8e2-aebd-4f06-b599-1160299b1881.json
:header-args:jupyter+: :ssh amazon-west
:header-args:jupyter+: :session smash-remote
:END:

#+NAME: get_remote_kernel_name
#+HEADER: :var remote="amazon-west"
#+BEGIN_SRC shell
  ssh "${remote}" 'find /run/user/1000/jupyter -name kernel\* -execdir basename -a {} +'
#+END_SRC

#+RESULTS: get_remote_kernel_name
: kernel-b424b8e2-aebd-4f06-b599-1160299b1881.json

#+NAME: get_remote_kernel_json
#+HEADER: :var remote="amazon-west"
#+BEGIN_SRC shell
  ssh "${remote}" 'find /run/user/1000/jupyter -name kernel\*'
#+END_SRC

#+RESULTS: get_remote_kernel_json
: /run/user/1000/jupyter/kernel-b424b8e2-aebd-4f06-b599-1160299b1881.json

#+NAME: copy_remote_kernel_json
#+HEADER: :var remote_file=get_remote_kernel_json(remote="amazon-west")
#+HEADER: :var remote="amazon-west"
#+BEGIN_SRC shell 
  scp "${remote}:${remote_file}" ~/Library/Jupyter/runtime/$(basename "${remote_file}")
  find ~/Library/Jupyter/runtime -name 'kernel*' -execdir basename -a {} +
#+END_SRC

#+RESULTS: copy_remote_kernel_json
| kernel-28d1e7fe-a35d-44d5-95a6-7d98eb62b071.json |
| kernel-930257cb-ef69-4b9c-8a83-b1efd9f7e985.json |
| kernel-9e3bbebe-fa1c-4ab5-b1ed-857b240cd8d7.json |
| kernel-b297ae86-4dd7-4306-ac76-40a58503bab8.json |
| kernel-b424b8e2-aebd-4f06-b599-1160299b1881.json |
| kernel-bcc7f36c-117a-4af3-9c4e-b4051f4fabab.json |
| kernel-e998e99b-a28a-45e2-957b-20397a6b678c.json |

#+BEGIN_SRC jupyter 
"trivial setup complete"
#+END_SRC

#+RESULTS:
: trivial setup complete

* One Image Prototype
** Prior Art

#+BEGIN_SRC jupyter
  character = 'samus'
  color = 4
  stage = 'battlefield'
  orientation = 'right'
  img_number = 3
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter
  image_dir = './images/'
  base = f'{character}_{color}_{stage}_{orientation}_bg_on_{img_number:03d}.jpg'
  nobg = f'{character}_{color}_{stage}_{orientation}_bg_off_{img_number:03d}.jpg'
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter
  bgr_base = cv2.imread(image_dir + base)
  bgr_nobg = cv2.imread(image_dir + nobg)

  rgb_base = cv2.cvtColor(bgr_base, cv2.COLOR_BGR2RGB)
  rgb_nobg = cv2.cvtColor(bgr_nobg, cv2.COLOR_BGR2RGB)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter :results file
  fig, (ax1, ax2) = plt.subplots(1, 2)
  ax1.imshow(rgb_base)
  ax2.imshow(rgb_nobg)
#+END_SRC

#+RESULTS:
[[file:./jupyter_images/93298Tod.png]]

** Convenience Function

#+BEGIN_SRC jupyter
  def plot_background_images(character, color, stage, orientation,
                             img_number):
      "Create two side-by-side plots of a scenario, with bg ON and bg OFF."
      image_dir = './images/'
      base = f'{character}_{color}_{stage}_{orientation}_bg_on_{img_number:03d}.jpg'
      nobg = f'{character}_{color}_{stage}_{orientation}_bg_off_{img_number:03d}.jpg'

      bgr_base = cv2.imread(image_dir + base)
      bgr_nobg = cv2.imread(image_dir + nobg)

      rgb_base = cv2.cvtColor(bgr_base, cv2.COLOR_BGR2RGB)
      rgb_nobg = cv2.cvtColor(bgr_nobg, cv2.COLOR_BGR2RGB)

      fig, (ax1, ax2) = plt.subplots(1, 2)
      ax1.imshow(rgb_base)
      ax2.imshow(rgb_nobg)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter :results file
  plot_background_images('samus', 2, 'fountain', 'right', 305)
#+END_SRC

#+RESULTS:
[[file:./jupyter_images/93298gyj.png]]

* Localization by Histogram

** DONE why do we make reference histograms instead of using all the histogram data?
CLOSED: [2018-03-05 Mon 14:29]
I’m pretty sure this is because there’s so much noise in the full data.  It was easier to make reference data (even though I couldn’t make that work in the makefile) and just use that for the segmentation / localization.

** DONE add makefile for reference data
CLOSED: [2018-03-17 Sat 19:21]
- Note taken on [2018-03-17 Sat 19:21] \\
  this was actually easy, just set up a MOVES_LIST variable in the Makefile and away we go
we need to generate this stuff for all the characters and all the colors

there’s some issue where it’s so close to the basic data, but just different enough

** Reference Histograms
We’ve got a lot of reference data.
#+BEGIN_SRC jupyter
  df = pd.read_csv('./reference_data/hist.csv')
  cha_col_df = df.drop(['stage', 'orientation', 'number'], axis=1)
  h_cols = cha_col_df.columns.str.contains('H')
  h_total = cha_col_df.iloc[:, h_cols].sum(axis=1)

  agg_df = cha_col_df.loc[h_total > 0]
  agg_total = h_total.loc[h_total > 0]

  agg_df.iloc[:, h_cols] = agg_df.iloc[:, h_cols].divide(agg_total, axis='index')
  agg_df = agg_df.groupby(['character', 'color']).agg(np.mean)
#+END_SRC

#+RESULTS:

*** Exploratory Analysis
:PROPERTIES:
:header-args:jupyter+: :exports none
:END:
Need to find a way to cut off the histograms for backpropagation purposes.

We can plot the summary histograms.
#+BEGIN_SRC jupyter :results file
  indices = agg_df.index.levels
  char_index = indices[0]
  color_index = indices[1]

  fig, axarr = plt.subplots(len(char_index), len(color_index),
                            sharey=True)
  char = 'samus'
  # for i, char in enumerate(char_index):
  for j, color in enumerate(color_index):
      if j == 0:
          axarr[j].set_ylabel(char, rotation=0, size='large')
      float_hist = agg_df.loc[char, color].values.reshape(180, 1)
      norm_hist = cv2.normalize(float_hist, float_hist, 0, 255,
                                  cv2.NORM_MINMAX, cv2.CV_8U)
      # axarr[i, j].plot(norm_hist)
      axarr[j].plot(norm_hist)

  fig.tight_layout()
#+END_SRC

#+RESULTS:
[[file:./jupyter_images/58906J8V.png]]

Just from visual inspection, we can see spikes that mark the most character-defining hues.  So let’s pick thresholding values that cut off just the tops of those peaks.

These are the thresholds I found by eyeballing.
#+BEGIN_SRC jupyter
  eyeball_thresh_dict = {0: 27, 1: 11, 2: 12, 3: 17, 4: 18}
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter :results file
  indices = agg_df.index.levels
  char_index = indices[0]
  color_index = indices[1]

  fig, axarr = plt.subplots(len(char_index), len(color_index),
                          sharey=True)
  char = 'samus'
  # for i, char in enumerate(char_index):
  for j, color in enumerate(color_index):
      if j == 0:
          axarr[j].set_ylabel(char, rotation=0, size='large')
      axarr[j].set_title(color)

      float_hist = agg_df.loc[char, color].values.reshape(180, 1)
      norm_hist = cv2.normalize(float_hist, float_hist, 0, 255,
                                  cv2.NORM_MINMAX, cv2.CV_8U)
      # axarr[i, j].plot(norm_hist)
      axarr[j].plot(norm_hist)
      axarr[j].axhline(eyeball_thresh_dict[color])

  fig.tight_layout()
#+END_SRC

#+RESULTS:
[[file:./jupyter_images/932986Gw.png]]

Ugh, this works, but constantly reshaping np arrays is just so annoying.
#+BEGIN_SRC jupyter
  def select_hist(character, color):
      float_hist = agg_df.loc[character, color].values.reshape(180, 1)
      norm_hist = cv2.normalize(float_hist, float_hist, 0, 255,
                                cv2.NORM_MINMAX, cv2.CV_8U)
      return norm_hist
#+END_SRC

#+RESULTS:

This feels like the worst hack, but eh, it works.
#+BEGIN_SRC jupyter
  def pick_peaks(hist):
      list_hist = hist.reshape(180)
      first = [list_hist[0] > list_hist[1]]
      middle = [list_hist[i] < e > list_hist[i+2] for i, e
                in enumerate(list_hist[1:-1])]
      last = [list_hist[-2] < list_hist[-1]]
      return first + middle + last
#+END_SRC

#+RESULTS:

This is easy to explain, I think, but the graph is really hard to interpret.  It’s like watching for when the noise drops out of my cutoff and I’m left with just the big peaks.
#+BEGIN_SRC jupyter
  def count_peaks(hist):
      arr = np.zeros(255, dtype=np.uint8)
      peaks = pick_peaks(hist)
      for x in range(hist.max()):
          arr[x] = sum(abovep and peakp for abovep, peakp in zip(hist > x, peaks))
      return arr
#+END_SRC

#+RESULTS:

Kinda makes me wonder … if I could write code that sliced off the top 25% of the area under the histogram, would that work?  But JFC I don’t want to write that code.

And this is the hackiest way to find a transition from “noisy peaks” to “big peaks”.  When does my cutoff line get past the noise of closely bunched together peaks and into the signal of distantly removed peaks?
#+BEGIN_SRC jupyter
  from itertools import groupby

  def first_peak_run(hist, min_length=4):
      peaks = count_peaks(hist)
      for k, g in groupby(peaks):
          if len(list(g)) >= min_length:
              peak_count = k
              break
      for i, e in enumerate(peaks):
          if e == peak_count:
              first_index = i
              break
      return first_index
#+END_SRC

#+RESULTS:

And these are the thresholds from my code.

#+BEGIN_SRC jupyter :results verbatim
  calc_thresh_dict = {n: first_peak_run(select_hist('samus', n))
                      for n in range(5)}
  calc_thresh_dict
#+END_SRC

#+RESULTS:
: {0: 10, 1: 12, 2: 11, 3: 14, 4: 27}

Here’s what the thresholds look like, side by side.
#+BEGIN_SRC jupyter :results file
  indices = agg_df.index.levels
  char_index = indices[0]
  color_index = indices[1]

  fig, axarr = plt.subplots(len(char_index), len(color_index),
                          sharey=True)
  char = 'samus'
  # for i, char in enumerate(char_index):
  for j, color in enumerate(color_index):
      if j == 0:
          axarr[j].set_ylabel(char, rotation=0, size='large')
      axarr[j].set_title(color)

      float_hist = agg_df.loc[char, color].values.reshape(180, 1)
      norm_hist = cv2.normalize(float_hist, float_hist, 0, 255,
                                  cv2.NORM_MINMAX, cv2.CV_8U)
      axarr[j].plot(norm_hist)
      axarr[j].axhline(eyeball_thresh_dict[color], color='b')
      axarr[j].axhline(calc_thresh_dict[color], color='r')

  fig.tight_layout()
#+END_SRC

#+RESULTS:
[[file:./jupyter_images/58906HUj.png]]

*** God Dammit the Peak Height Heuristic kinda fails

Check this out.

#+BEGIN_SRC jupyter
  df = pd.read_csv('reference_data/samus_2_fountain_left_bg_off_hist.csv')

  cha_col_df = df.drop(['stage', 'orientation', 'number'], axis=1)
  h_cols = cha_col_df.columns.str.contains('H')
  h_total = cha_col_df.iloc[:, h_cols].sum(axis=1)

  cha_col_df = cha_col_df.loc[h_total > 0]
  h_total = h_total.loc[h_total > 0]

  cha_col_df.iloc[:, h_cols] = cha_col_df.iloc[:, h_cols].divide(h_total, axis='index')
  agg_df = cha_col_df.groupby(['character', 'color']).agg(np.mean)

  float_hist = agg_df.loc['samus', 2].values
  float_hist = float_hist.reshape(len(float_hist), 1)
  back_proj_hist = cv2.normalize(float_hist, float_hist,
                                 alpha=0, beta=int8_max,
                                 norm_type=cv2.NORM_MINMAX,
                                 dtype=cv2.CV_8U)
  back_proj_hist = back_proj_hist.reshape(len(back_proj_hist))
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter :results file
  plt.plot(back_proj_hist)
#+END_SRC

#+RESULTS:
[[file:./jupyter_images/589061ID.png]]

Tough to eyeball the problem buuuuut the first peak here is actually 4 units high.  Our heuristic of “climb up until you get to a peak 4 higher than the last one” doesn’t really work.  We could probably tweak it, like “ignore the first peak” but seriously this is starting to smell like real hack work.

Let’s try cutting off the top X% of the histogram.

#+BEGIN_SRC jupyter
  def cutoff_thresh(hist, percentage=0.25):
      "Determine a threshold to select the top percentage of HIST."
      total_area = hist.sum()
      result = 0
      for x in range(hist.max(), 0, -1):
          cutoff_area = sum(map(lambda e: max(e - x, 0), hist))
          if cutoff_area > total_area * percentage:
              result = x
              break
      return result
#+END_SRC

#+RESULTS:

Nope, that’s shitty, there’s no real good correspondence with the stuff I found earlier.
#+NAME: perc_thresh_code
#+BEGIN_SRC jupyter :var perc=0.55 :results verbatim
  perc_thresh_dict = {n: cutoff_thresh(select_hist('samus', n).reshape(180),
                                       percentage=perc) for n in range(5)}
  perc_thresh_dict
#+END_SRC

#+RESULTS:
: {0: 22, 1: 14, 2: 11, 3: 16, 4: 15}

#+BEGIN_SRC jupyter :results file
  indices = agg_df.index.levels
  char_index = indices[0]
  color_index = indices[1]

  fig, axarr = plt.subplots(len(char_index), len(color_index),
                          sharey=True)
  char = 'samus'
  # for i, char in enumerate(char_index):
  for j, color in enumerate(color_index):
      if j == 0:
          axarr[j].set_ylabel(char, rotation=0, size='large')
      axarr[j].set_title(color)

      float_hist = agg_df.loc[char, color].values.reshape(180, 1)
      norm_hist = cv2.normalize(float_hist, float_hist, 0, 255,
                                  cv2.NORM_MINMAX, cv2.CV_8U)
      axarr[j].plot(norm_hist)
      axarr[j].axhline(perc_thresh_dict[color], color='b')
      axarr[j].axhline(calc_thresh_dict[color], color='r')

  fig.tight_layout()
#+END_SRC

#+RESULTS:
[[file:./jupyter_images/58906GEz.png]]

Hmm but actually maybe that’s not so bad?

** Object Segmentation and Localization

So let’s now see how we can do with object localization / segmentation.  That is to say, how well can we come up with bounding boxes / object masks.  There are two ways to segment: just straight masking of the images; and hue backprojection plus thresholding.  Let’s take a look at some random examples, find some edge cases, and move on.

#+BEGIN_SRC jupyter
  def four_images(character, color, stage, orientation, img_number,
                  agg_df, thresh_dict):

      img_name_on = f'./images/{character}_{color}_{stage}_{orientation}_bg_on_{img_number:03d}.jpg'
      rgb_img = cv2.cvtColor(cv2.imread(img_name_on), cv2.COLOR_BGR2RGB)

      img_name_off = f'./images/{character}_{color}_{stage}_{orientation}_bg_off_{img_number:03d}.jpg'
      hsv_img = cv2.cvtColor(cv2.imread(img_name_off), cv2.COLOR_BGR2HSV)
      hsv_mask = cv2.inRange(hsv_img, np.array([0, 50, 50]),
                          np.array([179, 255, 255]))
      h_img = hsv_img[:, :, 0]
      h_hist = cv2.calcHist([h_img], [0], hsv_mask, [180], [0, 180])

      target_hist = agg_df.loc[character, color].values.reshape(180, 1)
      norm_hist = cv2.normalize(target_hist, target_hist, 0, 255,
                              cv2.NORM_MINMAX, cv2.CV_8U)
      back_proj = norm_hist.flatten()[h_img.ravel()].reshape(h_img.shape)
      back_proj &= hsv_mask

      _, thresh_img = cv2.threshold(back_proj, thresh_dict[color],
                                    255, cv2.THRESH_BINARY)

      blur_img = cv2.GaussianBlur(thresh_img, (11, 11), 0)
      _, contours, _ = cv2.findContours(blur_img, cv2.RETR_EXTERNAL,
                                      cv2.CHAIN_APPROX_SIMPLE)
      contours = sorted(contours, key=cv2.contourArea, reverse=True)
      if contours:
          large_contour = contours[0]
          x, y, w, h = cv2.boundingRect(large_contour)
          final_img = cv2.rectangle(rgb_img.copy(), (x, y),
                                    (x + w, y + h), 255, 3)
          final_mask = cv2.rectangle(np.zeros(blur_img.shape,
                                              dtype=np.uint8),
                                     (x, y), (x + w, y + h), 255, -1)
          final_mask &= blur_img
      else:
          final_img = rgb_img
          final_mask = np.zeros(blur_img.shape)

      return hsv_mask, thresh_img, final_img, final_mask
#+END_SRC

#+RESULTS:

I think I actually may get better results from including just the tip of the last noisy peak?
#+NAME: calc_thresh_code
#+BEGIN_SRC jupyter :results verbatim
  calc_thresh_dict = {n: first_peak_run(select_hist('samus', n)) - 1
                      for n in range(5)}
  calc_thresh_dict
#+END_SRC

#+RESULTS:
: {0: 9, 1: 11, 2: 10, 3: 13, 4: 26}

#+BEGIN_SRC jupyter :results file
  indices = agg_df.index.levels
  char_index = indices[0]
  color_index = indices[1]

  nrows = 4
  ncols = 5

  fig, axarr = plt.subplots(nrows, ncols, sharey=True)

  for i in range(ncols):
      char = 'samus'
      color = np.random.choice(5)
      stage = np.random.choice(['final', 'fountain', 'stadium', 'story',
                              'battlefield', 'dreamland'])
      orientation = np.random.choice(['left', 'right'])
      img_number = np.random.randint(300)

      mask, thresh, final, segment = four_images(char, color, stage,
                                                 orientation,
                                                 img_number,
                                                 agg_df,
                                                 perc_thresh_dict)

      axarr[0, i].imshow(mask, cmap='gray')
      axarr[1, i].imshow(thresh, cmap='gray')
      axarr[2, i].imshow(final)
      axarr[3, i].imshow(segment, cmap='gray')
#+END_SRC

#+RESULTS:
[[file:./jupyter_images/58906GSb.png]]

This produces some really decent localization annotations.  And when we intersect the hue mask with the bounding box we get some decent segmentation.

There are definitely some bullshit edge cases.  Samus has a lot of yellow and red in her costume.  She also makes a lot of fire (yellow/red/orange) that gets found as a false positive.  In extreme cases, like when she’s curled in a ball above an exploding bomb, the bomb explosion is found but nothing of her.  Additionally, Yoshi’s Story has these red shy guys that are frequent false-positives.

Part of me says, go ahead and train the fucking Keras model.  If / when it gets shitty results, come back and clean the data some more.

*** Does it work with the reference data?
We got in trouble earlier trying to use the thresholding algorithm with a different data set.  Is the percentile algorithm better?

#+BEGIN_SRC jupyter
  def read_agg_df(path):
      df = pd.read_csv(path)

      cha_col_df = df.drop(['stage', 'orientation', 'number'], axis=1)
      h_cols = cha_col_df.columns.str.contains('H')
      h_total = cha_col_df.iloc[:, h_cols].sum(axis=1)

      cha_col_df = cha_col_df.loc[h_total > 0]
      h_total = h_total.loc[h_total > 0]

      cha_col_df.iloc[:, h_cols] = cha_col_df.iloc[:, h_cols].divide(h_total, axis='index')
      agg_df = cha_col_df.groupby(['character', 'color']).agg(np.mean)
      return agg_df
#+END_SRC

#+RESULTS:

#+NAME: set_agg_df
#+HEADER: :var path="reference_data/samus_2_fountain_left_bg_off_hist.csv"
#+BEGIN_SRC jupyter
  agg_df = read_agg_df(path)
#+END_SRC

#+RESULTS: set_agg_df

And let’s redefine that histogram function to be more functional, less global variable.

#+BEGIN_SRC jupyter
  def select_hist(agg_df, character, color):
      float_hist = agg_df.loc[character, color].values.reshape(180, 1)
      norm_hist = cv2.normalize(float_hist, float_hist, 0, 255,
                                cv2.NORM_MINMAX, cv2.CV_8U)
      return norm_hist.reshape(180)
#+END_SRC

#+RESULTS:

Check it works.

#+BEGIN_SRC jupyter
  cutoff_thresh(select_hist(agg_df, 'samus', 2))
#+END_SRC

#+RESULTS:
: 40

Okay so that’s a number, but where is it on the histogram?

#+BEGIN_SRC jupyter :results file
  plt.plot(select_hist(agg_df, 'samus', 2))
  plt.axhline(cutoff_thresh(select_hist(agg_df, 'samus', 2)))
#+END_SRC

#+RESULTS:
[[file:./jupyter_images/589060Tv.png]]

Eh?  Looks okay?  How does it perform on my data set?

Gotta get all the reference histograms.

#+CALL: set_agg_df(path="reference_data/hist.csv")

#+RESULTS:

#+NAME: set_perc_thresh
#+BEGIN_SRC jupyter :var perc=0.55 :results verbatim
  perc_thresh_dict = {n: cutoff_thresh(select_hist(agg_df, 'samus', n),
                                       percentage=perc) for n in range(5)}
  perc_thresh_dict
#+END_SRC

#+RESULTS: set_perc_thresh
: {0: 22, 1: 14, 2: 11, 3: 16, 4: 15}

Let’s make that image code reusable.

#+NAME: make_segmented_images
#+HEADER: :var dictionary="perc_thresh_dict"
#+BEGIN_SRC jupyter :results file
  indices = agg_df.index.levels
  char_index = indices[0]
  color_index = indices[1]

  nrows = 4
  ncols = 5

  fig, axarr = plt.subplots(nrows, ncols, sharey=True)

  for i in range(ncols):
      char = 'samus'
      color = np.random.choice(5)
      stage = np.random.choice(['final', 'fountain', 'stadium', 'story',
                              'battlefield', 'dreamland'])
      orientation = np.random.choice(['left', 'right'])
      img_number = np.random.randint(300)

      mask, thresh, final, segment = four_images(char, color, stage,
                                                 orientation,
                                                 img_number,
                                                 agg_df,
                                                 eval(dictionary))

      axarr[0, i].imshow(mask, cmap='gray')
      axarr[1, i].imshow(thresh, cmap='gray')
      axarr[2, i].imshow(final)
      axarr[3, i].imshow(segment, cmap='gray')
#+END_SRC

#+CALL: make_segmented_images(dictionary="perc_thresh_dict")

#+RESULTS:
[[file:./jupyter_images/58906bAq.png]]

Not bad.  I like how it’s better at picking up Samus’ gun.  Still got those shy guy and fire problems.

What about different thresholds?

#+CALL: set_perc_thresh(perc=0.55)

#+RESULTS:
: {0: 22, 1: 14, 2: 11, 3: 16, 4: 15}

#+CALL: make_segmented_images(dictionary="perc_thresh_dict")

#+RESULTS:
[[file:./jupyter_images/58906Pw2.png]]

I can’t really tell a difference from eyeballing.  Obvi a lower percentage / higher cutoff produces grainier / more disjointed masks.  Let’s just arbitrarily pick 55% as our threshold.

Yeah, fine, works with ref data.

* Neural Network Training
The hard part!  The fun part!  The part where I need a mentor!  Let’s do it!

** Follow a Keras tutorial
I like https://www.kaggle.com/xingyang/show-me-the-fishes-object-localization-with-cnn
 for the code, though it references [[file:~/Code/deepsense-whales][file:~/Code/deepsense-whales]] too.

#+NAME: fishes-localization
#+BEGIN_SRC jupyter :eval never :tangle fishes-localization.py
  """
      In order to run this script, you need to download the annotation files from https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring/discussion/25902 and modify the DATASET_FOLDER_PATH variable. The script has been tested on Python 3.6 with latest packages. You might need to modify the script because of the possible compatibility issues.
      The localization algorithm implemented here could achieve satisfactory results on the testing dataset. To further improve the performance, you may find the following links useful.
      https://deepsense.io/deep-learning-right-whale-recognition-kaggle/
      http://felixlaumon.github.io/2015/01/08/kaggle-right-whale.html
      https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html
  """
  assert False

  import matplotlib
  matplotlib.use("Agg")

  import os
  import glob
  import shutil
  import json
  import pylab
  import numpy as np
  from keras.applications.vgg16 import VGG16
  from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint
  from keras.layers import Dense, Dropout, Flatten, Input
  from keras.layers.normalization import BatchNormalization
  from keras.models import Model
  from keras.optimizers import Adam
  from keras.preprocessing.image import ImageDataGenerator
  from keras.utils.visualize_util import plot
  from scipy.misc import imread, imsave, imresize
  from sklearn.cluster import DBSCAN
  from sklearn.model_selection import GroupShuffleSplit

  # Dataset
  DATASET_FOLDER_PATH = os.path.join(os.path.expanduser("~"), "Documents/Dataset/The Nature Conservancy Fisheries Monitoring")
  TRAIN_FOLDER_PATH = os.path.join(DATASET_FOLDER_PATH, "train")
  TEST_FOLDER_PATH = os.path.join(DATASET_FOLDER_PATH, "test_stg1")
  LOCALIZATION_FOLDER_PATH = os.path.join(DATASET_FOLDER_PATH, "localization")
  ANNOTATION_FOLDER_PATH = os.path.join(DATASET_FOLDER_PATH, "annotations")
  CLUSTERING_RESULT_FILE_PATH = os.path.join(DATASET_FOLDER_PATH, "clustering_result.npy")

  # Workspace
  WORKSPACE_FOLDER_PATH = os.path.join("/tmp", os.path.basename(DATASET_FOLDER_PATH))
  CLUSTERING_FOLDER_PATH = os.path.join(WORKSPACE_FOLDER_PATH, "clustering")
  ACTUAL_DATASET_FOLDER_PATH = os.path.join(WORKSPACE_FOLDER_PATH, "actual_dataset")
  ACTUAL_TRAIN_ORIGINAL_FOLDER_PATH = os.path.join(ACTUAL_DATASET_FOLDER_PATH, "train_original")
  ACTUAL_VALID_ORIGINAL_FOLDER_PATH = os.path.join(ACTUAL_DATASET_FOLDER_PATH, "valid_original")
  ACTUAL_TRAIN_LOCALIZATION_FOLDER_PATH = os.path.join(ACTUAL_DATASET_FOLDER_PATH, "train_localization")
  ACTUAL_VALID_LOCALIZATION_FOLDER_PATH = os.path.join(ACTUAL_DATASET_FOLDER_PATH, "valid_localization")

  # Output
  OUTPUT_FOLDER_PATH = os.path.join(DATASET_FOLDER_PATH, "{}_output".format(os.path.basename(__file__).split(".")[0]))
  VISUALIZATION_FOLDER_PATH = os.path.join(OUTPUT_FOLDER_PATH, "Visualization")
  OPTIMAL_WEIGHTS_FOLDER_PATH = os.path.join(OUTPUT_FOLDER_PATH, "Optimal Weights")
  OPTIMAL_WEIGHTS_FILE_RULE = os.path.join(OPTIMAL_WEIGHTS_FOLDER_PATH, "epoch_{epoch:03d}-loss_{loss:.5f}-val_loss_{val_loss:.5f}.h5")

  # Image processing
  IMAGE_ROW_SIZE = 256
  IMAGE_COLUMN_SIZE = 256

  # Training and Testing procedure
  MAXIMUM_EPOCH_NUM = 1000
  PATIENCE = 100
  BATCH_SIZE = 32
  INSPECT_SIZE = 4

  def reformat_testing_dataset():
      # Create a dummy folder
      dummy_test_folder_path = os.path.join(TEST_FOLDER_PATH, "dummy")
      os.makedirs(dummy_test_folder_path, exist_ok=True)

      # Move files to the dummy folder if needed
      file_path_list = glob.glob(os.path.join(TEST_FOLDER_PATH, "*"))
      for file_path in file_path_list:
          if os.path.isfile(file_path):
              shutil.move(file_path, os.path.join(dummy_test_folder_path, os.path.basename(file_path)))

  def load_annotation():
      annotation_dict = {}
      annotation_file_path_list = glob.glob(os.path.join(ANNOTATION_FOLDER_PATH, "*.json"))
      for annotation_file_path in annotation_file_path_list:
          with open(annotation_file_path) as annotation_file:
              annotation_file_content = json.load(annotation_file)
              for item in annotation_file_content:
                  key = os.path.basename(item["filename"])
                  if key in annotation_dict:
                      assert False, "Found existing key {}!!!".format(key)
                  value = []
                  for annotation in item["annotations"]:
                      value.append(np.clip((annotation["x"], annotation["width"], annotation["y"], annotation["height"]), 0, np.inf).astype(np.int))
                  annotation_dict[key] = value
      return annotation_dict

  def reformat_localization():
      print("Creating the localization folder ...")
      os.makedirs(LOCALIZATION_FOLDER_PATH, exist_ok=True)

      print("Loading annotation ...")
      annotation_dict = load_annotation()

      original_image_path_list = glob.glob(os.path.join(TRAIN_FOLDER_PATH, "*/*"))
      for original_image_path in original_image_path_list:
          localization_image_path = LOCALIZATION_FOLDER_PATH + original_image_path[len(TRAIN_FOLDER_PATH):]
          if os.path.isfile(localization_image_path):
              continue

          localization_image_content = np.zeros(imread(original_image_path).shape[:2], dtype=np.uint8)
          for annotation_x, annotation_width, annotation_y, annotation_height in annotation_dict.get(os.path.basename(original_image_path), []):
              localization_image_content[annotation_y:annotation_y + annotation_height, annotation_x:annotation_x + annotation_width] = 255

          os.makedirs(os.path.abspath(os.path.join(localization_image_path, os.pardir)), exist_ok=True)
          imsave(localization_image_path, localization_image_content)

  def perform_CV(image_path_list, resized_image_row_size=64, resized_image_column_size=64):
      if os.path.isfile(CLUSTERING_RESULT_FILE_PATH):
          print("Loading clustering result ...")
          image_name_to_cluster_ID_array = np.load(CLUSTERING_RESULT_FILE_PATH)
          image_name_to_cluster_ID_dict = dict(image_name_to_cluster_ID_array)
          cluster_ID_array = np.array([image_name_to_cluster_ID_dict[os.path.basename(image_path)] for image_path in image_path_list], dtype=np.int)
      else:
          print("Reading image content ...")
          image_content_array = np.array([imresize(imread(image_path), (resized_image_row_size, resized_image_column_size)) for image_path in image_path_list])
          image_content_array = np.reshape(image_content_array, (len(image_content_array), -1))
          image_content_array = np.array([(image_content - image_content.mean()) / image_content.std() for image_content in image_content_array], dtype=np.float32)

          print("Apply clustering ...")
          cluster_ID_array = DBSCAN(eps=1.5 * resized_image_row_size * resized_image_column_size, min_samples=20, metric="l1", n_jobs=-1).fit_predict(image_content_array)

          print("Saving clustering result ...")
          image_name_to_cluster_ID_array = np.transpose(np.vstack(([os.path.basename(image_path) for image_path in image_path_list], cluster_ID_array)))
          np.save(CLUSTERING_RESULT_FILE_PATH, image_name_to_cluster_ID_array)

      print("The ID value and count are as follows:")
      cluster_ID_values, cluster_ID_counts = np.unique(cluster_ID_array, return_counts=True)
      for cluster_ID_value, cluster_ID_count in zip(cluster_ID_values, cluster_ID_counts):
          print("{}\t{}".format(cluster_ID_value, cluster_ID_count))

      print("Visualizing clustering result ...")
      shutil.rmtree(CLUSTERING_FOLDER_PATH, ignore_errors=True)
      for image_path, cluster_ID in zip(image_path_list, cluster_ID_array):
          sub_clustering_folder_path = os.path.join(CLUSTERING_FOLDER_PATH, str(cluster_ID))
          if not os.path.isdir(sub_clustering_folder_path):
              os.makedirs(sub_clustering_folder_path)
          os.symlink(image_path, os.path.join(sub_clustering_folder_path, os.path.basename(image_path)))

      cv_object = GroupShuffleSplit(n_splits=100, test_size=0.2, random_state=0)
      for cv_index, (train_index_array, valid_index_array) in enumerate(cv_object.split(X=np.zeros((len(cluster_ID_array), 1)), groups=cluster_ID_array), start=1):
          print("Checking cv {} ...".format(cv_index))
          valid_sample_ratio = len(valid_index_array) / (len(train_index_array) + len(valid_index_array))
          if -1 in np.unique(cluster_ID_array[train_index_array]) and valid_sample_ratio > 0.15 and valid_sample_ratio < 0.25:
              train_unique_label, train_unique_counts = np.unique([image_path.split("/")[-2] for image_path in np.array(image_path_list)[train_index_array]], return_counts=True)
              valid_unique_label, valid_unique_counts = np.unique([image_path.split("/")[-2] for image_path in np.array(image_path_list)[valid_index_array]], return_counts=True)
              if np.array_equal(train_unique_label, valid_unique_label):
                  train_unique_ratio = train_unique_counts / np.sum(train_unique_counts)
                  valid_unique_ratio = valid_unique_counts / np.sum(valid_unique_counts)
                  print("Using {:.2f}% original training samples as validation samples ...".format(valid_sample_ratio * 100))
                  print("For training samples: {}".format(train_unique_ratio))
                  print("For validation samples: {}".format(valid_unique_ratio))
                  return train_index_array, valid_index_array

      assert False

  def reorganize_dataset():
      # Get list of files
      original_image_path_list = sorted(glob.glob(os.path.join(TRAIN_FOLDER_PATH, "*/*")))
      localization_image_path_list = sorted(glob.glob(os.path.join(LOCALIZATION_FOLDER_PATH, "*/*")))

      # Sanity check
      original_image_name_list = [os.path.basename(image_path) for image_path in original_image_path_list]
      localization_image_name_list = [os.path.basename(image_path) for image_path in localization_image_path_list]
      assert np.array_equal(original_image_name_list, localization_image_name_list)

      # Perform Cross Validation
      train_index_array, valid_index_array = perform_CV(original_image_path_list)

      # Create symbolic links
      shutil.rmtree(ACTUAL_DATASET_FOLDER_PATH, ignore_errors=True)
      for (actual_original_folder_path, actual_localization_folder_path), index_array in zip(
              ((ACTUAL_TRAIN_ORIGINAL_FOLDER_PATH, ACTUAL_TRAIN_LOCALIZATION_FOLDER_PATH),
              (ACTUAL_VALID_ORIGINAL_FOLDER_PATH, ACTUAL_VALID_LOCALIZATION_FOLDER_PATH)),
              (train_index_array, valid_index_array)):
          for index_value in index_array:
              original_image_path = original_image_path_list[index_value]
              localization_image_path = localization_image_path_list[index_value]

              path_suffix = original_image_path[len(TRAIN_FOLDER_PATH):]
              assert path_suffix == localization_image_path[len(LOCALIZATION_FOLDER_PATH):]

              if path_suffix[1:].startswith("NoF"):
                  continue

              actual_original_image_path = actual_original_folder_path + path_suffix
              actual_localization_image_path = actual_localization_folder_path + path_suffix

              os.makedirs(os.path.abspath(os.path.join(actual_original_image_path, os.pardir)), exist_ok=True)
              os.makedirs(os.path.abspath(os.path.join(actual_localization_image_path, os.pardir)), exist_ok=True)

              os.symlink(original_image_path, actual_original_image_path)
              os.symlink(localization_image_path, actual_localization_image_path)

      return len(glob.glob(os.path.join(ACTUAL_TRAIN_ORIGINAL_FOLDER_PATH, "*/*"))), len(glob.glob(os.path.join(ACTUAL_VALID_ORIGINAL_FOLDER_PATH, "*/*")))

  def init_model(target_num=4, FC_block_num=2, FC_feature_dim=512, dropout_ratio=0.5, learning_rate=0.0001):
      # Get the input tensor
      input_tensor = Input(shape=(3, IMAGE_ROW_SIZE, IMAGE_COLUMN_SIZE))

      # Convolutional blocks
      pretrained_model = VGG16(include_top=False, weights="imagenet")
      for layer in pretrained_model.layers:
          layer.trainable = False
      output_tensor = pretrained_model(input_tensor)

      # FullyConnected blocks
      output_tensor = Flatten()(output_tensor)
      for _ in range(FC_block_num):
          output_tensor = Dense(FC_feature_dim, activation="relu")(output_tensor)
          output_tensor = BatchNormalization()(output_tensor)
          output_tensor = Dropout(dropout_ratio)(output_tensor)
      output_tensor = Dense(target_num, activation="sigmoid")(output_tensor)

      # Define and compile the model
      model = Model(input_tensor, output_tensor)
      model.compile(optimizer=Adam(lr=learning_rate), loss="mse")
      plot(model, to_file=os.path.join(OPTIMAL_WEIGHTS_FOLDER_PATH, "model.png"), show_shapes=True, show_layer_names=True)

      return model

  def convert_localization_to_annotation(localization_array, row_size=IMAGE_ROW_SIZE, column_size=IMAGE_COLUMN_SIZE):
      annotation_list = []
      for localization in localization_array:
          localization = localization[0]

          mask_along_row = np.max(localization, axis=1) > 0.5
          row_start_index = np.argmax(mask_along_row)
          row_end_index = len(mask_along_row) - np.argmax(np.flipud(mask_along_row)) - 1

          mask_along_column = np.max(localization, axis=0) > 0.5
          column_start_index = np.argmax(mask_along_column)
          column_end_index = len(mask_along_column) - np.argmax(np.flipud(mask_along_column)) - 1

          annotation = (row_start_index / row_size, (row_end_index - row_start_index) / row_size, column_start_index / column_size, (column_end_index - column_start_index) / column_size)
          annotation_list.append(annotation)

      return np.array(annotation_list).astype(np.float32)

  def convert_annotation_to_localization(annotation_array, row_size=IMAGE_ROW_SIZE, column_size=IMAGE_COLUMN_SIZE):
      localization_list = []
      for annotation in annotation_array:
          localization = np.zeros((row_size, column_size))

          row_start_index = np.max((0, int(annotation[0] * row_size)))
          row_end_index = np.min((row_start_index + int(annotation[1] * row_size), row_size - 1))

          column_start_index = np.max((0, int(annotation[2] * column_size)))
          column_end_index = np.min((column_start_index + int(annotation[3] * column_size), column_size - 1))

          localization[row_start_index:row_end_index + 1, column_start_index:column_end_index + 1] = 1
          localization_list.append(np.expand_dims(localization, axis=0))

      return np.array(localization_list).astype(np.float32)

  def load_dataset(folder_path_list, color_mode_list, batch_size, classes=None, class_mode=None, shuffle=True, seed=None, apply_conversion=False):
      # Get the generator of the dataset
      data_generator_list = []
      for folder_path, color_mode in zip(folder_path_list, color_mode_list):
          data_generator_object = ImageDataGenerator(
              rotation_range=10,
              width_shift_range=0.05,
              height_shift_range=0.05,
              shear_range=0.05,
              zoom_range=0.2,
              horizontal_flip=True,
              rescale=1.0 / 255)
          data_generator = data_generator_object.flow_from_directory(
              directory=folder_path,
              target_size=(IMAGE_ROW_SIZE, IMAGE_COLUMN_SIZE),
              color_mode=color_mode,
              classes=classes,
              class_mode=class_mode,
              batch_size=batch_size,
              shuffle=shuffle,
              seed=seed)
          data_generator_list.append(data_generator)

      # Sanity check
      filenames_list = [data_generator.filenames for data_generator in data_generator_list]
      assert all(filenames == filenames_list[0] for filenames in filenames_list)

      if apply_conversion:
          assert len(data_generator_list) == 2
          for X_array, Y_array in zip(*data_generator_list):
              yield (X_array, convert_localization_to_annotation(Y_array))
      else:
          for array_tuple in zip(*data_generator_list):
              yield array_tuple

  class InspectPrediction(Callback):
      def __init__(self, data_generator_list):
          super(InspectPrediction, self).__init__()

          self.data_generator_list = data_generator_list

      def on_epoch_end(self, epoch, logs=None):
          for data_generator_index, data_generator in enumerate(self.data_generator_list, start=1):
              X_array, GT_Y_array = next(data_generator)
              P_Y_array = convert_annotation_to_localization(self.model.predict_on_batch(X_array))

              for sample_index, (X, GT_Y, P_Y) in enumerate(zip(X_array, GT_Y_array, P_Y_array), start=1):
                  pylab.figure()
                  pylab.subplot(1, 3, 1)
                  pylab.imshow(np.rollaxis(X, 0, 3))
                  pylab.title("X")
                  pylab.axis("off")
                  pylab.subplot(1, 3, 2)
                  pylab.imshow(GT_Y[0], cmap="gray")
                  pylab.title("GT_Y")
                  pylab.axis("off")
                  pylab.subplot(1, 3, 3)
                  pylab.imshow(P_Y[0], cmap="gray")
                  pylab.title("P_Y")
                  pylab.axis("off")
                  pylab.savefig(os.path.join(VISUALIZATION_FOLDER_PATH, "Epoch_{}_Split_{}_Sample_{}.png".format(epoch + 1, data_generator_index, sample_index)))
                  pylab.close()

  class InspectLoss(Callback):
      def __init__(self):
          super(InspectLoss, self).__init__()

          self.train_loss_list = []
          self.valid_loss_list = []

      def on_epoch_end(self, epoch, logs=None):
          train_loss = logs.get("loss")
          valid_loss = logs.get("val_loss")
          self.train_loss_list.append(train_loss)
          self.valid_loss_list.append(valid_loss)
          epoch_index_array = np.arange(len(self.train_loss_list)) + 1

          pylab.figure()
          pylab.plot(epoch_index_array, self.train_loss_list, "yellowgreen", label="train_loss")
          pylab.plot(epoch_index_array, self.valid_loss_list, "lightskyblue", label="valid_loss")
          pylab.grid()
          pylab.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=2, ncol=2, mode="expand", borderaxespad=0.)
          pylab.savefig(os.path.join(OUTPUT_FOLDER_PATH, "Loss Curve.png"))
          pylab.close()

  def run():
      print("Creating folders ...")
      os.makedirs(VISUALIZATION_FOLDER_PATH, exist_ok=True)
      os.makedirs(OPTIMAL_WEIGHTS_FOLDER_PATH, exist_ok=True)

      print("Reformatting testing dataset ...")
      reformat_testing_dataset()

      print("Reformatting localization ...")
      reformat_localization()

      print("Reorganizing dataset ...")
      train_sample_num, valid_sample_num = reorganize_dataset()

      print("Initializing model ...")
      model = init_model()

      weights_file_path_list = sorted(glob.glob(os.path.join(OPTIMAL_WEIGHTS_FOLDER_PATH, "*.h5")))
      if len(weights_file_path_list) == 0:
          print("Performing the training procedure ...")
          train_generator = load_dataset(folder_path_list=[ACTUAL_TRAIN_ORIGINAL_FOLDER_PATH, ACTUAL_TRAIN_LOCALIZATION_FOLDER_PATH], color_mode_list=["rgb", "grayscale"], batch_size=BATCH_SIZE, seed=0, apply_conversion=True)
          valid_generator = load_dataset(folder_path_list=[ACTUAL_VALID_ORIGINAL_FOLDER_PATH, ACTUAL_VALID_LOCALIZATION_FOLDER_PATH], color_mode_list=["rgb", "grayscale"], batch_size=BATCH_SIZE, seed=0, apply_conversion=True)
          train_generator_for_inspection = load_dataset(folder_path_list=[ACTUAL_TRAIN_ORIGINAL_FOLDER_PATH, ACTUAL_TRAIN_LOCALIZATION_FOLDER_PATH], color_mode_list=["rgb", "grayscale"], batch_size=INSPECT_SIZE, seed=1)
          valid_generator_for_inspection = load_dataset(folder_path_list=[ACTUAL_VALID_ORIGINAL_FOLDER_PATH, ACTUAL_VALID_LOCALIZATION_FOLDER_PATH], color_mode_list=["rgb", "grayscale"], batch_size=INSPECT_SIZE, seed=1)
          earlystopping_callback = EarlyStopping(monitor="val_loss", patience=PATIENCE)
          modelcheckpoint_callback = ModelCheckpoint(OPTIMAL_WEIGHTS_FILE_RULE, monitor="val_loss", save_best_only=True, save_weights_only=True)
          inspectprediction_callback = InspectPrediction([train_generator_for_inspection, valid_generator_for_inspection])
          inspectloss_callback = InspectLoss()
          model.fit_generator(generator=train_generator,
                              samples_per_epoch=train_sample_num,
                              validation_data=valid_generator,
                              nb_val_samples=valid_sample_num,
                              callbacks=[earlystopping_callback, modelcheckpoint_callback, inspectprediction_callback, inspectloss_callback],
                              nb_epoch=MAXIMUM_EPOCH_NUM, verbose=2)
          weights_file_path_list = sorted(glob.glob(os.path.join(OPTIMAL_WEIGHTS_FOLDER_PATH, "*.h5")))

      print("All done!")

  if __name__ == "__main__":
      run()
#+END_SRC

There’s this thing with the input tensor below.  The shape parameter varies with the Keras backend.  Ugh.  Couldn’t ask for two parameters and handle that logic yourself?  Or I dunno.  Everything is terrible.

Note also that the final loss is measured by mean squared error and we’re targeting four neurons.  Makes sense in my base case.  Eventually may have to learn how to model two different losses at the same time, to do category and regression.

#+NAME: set_model
#+BEGIN_SRC jupyter
  from keras.applications.vgg16 import VGG16
  from keras.layers import Dense, Input, Flatten
  from keras.layers.normalization import BatchNormalization
  from keras.models import Model
  from keras.optimizers import Adam

  # Image processing
  IMAGE_ROW_SIZE = 584
  IMAGE_COLUMN_SIZE = 480

  def init_model(target_num=4, dropout_ratio=0.5, learning_rate=0.0001):
      input_shape = (IMAGE_ROW_SIZE, IMAGE_COLUMN_SIZE, 3)

      # Fine-tune prediction layer
      pretrained_model = VGG16(include_top=False, weights='imagenet',
                               input_shape=input_shape)
      for layer in pretrained_model.layers:
          layer.trainable = False

      output_tensor = pretrained_model.output
      output_tensor = Flatten()(output_tensor)
      # output_tensor = Dense(4096, activation='relu')(output_tensor)
      # output_tensor = Dense(4096, activation='relu')(output_tensor)
      output_tensor = Dense(target_num, activation="sigmoid", name="predictions")(output_tensor)

      # Define and compile the model
      model = Model(inputs=pretrained_model.input, outputs=output_tensor)
      model.compile(optimizer=Adam(lr=learning_rate), loss="mse")

      return model

  model = init_model()
#+END_SRC

#+RESULTS: set_model

We can check out what the model looks like.

#+HEADER: :var filename="./jupyter_images/model.png"
#+BEGIN_SRC jupyter :results replace :file model.png
  from keras.utils.vis_utils import plot_model
  plot_model(model, to_file=filename, show_shapes=True,
             show_layer_names=True)
#+END_SRC

#+RESULTS:
[[file:./jupyter_images/model.png]]

Time to generate some training and validation sets.

Here’s a snag, though.  If we wanna do data augmentation (and I guess we do?) then we can’t easily pull out =(image, annotation)= pairs.  It’s simple to randomly transform the images with ~ImageDataGenerator.flow_from_directory~ but *good luck* trying to apply the same set of random transformations to the annotations.

I kinda wonder if there’s an element here from Gelman’s modeling advice.  We could try to model the bounding box of the character, try to predict four numbers =(x, y, l, w)= … or we could try to model the segmentation of the character, all pixel-by-pixel probabilities of being the character or not, and *then* reduce that to a bounding box at the end if you want.

So, quick digression, we’ve gotta process the images to their masks.

** DONE make masks
CLOSED: [2018-03-28 Wed 15:00]
- Note taken on [2018-03-28 Wed 15:00] \\
  I’m happy with top 55%.  makefile away!
- Note taken on [2018-03-17 Sat 19:13] \\
  god dammit, my approach of finding large peaks is too brittle.  let’s try cutting off the top x% of the hist
need a makefile target, and need to finish the process_masks script

oh god, okay, now we need to actually figure out how to calculate the threshold from the histogram

so that needs to happen in the masking function itself

** Back to Keras
:PROPERTIES:
:header-args:jupyter+: :existing kernel-095f57d6-b7cd-4898-8806-e861db980deb.json
:header-args:jupyter+: :ssh amazon
:header-args:jupyter+: :session smash-remote
:END:

#+CALL: setup_libraries()

#+RESULTS:

*** Localization
So let’s just try to get bounding boxes first.  We’ll use the fine-tuned VGG network we built up above, and aim for just four points =(x, y, w, h)=.  For augmentation, don’t rotate or shear the images, but do shift, zoom, and flip them around.

**** VGG16

#+CALL: set_model()

#+RESULTS:

#+BEGIN_SRC jupyter
  from keras.preprocessing.image import ImageDataGenerator

  DATADIR = "data/keras"

  def preprocess_fn(three_channel_image):
      return three_channel_image[:, :, 0].astype(np.uint8)

  def bounding_box(mask_image):
      result = 0, 0, 0, 0

      _, contours, _ = cv2.findContours(mask_image, cv2.RETR_EXTERNAL,
                                      cv2.CHAIN_APPROX_SIMPLE)
      contours = sorted(contours, key=cv2.contourArea, reverse=True)

      if contours:
          result = cv2.boundingRect(contours[0])

      return result

  def bbox_gen_from_mask_gen(mask_gen):
      for batch in mask_gen:
          new_batch = [bounding_box(preprocess_fn(image))
                      for image in batch]
          new_batch = np.array(new_batch, dtype=np.float32)
          yield new_batch

  def combo_generator(folder="train",
                      data_gen_args={"fill_mode": "constant",
                                     "cval": 0,
                                     "width_shift_range": 0.05,
                                     "height_shift_range": 0.05,
                                     "zoom_range": 0.1,
                                     "horizontal_flip": True,
                                     "rescale": 1.0 / 255},
                      data_flow_args={"seed": 1,
                                      "batch_size": 32,
                                      "class_mode": None}):

      image_datagen = ImageDataGenerator(**data_gen_args)
      mask_datagen = ImageDataGenerator(**data_gen_args)

      image_generator = image_datagen.flow_from_directory(
          directory=os.path.join(DATADIR, folder, "images"),
          target_size=(IMAGE_ROW_SIZE, IMAGE_COLUMN_SIZE),
          color_mode='rgb',
          ,**data_flow_args)

      mask_generator = mask_datagen.flow_from_directory(
          directory=os.path.join(DATADIR, folder, "masks"),
          target_size=(IMAGE_ROW_SIZE, IMAGE_COLUMN_SIZE),
          color_mode='grayscale',
          ,**data_flow_args)

      bbox_generator = bbox_gen_from_mask_gen(mask_generator)

      return zip(image_generator, bbox_generator)

  def steps_per_epoch(folder="train", batch_size=32):
      directory = os.path.join(DATADIR, folder, "images", "dummy")
      data_size = len(os.listdir(directory))
      return data_size // batch_size
#+END_SRC

#+RESULTS:

Just fyi, this can take a *really* long time to run.  Gonna need to practice on small data sets and rely on Amazon to do the heavy lifting.

#+BEGIN_SRC jupyter :results output :eval query
  model.fit_generator(generator=combo_generator("train"),
                      steps_per_epoch=steps_per_epoch("train"),
                      epochs=10,
                      validation_data=combo_generator("valid"),
                      validation_steps=steps_per_epoch("valid"))
#+END_SRC

#+RESULTS:

Once we’re done, save the model so we don’t lose all that work!

#+HEADER: :var filename="./data/keras/model.h5"
#+BEGIN_SRC jupyter
  model.save(filename)
#+END_SRC

#+RESULTS:

Now how does it look on test data?

#+HEADER: :var filename="./data/keras/model.h5"
#+BEGIN_SRC jupyter
  from keras.models import load_model
  model = load_model(filename)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter
  def raw_image_generator(folder="test",
                          data_flow_args={"seed": 1,
                                          "batch_size": 32,
                                          "class_mode": None}):

      return ImageDataGenerator().flow_from_directory(
          directory=os.path.join(DATADIR, folder, "images"),
          target_size=(IMAGE_ROW_SIZE, IMAGE_COLUMN_SIZE),
          color_mode='rgb',
          ,**data_flow_args)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter
  predictions = model.predict_generator(raw_image_generator("test"),
                                        steps=1)
  predictions.tolist()
#+END_SRC

#+RESULTS:
| 1.0 | 1.0 | 1.0 | 1.0 |
| 1.0 | 1.0 | 1.0 | 1.0 |
| 1.0 | 1.0 | 1.0 | 1.0 |
| 1.0 | 1.0 | 1.0 | 1.0 |
| 1.0 | 1.0 | 1.0 | 1.0 |
| 1.0 | 1.0 | 1.0 | 1.0 |
| 1.0 | 1.0 | 1.0 | 1.0 |
| 1.0 | 1.0 | 1.0 | 1.0 |
| 1.0 | 1.0 | 1.0 | 1.0 |
| 1.0 | 1.0 | 1.0 | 1.0 |
| 1.0 | 1.0 | 1.0 | 1.0 |
| 1.0 | 1.0 | 1.0 | 1.0 |
| 1.0 | 1.0 | 1.0 | 1.0 |
| 1.0 | 1.0 | 1.0 | 1.0 |
| 1.0 | 1.0 | 1.0 | 1.0 |
| 1.0 | 1.0 | 1.0 | 1.0 |
| 1.0 | 1.0 | 1.0 | 1.0 |
| 1.0 | 1.0 | 1.0 | 1.0 |
| 1.0 | 1.0 | 1.0 | 1.0 |
| 1.0 | 1.0 | 1.0 | 1.0 |
| 1.0 | 1.0 | 1.0 | 1.0 |
| 1.0 | 1.0 | 1.0 | 1.0 |
| 1.0 | 1.0 | 1.0 | 1.0 |
| 1.0 | 1.0 | 1.0 | 1.0 |
| 1.0 | 1.0 | 1.0 | 1.0 |
| 1.0 | 1.0 | 1.0 | 1.0 |
| 1.0 | 1.0 | 1.0 | 1.0 |
| 1.0 | 1.0 | 1.0 | 1.0 |
| 1.0 | 1.0 | 1.0 | 1.0 |
| 1.0 | 1.0 | 1.0 | 1.0 |
| 1.0 | 1.0 | 1.0 | 1.0 |
| 1.0 | 1.0 | 1.0 | 1.0 |

lol okay well … maybe there’s a problem because I didn’t rescale the inputs (some sources on the internet say you should rescale though?).  Or maybe because I don’t have enough dense layers in between the VGG convolutional layers and my prediction layer?

Either way, it’s too slow to train on my laptop.  Time to figure out how to upload to Amazon.

*** DONE post process mask images
CLOSED: [2018-04-05 Thu 22:39]
ugh, so, we can read this big array of grayscale images … but we can’t hack into the preprocessing data generation argument to process each image to a list of four ints.

so I guess we gotta process that stuff after the images are flowed out of the directory.  okay, cool

*** DONE split data into train, test, and validate
CLOSED: [2018-04-06 Fri 13:36]
- Note taken on [2018-04-06 Fri 13:36] \\
  lol makefile all the way
this should maybe be handled in the code, not in the makefile?  like, how do we ensure the image and masks stay in sync?

*** DONE update makefile
CLOSED: [2018-04-06 Fri 13:36]
at the very least its gotta put images and masks in dummy categorical folders …

in the future it may have to put them in real categorical folders, but let’s cross that bridge when we get to it

*** TODO make seeding and splitting makefile variables
right now we hard code the shuffling seed in the actual script file, and we hard code the train/test/valid split in the makefile.  uh, that’s not great.  oh well

** Detour to learn AWS
So I’m reading some stuff.  They’ve got boxes set up specifically for machine learning, which is nice.  They run on either Amazon Linux (eh?) or Ubuntu (okay fine).  They use Anaconda for virtual environments (ugh, I don’t wanna bother learning that one).

It all seems pretty pricey, though.  p2.xlarge is 0.90 an hour (cheapest with access to a GPU) and p3.2xlarge is 3.06 / hour (recommended, has a shitzillion CPU and GPU cores).  I may want to find a cheaper option that’s all CPU just to practice AWS.  Then graduate to p2.xlarge.  Then read up on GPU and modify my script to take advantage of it.  Then graduate to p3.2xlarge.

I also need to make *all* the data.  It’s probably worth learning how to do that first, kicking off an instance with a shitzillion CPU cores, and finding a way to save all the data to S3.  Then come back and work on the machine learning stuff.  When the data is all done, hopefully, I’ll be in a place to start training my shit.

Okay, so plan.  

*** DONE how do I upload data to S3?
CLOSED: [2018-04-07 Sat 13:47]
actually pretty easy.  can do a lot of management from the web console, and can automate stuff with =s3cmd=

sidenote: did you know =find -exec += can’t handle ={}= in the middle of the command?  It has to come at the end!  Per somebody’s comment on stack exchange … why the fuck does ={}= need to be there if it must always come at the end?

It’s okay.  Just use =xargs= in that case.  But dang, I have been making that wrong assumption forever without getting bitten.

found because =s3cmd put= needs the filenames first, then the bucket last.  weird, but okay.

*** DONE how do I read data from S3 onto ML box?
CLOSED: [2018-04-07 Sat 14:09]
so there’s =s3cmd= but apparently also there’s and =aws s3= command that amazon provides.  eh?  

ah, looks like =aws s3= is the way to go because it has a =--sync= option.  it’s a lot like =rsync=.  don’t have to worry about overwriting stuff.

oh wait, nvm.  =s3cmd= has the same option.  looks like you might want =--skip-existing= just so it doesn’t bother checking, like, last modified time and md5 hash, or something.

I remember reading that =aws s3 --recursive= is weird about its include / exclude rules.  I’m kinda partial to =find | xargs= because of that, but if you want to go down the recursive route, just keep in mind that =--include= *stacks* on top of previous stuff, and the default is to include *everything*.  So for =--include= to have any effect, you need to precede it with an =--exclude '*'= or something similar.  Like, seriously?  You couldn’t figure that out automatically?  Ugh.

it’s also fucking slow to upload from my laptop to S3, but with any luck it’ll be way faster S3 <-> EC2.

*** DONE upload data to S3
CLOSED: [2018-04-07 Sat 14:10]
working on it

*** DONE how do I make S3 data from a CPU box?
CLOSED: [2018-04-12 Thu 12:20]
ah yeah, this is the part where I need some sort of windowing system.  super cool.  time to revisit the X-Windows disaster.

http://www.brianlinkletter.com/how-to-run-gui-applications-xfce-on-an-amazon-aws-cloud-server-instance/

except JFC I don’t want to install a whole desktop environment.  just a simple window manager and go from there (fingers crossed)

https://ubuntuforums.org/showthread.php?t=151703

and then I’ll need to get a VNC client for myself.  Oh wait, Mac has one, just Cmd-Space for Screen Sharing.

alrighty, so I just need to spin up an EC2 (start really cheap), get x-forwarding and VNC going, and record the steps.  then get dolphin on there, somehow, and get it running.  then get my Makefile on there and try to run one recording session.  then spin up a beefy EC2 and let it rip!

**** DONE get X and VNC on cheapo EC2
CLOSED: [2018-04-08 Sun 19:48]
so I’ve got an instance working.  I need to record some stuff about it.

InstanceID: i-06ed436a6e80e765b
DNS: ec2-34-207-53-187.compute-1.amazonaws.com
Private Key: ~/.ssh/amazon-ec2.pem
User: ubuntu

Set up permissions to log in via SSH, updated apt repositories, and boom!

Need a little more direction, but fortunately found it here

https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-vnc-on-ubuntu-14-04

Quick digression, gotta get Emacs set up on the remote!

https://andy.wordpress.com/2013/01/03/automatic-emacsclient/

Woo!

ANNNNNND I FINALLY GOT IT!  Took way too long to figure out how ssh port forwarding works … and I still don’t quite understand controlmaster stuff … but it works now!

Oh yeah, I wonder if I need those security permissions?  Nope!  SSH handles everything!

Also, kinda cool, I definitely didn’t need to install a full environment or anything.  twm is fine so far.

So the important stuff was … for Emacs, =apt install emacs24-nox= and scp my local Emacs’ server file up there.  For VNC, =apt install tightvncserver= (that should pull in X) and uncomment the x-window-manager lines.  That might change, though, once we get Dolphin going.

Lol.  Doesn’t look like much.  Took all day for me to figure it out, though.

**** DONE get Dolphin on cheapo EC2
CLOSED: [2018-04-08 Sun 20:29]
Woo!  Ubuntu just has it!

https://wiki.dolphin-emu.org/index.php?title=Installing_Dolphin#Ubuntu

Ah, but fuck, of course.  Needs fancy graphics, I guess GTK+.  So let’s go get that.

Shitty that it doesn’t get pulled in automatically?  Maybe because it’s suggested, I wonder if dolphin can work with other frameworks.

Nope!  Not a fancy graphics thing!  Just that command line dolphin didn’t know where to draw.  Set an environment variable =DISPLAY= to =:1= and bob’s your uncle!

**** DONE get a recording on cheapo EC2
CLOSED: [2018-04-09 Mon 11:42]
and move that data to S3

Oh shit this one’s hard.  Good, shake some bugs out of my stuff.

So, 1, gotta clone the repo.  Not too bad.  But then gotta tweak it.

Need to upload the melee stuff, and lol.  It’s buried in the =scripts/record_avi= directory.  Um, lol.  

Then gotta tweak the record script to use the right Ubuntu conventions.  God damn it.

And gotta put my config file in the dolphin folder.  And hope that it’s good for Ubuntu.

Holy fuck, that SCP transfer up to amazon is slow as shit!  Gonna take like 30 minutes to move that 1.4G melee iso.  Maybe look into FTP next time?

Whoa, rsync is ten times faster.  Nice.

Gotta upload my config folder, and gotta tweak the Dolphin.ini variables, and then gotta upload my memory card thing?

Okay, so, that was some manual crap that’s awful.

And now we’re all about that recording session.

Oh shit!  Stuck.  Installed ffmpeg, but before we get there we can’t even play Dolphin games.  No graphics hardware on EC2!  Gotta get an instance with a GPU and hope …

Okay, that’s for tomorrow, though.

**** DONE make a quick setup script
CLOSED: [2018-04-12 Thu 01:48]
fuck docker, I honestly don’t see the need yet

maybe it’s like an install script with better permissions?  I could see that.

Yeah, I’m sorry, I just don’t see the point of docker for data science yet.  The official tutorial makes it clear how useful this is for deploying applications … but for DS it just sorta looks like an install script. 

Which … thanks, but I’ve already got a requirements.txt for python.  Internet recommends writing a similar packages.txt for apt and just using xargs to get the right effect.  sounds fair.

eh, there’s the emacsclient script.  and the melee binary (though wouldn’t that be against copyright to distribute with the docker image anyway?)

and there’s my dolphin config stuff

I mean, I suppose I could do this, but ugh.  Not right now.  Yeah, shit’s not gonna be reproducible for anybody else, but meh.  I’ve got a ubuntu-packages file, now, so at least I can install everything.  and the emacs stuff is kinda me-specific so I don’t really need to bother.  

only real deal now is uploading the ISO (which, yeah, can’t really reproduce that for everybody) though maybe I should check the melee header into git.

okay, and the dolphin config stuff.  just throw that in the git repo.

arg, minor sticking point, I’ve gotta change the setup files list to skip over the “No Memory Card” popup.  it’s too annoying trying to update the config files to point to the uploaded memory card, and besides, I don’t need that

holy fucking shit, dolphin-emu on linux takes =-U= for the user arg, while on mac takes =-u=.  what.

now comes the fucking hardest part, is getting opengl working.  so, lesson learned, can’t actually use =vncserver= for that.  it starts its own baby X server, which never takes full advantage of the GPU.  check it

https://stackoverflow.com/questions/45035347/steam-aws-machine-opengl-not-using-direct-rendering

woo!  fuck yeah, x11vnc works instead!  time to tweak that command line and copy it down

okay, I think I’ve got it.  you could put this in like an xinit or something.

#+BEGIN_SRC shell
  DISPLAY=:0 x11vnc -q -auth ~/.Xauthority -usepw -rfbport 5901 -bg -forever -shared
#+END_SRC

I dunno why it can’t guess the authority file.  usepw means to use the password file I previously made (so gotta make that).  for some reason I can’t use the default port (5900) right now because ssh forwarding is getting mad at me for unknown reasons.  perhaps a reboot would solve that.  and then -bg -q are just obvious.  need a -forever in case of weird random drops, otherwise the server closes if the connection drops.

okay, it seems to die a lot on me, though.  annoying.  like, I type on or two keys and it dies.

apparently that’s because the mac client likes to create a second connection a few seconds after the first?  or something?  at any rate, the default behavior is to deny second clients, but maybe the first mac connection kills itself after the second one tries?  very strange, but anyway, -shared fixes it.

omg and of course, the version of dolphin on ubuntu is 4.0.2.  apparently my settings aren’t compatible with that one (no cheats for melee).  will have to see what I can do about that.

cool, so there’s a dolphin ppa that provides the latest stable binaries.  nice.

okay, now I’m feeling the benefits to a dockerfile.  like, yeah, I could automate all this with make.  make binaries, or something, but yeah.  it’d either be hard-coded for Ubuntu, or else would have to guess-and-check for various distros, or whatever.  okay, so just use docker for that.

--------------------------------------------------------------------------------

omigod I finally got it all working!  holy shit!

yeah, now I appreciate the need for a docker file.  let’s get that written tomorrow, before I have a chance to forget too much of what I got working today

--------------------------------------------------------------------------------

just one last time through this.  after I instantiate a new instance, I …

ssh in and 
#+BEGIN_SRC shell :dir /ssh:amazon:
  mkdir bin
  mkdir -p .emacs.d/server
#+END_SRC

#+RESULTS:

drop back and
#+BEGIN_SRC shell
  scp ~/bin/ec amazon:bin
  scp ~/.emacs.d/server/server amazon:.emacs.d/server
  scp ~/.ssh/id_rsa amazon:.ssh
  scp ~/.ssh/id_rsa.pub amazon:.ssh
  scp ~/.gitconfig amazon:
#+END_SRC

#+RESULTS:

I’m sure there’s a better way

then ssh in again and (this part could be dockerized)
#+BEGIN_SRC shell :dir /ssh:amazon:
  sudo add-apt-repository ppa:dolphin-emu/ppa -y
  sudo apt update

  git clone https://github.com/tmurph/datasci-smash
  cd datasci-smash
  xargs -a ubuntu-packages.txt sudo apt install -y
  pip3 install -r requirements.txt
  sudo X :0 &
#+END_SRC

#+RESULTS:

for my own sanity we gotta clean some stuff up
#+BEGIN_SRC shell :dir /ssh:amazon:
  sudo apt install emacs24-nox -y

  cd ~
  rm -rf datasci-smash

  echo 'github.com ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAq2A7hRGmdnm9tUDbO9IDSwBK6TbQa+PXYPCPy6rbTrTtw7PHkccKrpp0yVhp5HdEIcKr6pLlVDBfOLX9QUsyCOV0wzfjIJNlGEYsdlLJizHhbn2mUjvSAHQqZETYP81eFzLQNnPHt4EVVUh7VfDESU84KezmD5QlWpXLmvU31/yMf+Se8xhHTvKSCZIFImWwoG6mbUoWf9nzpIoaSjB+weqqUUmpaaasXVal72J+UX2B+2RPW3RcT0eOzQgqlJL3RKrTJvdsjE3JEAvGq3lGHSZXy28G3skua2SmVi/w4yCE6gbODqnTWlg7+wC604ydGXA8VJiS5ap43JXiUFFAaQ==' >>~/.ssh/known_hosts 
  git clone git@github.com:tmurph/datasci-smash.git

  cat >.bash_profile <<EOF
  export EDITOR=/home/ubuntu/bin/ec
  export PATH=/home/ubuntu/bin:/home/ubuntu/.local/bin:$PATH
  export DISPLAY=:0

  if [ -f ~/.bashrc ]; then
     . ~/.bashrc
  fi
  EOF
#+END_SRC

then (optionally) start some vnc action
#+BEGIN_SRC shell :dir /ssh:amazon:
  x11vnc -auth ~/.Xauthority -usepw -rfbport 5901 -bg -shared -forever
#+END_SRC

and finally upload the copyright questionable (come back home for this) (also this will take for fucking ever, so probably run it from a terminal)
#+BEGIN_SRC shell :eval never
  rsync -Paz ~/Code/datasci-smash/src/scripts/record_avi/'Super_Smash_Bros._Melee_(v1.02).iso' amazon:datasci-smash/scripts/record_avi &
#+END_SRC

and we’re off to the races.  can start running make stuff.

huh, I’m noticing there’s some weirdness with environment variables and whatnot … like, the coming back home, then ssh back in, back and forth … I guess that clears the env variables (makes sense, they’re not in profile or anything).  cross that bridge when you get to it, though

also, since a lot of this stuff is kinda long running, best start using tmux.  ugh, can’t tmux from in an emacs shell buffer, it’s too dumb.  cool.  gotta deal with shitty apple term (or get like an xterm, but ugh, I don’t want another terminal emulator)

huzzah!  making fox data tonight!  just gotta log in tomorrow, attach tmux to the last session with =tmux a #= and awayyyyyyy we go!

**** SKIPPED get a recording on expensive EC2
CLOSED: [2018-04-12 Thu 12:19]
got fox data going on fancy GPU computer!  just gonna move it tomorrow and away we go!

move that data to S3, too

*** DONE split character specific csv dependencies?
CLOSED: [2018-04-13 Fri 16:34]
right now, if I update any character’s frame data, make doesn’t know any better and decides it needs to update every character’s images.  maybe I can do better?

*** SKIPPED maybe clean up compile moves script?
CLOSED: [2018-04-13 Fri 16:34]
it’s so ugly, should really just be one big class, instead of a bunch of global variables.  oh well, it works.

*** DONE start making all the data
CLOSED: [2018-04-22 Sun 16:36]
oh god.  I ran out of space on my box, only 8G.  okay, we need a list:

- [-] samus
  - [X] images
  - [ ] hist
  - [ ] masks
- [-] fox
  - [ ] images
  - [X] hist
  - [ ] masks
- [-] marth
  - [ ] images
  - [X] hist
  - [ ] masks

    
then there’s some godawful inconsistencies that I guess I’m just gonna ¯\_(ツ)_/¯ about.  like

1. the first 24 images or so of each fox video have the giant “GO!” logo in front.  my prefix second calculation assumes an offset of 420, because that’s what worked on my laptop.  but that’s too much for the amazon box, I think because it’s so much faster.  cool.  if I want to recalculate that prefix, I could do some experiments and figure it out.  but then I’d have to regenerate *all* the fox data again.  Ugh, do I really wanna do that?  Or just make the mask data with the script as-is, upload everything, and later come up with some wacko script that renames everybody?  Definitely the latter.
2. samus histograms are crap, they only use ten images instead of the sixty or seventy they should.  I’m not sure why.  but you’ll want to redo that, redo the masks, and unless you wanna do all that work on your laptop, you’ll obvi have to redo the images to match.  ugh, cool.
3. shit, are the histograms offset incorrectly as well?  fuck.  yup, they are.
4. fuck, so all this work is really really borked.  glad you figured that out now.

   --------------------------------------------------------------------------------

Alright, new plan is to make videos on my laptop and images on amazon

- [X] falco
  - [X] images
    - [X] movies
    - [X] jpegs
  - [X] hist
    - [X] movies
    - [X] jpegs
  - [X] masks
    - [X] movies
    - [X] jpegs
- [X] falcon
  - [X] images
    - [X] movies
    - [X] jpegs
  - [X] hist
    - [X] movies
    - [X] jpegs
  - [X] masks
    - [X] movies
    - [X] jpegs
- [X] fox
  - [X] images
    - [X] movies
    - [X] jpegs
  - [X] hist
    - [X] movies
    - [X] jpegs
  - [X] masks
    - [X] movies
    - [X] jpegs
- [X] jigglypuff
  - [X] images
    - [X] movies
    - [X] jpegs
  - [X] hist
    - [X] movies
    - [X] jpegs
  - [X] masks
    - [X] movies
    - [X] jpegs
- [X] marth
  - [X] images
    - [X] movies
    - [X] jpegs
  - [X] hist
    - [X] movies
    - [X] jpegs
  - [X] masks
    - [X] movies
    - [X] jpegs
- [X] peach
  - [X] images
    - [X] movies
    - [X] jpegs
  - [X] hist
    - [X] movies
    - [X] jpegs
  - [X] masks
    - [X] movies
    - [X] jpegs
- [ ] samus
  - [ ] images
    - [ ] movies
    - [ ] jpegs
  - [ ] hist
    - [ ] movies
    - [ ] jpegs
  - [ ] masks
    - [ ] movies
    - [ ] jpegs
- [X] sheik
  - [X] images
    - [X] movies
    - [X] jpegs
  - [X] hist
    - [X] movies
    - [X] jpegs
  - [X] masks
    - [X] movies
    - [X] jpegs

*** TODO make the makefile error on bad python pipes
ugh.  I think I have the python stuff set to not error if it can’t write to the pipe

but I may be too general with that.  it should bubble up an error if, e.g., I forget to set up my virtual environment and it can’t import pandas

though this may also be a problem with piping commands.  if the source errors but the target “succeeds” in writing nothing to a file … does the whole pipeline status come back 0 or 1?

** Back to Keras
:PROPERTIES:
:header-args:jupyter+: :existing kernel-2a18454b-9ce1-4e66-8892-9732c46a2fe1.json
:header-args:jupyter+: :ssh amazon-gpu
:header-args:jupyter+: :session smash-remote
:END:

#+CALL: setup_libraries()

#+RESULTS:

*** Classification
Fuck, these models are a bitch to work with.  I’m sorta starting to understand the joys of docker, because I keep hitting all these god awful snags trying to manage versions between ubuntu, Mac, etc.

Also, I lost some of my goddamn data.  Wtf.  No Sheik, no Falcon, and obvi no Samus.

I have no clue what happened, but I’ll go back and remake that tonight.  Ugh.

So, let’s just build a real simple classifier, for now.  First pass, we gotta be able to classify these images.

#+NAME: model_prep
#+BEGIN_SRC jupyter
  import os

  DATADIR = "data/keras"
  NUM_CHARACTERS = len(os.listdir(os.path.join(DATADIR, "train", "images")))
#+END_SRC

#+NAME: set_categorical_model
#+BEGIN_SRC jupyter
  from keras.applications.vgg16 import VGG16
  from keras.layers import Dense, Input, Flatten
  from keras.layers.normalization import BatchNormalization
  from keras.models import Model
  from keras.optimizers import Adam

  # Image processing
  IMAGE_ROW_SIZE = 584
  IMAGE_COLUMN_SIZE = 480

  def init_model(target_num=4, dropout_ratio=0.5, learning_rate=0.0001):
      input_shape = (IMAGE_ROW_SIZE, IMAGE_COLUMN_SIZE, 3)

      # Fine-tune prediction layer
      pretrained_model = VGG16(include_top=False, weights='imagenet',
                               input_shape=input_shape)
      for layer in pretrained_model.layers:
          layer.trainable = False

      output_tensor = pretrained_model.output
      output_tensor = Flatten()(output_tensor)
      # output_tensor = Dense(4096, activation='relu')(output_tensor)
      # output_tensor = Dense(4096, activation='relu')(output_tensor)
      output_tensor = Dense(target_num, activation="softmax", name="predictions")(output_tensor)

      # Define and compile the model
      model = Model(inputs=pretrained_model.input, outputs=output_tensor)
      model.compile(optimizer=Adam(lr=learning_rate),
                    loss="categorical_crossentropy",
                    metrics=["accuracy"])

      return model

  model = init_model(target_num=NUM_CHARACTERS)
#+END_SRC

#+RESULTS: set_categorical_model

#+BEGIN_SRC jupyter
  from keras.preprocessing.image import ImageDataGenerator

  def make_generator(folder="train",
                     data_gen_args={"fill_mode": "constant",
                                    "cval": 0,
                                    "width_shift_range": 0.05,
                                    "height_shift_range": 0.05,
                                    "zoom_range": 0.1,
                                    "horizontal_flip": True,
                                    "rescale": 1.0 / 255},
                     data_flow_args={"seed": 1,
                                     "batch_size": 32}):

      image_datagen = ImageDataGenerator(**data_gen_args)

      image_generator = image_datagen.flow_from_directory(
          directory=os.path.join(DATADIR, folder, "images"),
          target_size=(IMAGE_ROW_SIZE, IMAGE_COLUMN_SIZE),
          color_mode='rgb',
          ,**data_flow_args)

      return image_generator

  def steps_per_epoch(folder="train", batch_size=32):
      image_directory = os.path.join(DATADIR, folder, "images")
      char_name = os.listdir(image_directory)[0]
      char_directory = os.path.join(image_directory, char_name)
      data_size = len(os.listdir(char_directory))
      data_size *= NUM_CHARACTERS
      return data_size // batch_size
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter :results output
model.summary()
#+END_SRC

#+RESULTS:
#+begin_example
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 584, 480, 3)       0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 584, 480, 64)      1792      
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 584, 480, 64)      36928     
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 292, 240, 64)      0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 292, 240, 128)     73856     
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 292, 240, 128)     147584    
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 146, 120, 128)     0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 146, 120, 256)     295168    
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 146, 120, 256)     590080    
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 146, 120, 256)     590080    
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 73, 60, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 73, 60, 512)       1180160   
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 73, 60, 512)       2359808   
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 73, 60, 512)       2359808   
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 36, 30, 512)       0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 36, 30, 512)       2359808   
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 36, 30, 512)       2359808   
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 36, 30, 512)       2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 18, 15, 512)       0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 138240)            0         
_________________________________________________________________
predictions (Dense)          (None, 5)                 691205    
=================================================================
Total params: 15,405,893
Trainable params: 691,205
Non-trainable params: 14,714,688
_________________________________________________________________
#+end_example

I found this code from the Deep Lizard keras tutorial, and it is ugly as sin.  Oh well, kinda helpful to me now.

#+BEGIN_SRC jupyter
  def plot_images(images, figsize=(12, 6), rows=1, interp=False, titles=None):
      if type(images[0]) is np.ndarray:
          images = np.array(images).astype(np.uint8)
      fig = plt.figure(figsize=figsize)
      cols = len(images) // rows if len(images) % rows == 0 else len(images) // rows + 1
      for i, image in enumerate(images):
          sp = fig.add_subplot(rows, cols, i + 1)
          sp.axis('Off')
          if titles is not None:
              sp.set_title(titles[i], fontsize=16)
          sp.imshow(image, interpolation=None if interp else 'none')
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter
  def make_test_generator(folder="test",
                          data_flow_args={"seed": 1,
                                          "batch_size": 32}):

      return ImageDataGenerator().flow_from_directory(
          directory=os.path.join(DATADIR, folder, "images"),
          target_size=(IMAGE_ROW_SIZE, IMAGE_COLUMN_SIZE),
          color_mode='rgb',
          ,**data_flow_args)
#+END_SRC

#+RESULTS:

The Deep Lizard keras tutorials do contain a lot of good info.  Like this tidbit about class indices (even though I need the reverse dictionary).

#+BEGIN_SRC jupyter
  test_batches = make_test_generator(data_flow_args={"batch_size": 3})
  class_dict = test_batches.class_indices
  index_dict = {i: c for c, i in class_dict.items()}
#+END_SRC

#+RESULTS:

Huzzah!  Look what I can plot!

#+BEGIN_SRC jupyter :results file
  images, vectors = next(test_batches)
  labels = [index_dict[np.argmax(v)] for v in vectors]
  plot_images(images, titles=labels)
#+END_SRC

#+RESULTS:
[[file:./jupyter_images/150705G.png]]

But this is the really cool shit.

#+BEGIN_SRC jupyter :results dataframe
  images, vectors = next(test_batches)
  true_labels = [index_dict[i] for i in vectors.argmax(1)]
  predictions = model.predict_on_batch(images)
  pred_labels = [index_dict[i] for i in predictions.argmax(1)]

  pd.DataFrame({'True': true_labels, 'Predicted': pred_labels})
#+END_SRC

#+BEGIN_SRC jupyter
  def plot_confusion_matrix(matrix, classes,
                            normalize=False,
                            title='Confusion matrix',
                            cmap=plt.cm.Blues):
      """
      This function prints and plots the confusion matrix.
      Normalization can be applied by setting `normalize=True`.
      """
      if normalize:
          matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]

      plt.imshow(matrix, interpolation='nearest', cmap=cmap)
      plt.title(title)
      plt.colorbar()
      tick_marks = np.arange(len(classes))
      plt.xticks(tick_marks, classes, rotation=45)
      plt.yticks(tick_marks, classes)

      fmt = '.2f' if normalize else 'd'
      thresh = matrix.max() / 2.
      for i, j in itertools.product(range(matrix.shape[0]), range(matrix.shape[1])):
          plt.text(j, i, format(matrix[i, j], fmt),
                   horizontalalignment="center",
                   color="white" if matrix[i, j] > thresh else "black")

      plt.tight_layout()
      plt.ylabel('True label')
      plt.xlabel('Predicted label')
#+END_SRC

*** Lessons Learned
I still think it’s so fucking cool that I can fire up a notebook on Amazon and (with, admittedly, a non-trivial amount of work) connect to it from here.  It’s definitely nice being able to play with the repl and the code buffers here, then click over to the browser to kick off really long-running training code.

Speaking of … training still takes *forever*.  Maybe it’s because I’m still a cheapskate using p2.xlarge.  I suppose I could splurge on a g2.8xlarge sometime and see how much that helps.  Because, here’s the thing, if it takes more than a minute or two to train then this iterative process is going to *suck*.  I can feel it already.

I’m not sure what an ideal epoch size is, but the internet makes me feel that each epoch should run over all my data.  I can’t do that on puny p2, I get a =ResourceExhaustedError= from tensorflow.  I’m still getting decent results from training with short epochs (got up to ~80% accuracy with no hidden layers, currently training one hidden layer to see what’s what).  But I can’t really experiment playfully and see what pops out.

Also, yeah, I have no clue what’s up with hidden layers.  Tried with none, now trying with one small one.  The dense hidden layer on top of VGG adds *so many* trainable variables, but actually, it’s not seeming to slow down my training.  I don’t quite get that.  Will have to play more with more hidden layers / wider hidden layers to see if it messes with things.  Again, gotta go fast, then start going complex.

Side note: I’m really cheesed to just now realize I can probably turn off all the bullshit stage nonsense, like Story’s shy guys, Fountain’s fountains, etc.  I don’t *think* it’s worth rerunning all those character videos, but if I ever get around to it, maybe.

--------------------------------------------------------------------------------

Now that the hidden layer model has finished, I plotted the confusion matrix.  

I could spend some time staring at the values and trying to puzzle out just why, say, fox is often mistaken for peach.  But honestly, I’m guessing it’s because there are plenty of images where the characters are dead.  So plenty of fox images of just a blank background, and plenty of peach images of just a blank background.

Would it be worth it to update the process_masks script to skip over images that are probably blanks?  I can see that.

Not today, though.  Today I’m happy.

** Detour to Filter Masks

*** Exploratory Code
Note to self: =C-c C-v C-s= calls ~org-babel-execute-subtree~

#+CALL: setup_libraries()

#+RESULTS:

#+HEADER: :var stem="fox_1_fountain_left_bg_off_111"
#+BEGIN_SRC jupyter :results file
  img_name = f"data/masks/{stem}.jpg"
  mask_name = f"data/masks/{stem}_mask.jpg"
  img = cv2.imread(img_name)
  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
  mask = cv2.imread(mask_name)
  mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)
  fig, (ax1, ax2) = plt.subplots(1, 2)
  ax1.imshow(img)
  ax2.imshow(mask, cmap='gray')
#+END_SRC

#+RESULTS:
[[file:./jupyter_images/1507tvf.png]]

#+BEGIN_SRC jupyter   
  non_zero = 1
  int8_max = 255
  _, thresh_img = cv2.threshold(mask, non_zero, int8_max, cv2.THRESH_BINARY)
  _, contours, _ = cv2.findContours(thresh_img, cv2.RETR_EXTERNAL,
                                    cv2.CHAIN_APPROX_SIMPLE)
  contours = sorted(contours, key=cv2.contourArea, reverse=True)
  len(contours)
#+END_SRC

#+RESULTS:
: 130

#+BEGIN_SRC jupyter 
  [cv2.contourArea(c) for c in contours][:5]
#+END_SRC

#+RESULTS:
| 23387.0 | 0.0 | 0.0 | 0.0 | 0.0 |

#+BEGIN_SRC jupyter 
  if contours:
      big_contour = contours[0]
      x, y, w, h = cv2.boundingRect(big_contour)
      bbox_area = w * h
      cont_area = cv2.contourArea(big_contour)
  else:
      x, y, w, h = 0, 0, 0, 0
      bbox_area = 0
      cont_area = 0

  x, y, w, h
#+END_SRC

#+RESULTS:
| 203 | 0 | 122 | 480 |

#+BEGIN_SRC jupyter :results file
  mask_w_box = cv2.rectangle(mask.copy(), (x, y), (x + w, y + h), 255, 3)
  plt.imshow(mask_w_box, cmap='gray')
#+END_SRC

#+RESULTS:
[[file:./jupyter_images/1507HEs.png]]

#+BEGIN_SRC jupyter 
  total_area = mask.shape[0] * mask.shape[1]

  (bbox_area / total_area, cont_area / total_area)
#+END_SRC

#+RESULTS:
| 0.2089041095890411 | 0.08342965182648401 |

*** Lessons Learned
We can filter out a lot of garbage by looking for contour areas in the 1% to 7% range.  I dunno about that upper bound, but the lower bound looks good based on Fox.  God dammit I should probably check it for puff too.  Ugh.

Sunova.  I was thinking of using other heuristics, like filtering out stuff that was too wide or too tall.  But what about crouching puff?  Samus uair?  

Better to throw out more data and get better training?  Or keep more data?  Ugh, tradeoffs, just pick something and move on.

And, of course, you could always MTurk it.  In a real context that’s probably what you’d do.  Is it?  ~ 21600 images?  Even at one image per second that’s 6 hours of work.  Okay, not that bad.  How much work are you putting into this code?  It’s a terrible 6 hours, of course, but meh.  If you split it across a few folks, a few days.  

Hmm, but if it takes a few days anyway, then is it really 6 hours of work?  If two people take all day to do it, then it’s two person-days.  TBH that feels like a better estimate.

Yeah, let’s just throw some very tight filtering bounds on there, throw away a lot of data, and move on.

** Back to Keras
:PROPERTIES:
:header-args:jupyter+: :existing kernel-f38a1b5f-4fc1-497c-9e1f-4973c4e50a54.json
:header-args:jupyter+: :ssh amazon-gpu
:header-args:jupyter+: :session smash-remote
:END:

#+NAME: get_remote_kernel_json
#+BEGIN_SRC shell :dir /ssh:amazon-gpu:
  find /run/user/1000/jupyter -name 'kernel*'
#+END_SRC

#+RESULTS: get_remote_kernel_json
: /run/user/1000/jupyter/kernel-f38a1b5f-4fc1-497c-9e1f-4973c4e50a54.json

#+HEADER: :var remote_file=get_remote_kernel_json
#+HEADER: :var remote="amazon-gpu"
#+BEGIN_SRC shell 
  scp "${remote}:${remote_file}" ~/Library/Jupyter/runtime/$(basename "${remote_file}")
  find ~/Library/Jupyter/runtime -name 'kernel*' -execdir basename -a {} +
#+END_SRC

#+RESULTS:
| kernel-2a18454b-9ce1-4e66-8892-9732c46a2fe1.json |
| kernel-36b8c574-3dbd-48ac-a423-3540436ecf7b.json |
| kernel-9a8479c8-50f9-44d2-a32a-a4a7da424c50.json |
| kernel-f38a1b5f-4fc1-497c-9e1f-4973c4e50a54.json |

#+BEGIN_SRC jupyter 
  "trivial setup complete"
#+END_SRC

#+RESULTS:
: trivial setup complete

*** Classification Again
The code from above is saved in [[file:smash.ipynb][smash.ipynb]] so I’ll just run it from there.  Use this space to do ad-hoc investigation from Emacs.  Probably mainly images.

--------------------------------------------------------------------------------

So I’ve been running a lot of models, let’s list them out:

#+BEGIN_SRC shell
  find data/keras -name '*h5'
#+END_SRC

#+RESULTS:
| data/keras/2018-05-02-long-epochs-2-hidden-layers.h5 |
| data/keras/2018-05-01-long-epochs-1-hidden-layer.h5  |

*** Lessons Learned
I’m not sure if long or short epochs make a difference in training accuracy, but let’s do long epochs out of respect to intent.  (“short” epochs only ranged over maybe one sixth the data, while “long” epochs try to cover roughly the whole data set)

Two hidden layers perform *way* better than one.  Well at least …

Preprocessing the images made a *huge* difference.  And [[http://theorangeduck.com/page/neural-network-not-working?imm_mid=0f6562&cmp=em-data-na-na-newsltr_20170920][this blog]] reminds that, yeah, it’s a total rookie mistake not to normalize data somehow.  Turns out, VGG16 does a non-standard thing where it centers images around some arbitrary point and doesn’t shrink them down to standard deviation one.  Fortunately Keras includes a VGG16-specific preprocessing function, so yay!

I may want to go back and check what happens with preprocessing and just 1 hidden layer.  I’ll expect it to be worse than with 2, but we’ll see.

The blog recommends I should play with learning rate (make it bigger) and “regularization” a/k/a dropout a/k/a inserting some randomness.  

--------------------------------------------------------------------------------

Oh wow.  So I found a bug in my train / test / valid splitting code.  Rerunning now, I’m really hoping that improves performance.

Also it seems I’ve topped out on playing with some of the suggestions from that blog.  I can’t really get beyond 75% / 80% accuracy.  Could be too little data?  But it takes for fucking ever to run …

Turns out!  It’s because I’m rerunning VGG every time, and even with a beefy GPU that’s expensive.  There’s a great [[https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html][keras blog post]] though that lays out a good strategy … run VGG once to extract features, then just train a tiny network on that.  I can’t use data augmentation … but I could make up for that by tweaking dropout and using all my images.  Neat!

*** New Classification Approach
:PROPERTIES:
:header-args:jupyter+: :existing kernel-9e3bbebe-fa1c-4ab5-b1ed-857b240cd8d7.json
:header-args:jupyter+: :ssh amazon
:header-args:jupyter+: :session smash-remote
:END:

#+CALL: get_remote_kernel_name(remote="amazon")

#+RESULTS:
: kernel-9e3bbebe-fa1c-4ab5-b1ed-857b240cd8d7.json

Let’s use this space to prototype a new VGG setup that runs the model once to cache features, then allows me to train tiny nets on that.

#+BEGIN_SRC jupyter 
  from keras.applications.vgg16 import VGG16

  model = VGG16(include_top=False, weights="imagenet")
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter
  image_count_train = count_images("train")
  generator_train = make_generator(
      "train",
      data_gen_args={"preprocessing_function": preprocess_input},
      data_flow_args={"batch_size": 1,
                      "class_mode": None,
                      "shuffle": False})
  generator_train_labels = make_generator(
      "train",
      data_gen_args={},
      data_flow_args={"batch_size": BATCH_SIZE,
                      "shuffle": False})
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter 
  labels_train = []
  i = 0
  for _, labels in generator_train_labels:
      if i >= image_count_train:
          break
      else:
          labels_train.extend(labels)
          i += BATCH_SIZE
  labels_train = np.array(labels_train[:image_count_train])
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter 
  bottleneck_features_train = model.predict_generator(generator_train,
                                                      image_count_train)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter 
  with open(os.path.join(DATA_DIR, 'bottleneck_features_train.npy'), 'bw') as train:
      np.save(train, bottleneck_features_train)
  with open(os.path.join(DATA_DIR, 'bottleneck_feature_labels_train.npy'), 'bw') as train:
      np.save(train, labels_train)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter 
  image_count_valid = count_images("valid")
  generator_valid = make_generator(
      "valid",
      data_gen_args={"preprocessing_function": preprocess_input},
      data_flow_args={"batch_size": 1,
                      "class_mode": None,
                      "shuffle": False})
  generator_valid_labels = make_generator(
      "valid",
      data_gen_args={},
      data_flow_args={"batch_size": BATCH_SIZE,
                      "shuffle": False})
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter 
  labels_valid = []
  for i, (_, labels) in enumerate(generator_valid_labels):
      if i >= image_count_valid:
          break
      else:
          labels_valid.extend(labels)
  labels_valid = np.array(labels_valid[:image_count_valid])
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter 
  bottleneck_features_valid = model.predict_generator(generator_valid,
                                                      image_count_valid)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter 
  with open(os.path.join(DATA_DIR, 'bottleneck_features_valid.npy'), 'bw') as valid:
      np.save(valid, bottleneck_features_valid)

  with open(os.path.join(DATA_DIR, 'bottleneck_feature_labels_valid.npy'), 'bw') as valid:
      np.save(valid, labels_valid)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter 
  image_count_test = count_images("test")
  image_count_test_generator_cutoff = image_count_test / BIG_BATCH_SIZE + 1

  generator_test = make_generator(
      "test",
      data_gen_args={"preprocessing_function": preprocess_input},
      data_flow_args={"batch_size": BIG_BATCH_SIZE,
                      "class_mode": None,
                      "shuffle": False})
  generator_test_labels = make_generator(
      "test",
      data_gen_args={},
      data_flow_args={"batch_size": BIG_BATCH_SIZE,
                      "shuffle": False})

  labels_test = []
  for i, (_, labels) in enumerate(generator_test_labels):
      if i >= image_count_test_generator_cutoff:
          break
      else:
          labels_test.extend(labels)
  labels_test = np.array(labels_test[:image_count_test])

  bottleneck_features_test = model.predict_generator(generator_test,
                                                     image_count_test_generator_cutoff)
  bottleneck_features_test = bottleneck_features_test[:image_count_test]

  np.savez_compressed(os.path.join(DATADIR, 'bottleneck_features_test'), bottleneck_features_test)
  np.savez_compressed(os.path.join(DATADIR, 'bottleneck_feature_labels_test'), labels_test)
#+END_SRC

#+BEGIN_SRC jupyter 
  train_data = np.load(os.path.join(DATADIR, 'bottleneck_features_train.npz'))
  train_data = train_data['arr_0']

  train_labels = np.load(os.path.join(DATADIR, 'bottleneck_feature_labels_train.npz'))
  train_labels = train_labels['arr_0']

  valid_data = np.load(os.path.join(DATADIR, 'bottleneck_features_valid.npz'))
  valid_data = valid_data['arr_0']

  valid_labels = np.load(os.path.join(DATADIR, 'bottleneck_feature_labels_valid.npz'))
  valid_labels = valid_labels['arr_0']

  test_data = np.load(os.path.join(DATADIR, 'bottleneck_features_test.npz'))
  test_data = test_data['arr_0']

  test_labels = np.load(os.path.join(DATADIR, 'bottleneck_feature_labels_test.npz'))
  test_labels = test_labels['arr_0']
#+END_SRC

#+BEGIN_SRC jupyter
  from keras.models import Sequential
  from keras.optimizers import Adam
  from keras.layers import Dropout, Flatten, Dense

  def init_model(input_shape, target_num=NUM_CHARACTERS,
                 learning_rate=LEARNING_RATE):
      model = Sequential()
      model.add(Flatten(input_shape=input_shape))
      model.add(Dense(256, activation='relu', kernel_initializer='lecun_uniform'))
      model.add(Dropout(0.5))
      model.add(Dense(target_num, activation='softmax'))

      model.compile(optimizer=Adam(lr=learning_rate),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
      return model
#+END_SRC

#+BEGIN_SRC jupyter 
  model = init_model(train_data.shape[1:])
#+END_SRC

#+BEGIN_SRC jupyter 
  model.fit(train_data, train_labels,
            epochs=50,
            batch_size=BATCH_SIZE,
            validation_data=(valid_data, valid_labels))
#+END_SRC

*** Lessons Learned
Ugh, well … for whatever reason I can’t make the damn thing learn from this approach.  Maybe I don’t have enough data in my classes?  That doesn’t make sense, though.  Maybe my learning rate is too high?  I think I remember that happened with the full model.

--------------------------------------------------------------------------------

Okay, I dropped the learning rate down and it’s learning now.  But it never gets past around 70% accuracy, plus or minus 5.  Past that point it just starts overfitting, in fact the validation loss gets constant for some reason.

I tried adding more hidden layers, that did nothing.  I remember the advice that yeah, that never solves the fundamental problem.  So now I’ve widened out the one layer (128 to 256) to try to capture enough features (I think that’s what this does?)  We’ll see how that goes.  It doesn’t learn as quickly, the loss started high and isn’t dropping nearly as fast … but maybe this one will get higher accuracy in the end?

If not, maybe I just need more data.  Ugh.

--------------------------------------------------------------------------------

So I’m trying smaller hidden layers with dropout in between.  Near as I can tell that just slows down the learning rate.  Ugh.

I get the sense that that’s a tradeoff with most regularization techniques, be it dropout or batch norm.  It slows down learning (though you’re supposed to increase the learning rate to compensate, I guess).

Okay, yeah, let’s just get more data.

*** Train Tiny Net on my Laptop
This is the goal of all that expensive work I did on the beefy Amazon machine.  I cached the results of VGG16 feature extraction so that I could (fingers crossed) train a tiny FC network on my laptop.  Let’s try it.

--------------------------------------------------------------------------------

nope, that doesn’t work.  still too much data to train on my laptop, it takes over 10 min per epoch.

but I can rent out a cheapo Amazon ec2 (p2.xlarge for ~0.30 / hr) and get it to around 90s per epoch, so that’s a win

*** Train Tiny Net on Amazon
:PROPERTIES:
:header-args:jupyter+: :existing kernel-28d1e7fe-a35d-44d5-95a6-7d98eb62b071.json
:header-args:jupyter+: :ssh amazon-west
:header-args:jupyter+: :session smash-remote
:END:

#+CALL: get_remote_kernel_name(remote="amazon-west")

#+RESULTS:
: kernel-28d1e7fe-a35d-44d5-95a6-7d98eb62b071.json

#+CALL: get_remote_kernel_json(remote="amazon-west")

#+RESULTS:
: /run/user/1000/jupyter/kernel-28d1e7fe-a35d-44d5-95a6-7d98eb62b071.json

#+CALL: copy_remote_kernel_json(remote_file="/run/user/1000/jupyter/kernel-28d1e7fe-a35d-44d5-95a6-7d98eb62b071.json", remote="amazon-west")

#+RESULTS:
| kernel-28d1e7fe-a35d-44d5-95a6-7d98eb62b071.json |
| kernel-930257cb-ef69-4b9c-8a83-b1efd9f7e985.json |
| kernel-9e3bbebe-fa1c-4ab5-b1ed-857b240cd8d7.json |
| kernel-bcc7f36c-117a-4af3-9c4e-b4051f4fabab.json |

#+BEGIN_SRC jupyter 
"trivial setup complete"
#+END_SRC

#+RESULTS:
: trivial setup complete

Except god dammit.  I trained the model for a bazillion epochs, and it could only get up to 50% accuracy.  What the hell?

It’s seriously looking like the only way for me to get good results is to train the full model on all my data.  Let’s look into that expensive ec2 again.

*** Train Big Model on Amazon
:PROPERTIES:
:header-args:jupyter+: :existing kernel-e998e99b-a28a-45e2-957b-20397a6b678c.json
:header-args:jupyter+: :ssh amazon-west
:header-args:jupyter+: :session smash-remote
:END:

#+CALL: get_remote_kernel_json(remote="amazon-west")

#+RESULTS:
: /run/user/1000/jupyter/kernel-e998e99b-a28a-45e2-957b-20397a6b678c.json

#+CALL: copy_remote_kernel_json(remote_file="/run/user/1000/jupyter/kernel-e998e99b-a28a-45e2-957b-20397a6b678c.json", remote="amazon-west")

#+RESULTS:
| kernel-28d1e7fe-a35d-44d5-95a6-7d98eb62b071.json |
| kernel-930257cb-ef69-4b9c-8a83-b1efd9f7e985.json |
| kernel-9e3bbebe-fa1c-4ab5-b1ed-857b240cd8d7.json |
| kernel-b297ae86-4dd7-4306-ac76-40a58503bab8.json |
| kernel-bcc7f36c-117a-4af3-9c4e-b4051f4fabab.json |
| kernel-e998e99b-a28a-45e2-957b-20397a6b678c.json |

#+BEGIN_SRC jupyter 
  "trivial setup complete"
#+END_SRC

#+RESULTS:
: trivial setup complete

Well this is enlightening / embarassing.  I rented out the cheapo amazon (p2.xlarge) and I’m training the full model.  Buuuuut I’m using just the battlefield images.  Up to 80% accuracy after just four epochs.  I’m starting to think I should spin up more instances to check the other stages.  Presumably one of them just sucks?  Fountain?  Yoshi’s?

And then maybe a thing to try is multiclass classification?  Like how [[https://www.pyimagesearch.com/2018/05/07/multi-label-classification-with-keras/][the master salesman]] does it?

I’m a little frustrated cuz it seems my browser has lost the connection to the notebook output stream.  So I’m not getting real time updates anymore of progress.

Looks like the epochs are down to 20 minutes, too.  I guess I’ll just try to check in every hour and see if it’s done yet.  I can check the stats afterward.

--------------------------------------------------------------------------------

Real interesting.  I started a parallel process to do just fountain, and it seems like it’s gonna suck.  Too early to call it, but that’s my initial impression.

Annnnnd yeah, the battlefield ran to completion (I forgot to check the final epoch accuracy, but I’m hoping it was around 90%.  Almost certainly it was above 89%.  And I need to build the testing code tomorrow).  The fountain code never really got off the ground, though it did keep running forever.  Somehow it kept eeking out just the barest improvements on the validation loss metric … but it never really made big gains in either loss or accuracy.  I just killed it after like 35 epochs.

New test is final and story.  I’m expecting final to do well, story to do poorly.  Check in the morning.

--------------------------------------------------------------------------------

Whelp, my prediction was super wrong.  Final doing well, story doing just as well.  Ugh.

Time to try dreamland and stadium just to see if they also work well.  Wtf is wrong with fountain?  Is there something wrong with fountain?  Or did I just get unlucky that one time?

It’s kinda interesting that dreamland is doing really well and stadium is doing just okay.  Stadium is getting better and better with no signs of slowing down, but its rate of improvement is way slower than dreamland, or final, or battlefield.  How much of this is randomness at work?  I just don’t know.

--------------------------------------------------------------------------------

Well now I’m trying again with mixing stages.  Dreamland, final, and battlefield (since those were the three that went quickly by themselves).  Let’s see if this trains at all?

Yeah, looks like it’s gonna train well.

*** Lessons Learned
:PROPERTIES:
:header-args:jupyter+: :existing 'nil
:header-args:jupyter+: :ssh 'nil
:header-args:jupyter+: :session smash
:END:

Even though my training accuracy would eventually get up above 80%, the validation accuracy never did.  And of course, with fountain it never did well at all.

I could chalk fountain up to bad luck, but let’s assume not and instead investigate the images.  Some reason they are bad for training?  And if so … does fixing that cross apply to the other stages?

#+BEGIN_SRC jupyter 
"trivial setup complete"
#+END_SRC

#+RESULTS:
: trivial setup complete

Let’s just check out some random masks vs images and see what we see.

#+CALL: setup_libraries()

#+RESULTS:

#+BEGIN_SRC jupyter 
import os
import re
import random
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter
  def keras_image_and_mask(character, color, stage, orientation,
                                img_number):
      "Create two side-by-side plots of a scenario."
      data_dir = './data/keras/train/'
      image = os.path.join(
          data_dir, 'images', character,
          f'{character}_{color}_{stage}_{orientation}_bg_on_{img_number}.jpg')
      mask = os.path.join(
          data_dir, 'masks', character,
          f'{character}_{color}_{stage}_{orientation}_bg_off_{img_number}_mask.jpg')

      bgr_image = cv2.imread(image)
      rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)
      grey_mask = cv2.imread(mask)

      return rgb_image, grey_mask
#+END_SRC

#+RESULTS:

#+HEADER: :var character="falco"
#+HEADER: :var color=3
#+HEADER: :var stage="battlefield"
#+HEADER: :var orientation="left"
#+HEADER: :var img_number=185
#+BEGIN_SRC jupyter :results file
  rgb_image, grey_mask = keras_image_and_mask(character, color, stage,
                                             orientation, img_number)
  fig, (ax1, ax2) = plt.subplots(1, 2)
  ax1.imshow(rgb_image)
  ax2.imshow(grey_mask)
#+END_SRC

#+RESULTS:
[[file:./jupyter_images/15075pd.png]]

#+BEGIN_SRC jupyter
  def list_all_images(directory="train"):
      data_dir = './data/keras/'
      image_dir = os.path.join(data_dir, directory, 'images')
      all_images = []

      for root, dirs, files in os.walk(image_dir):
          all_images.extend(files)

      return all_images
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter 
  all_images = list_all_images()
  img_re = re.compile('(?:[^/]*/)*'
                      '(?P<character>[^_]*)_'
                      '(?P<color>[^_]*)_'
                      '(?P<stage>[^_]*)_'
                      '(?P<orientation>[^_]*)_'
                      'bg_on_'
                      '(?P<img_number>[0-9]{3})'
                      '.jpg')
#+END_SRC

#+RESULTS:

#+HEADER: :var num_samples=3
#+BEGIN_SRC jupyter :results file
  fig, axarr = plt.subplots(num_samples, 3, sharex=True, sharey=True,
                            squeeze=False)

  for i, image_name in enumerate(random.sample(all_images, num_samples)):
      match = re.search(img_re, image_name)
      rgb_image, grey_mask = keras_image_and_mask(**match.groupdict())
      ax_left, ax_center, ax_right = axarr[i, 0], axarr[i, 1], axarr[i, 2]

      ax_left.tick_params(axis='both', which='both', bottom='off',
                              top='off', labelbottom='off', right='off',
                              left='off', labelleft='off')
      ax_left.imshow(rgb_image)
      ax_center.tick_params(axis='both', which='both', bottom='off',
                              top='off', labelbottom='off', right='off',
                              left='off', labelleft='off')
      ax_center.imshow(grey_mask)
      ax_right.tick_params(axis='both', which='both', bottom='off',
                              top='off', labelbottom='off', right='off',
                              left='off', labelleft='off')
      ax_right.text(0.5, 0.5, '\n'.join(match.groups()),
                    horizontalalignment='center', verticalalignment='center',
                    transform=ax_right.transAxes)

  fig.tight_layout()
#+END_SRC

#+RESULTS:
[[file:./jupyter_images/15071En.png]]

Some things I’m seeing:
 - falcon 1 (grey falcon) doesn’t get masked out very well
 - but more importantly … fountain just has a lot of background noise
   
So I’m gonna guess that’s why fountain classification just sucks.  The background is so loud it can barely figure out what’s character and what’s not.  I wonder if the object segmentation task will be easier?

At any rate, I’m guessing it’s the relative size of the objects that’s making classification kinda hard.  Maybe?  Certainly all the examples on the internet use really big well-cropped images.

Let’s go back and try different models to see if we can get better classification results.

*** Train Big Model on Amazon
:PROPERTIES:
:header-args:jupyter+: :existing kernel-e998e99b-a28a-45e2-957b-20397a6b678c.json
:header-args:jupyter+: :ssh amazon-west
:header-args:jupyter+: :session smash-remote
:END:

#+CALL: get_remote_kernel_json(remote="amazon-west")

#+RESULTS:
: /run/user/1000/jupyter/kernel-e998e99b-a28a-45e2-957b-20397a6b678c.json

#+CALL: copy_remote_kernel_json(remote_file="/run/user/1000/jupyter/kernel-e998e99b-a28a-45e2-957b-20397a6b678c.json", remote="amazon-west")

#+RESULTS:
| kernel-28d1e7fe-a35d-44d5-95a6-7d98eb62b071.json |
| kernel-930257cb-ef69-4b9c-8a83-b1efd9f7e985.json |
| kernel-9e3bbebe-fa1c-4ab5-b1ed-857b240cd8d7.json |
| kernel-b297ae86-4dd7-4306-ac76-40a58503bab8.json |
| kernel-bcc7f36c-117a-4af3-9c4e-b4051f4fabab.json |
| kernel-e998e99b-a28a-45e2-957b-20397a6b678c.json |

#+BEGIN_SRC jupyter 
  "trivial setup complete"
#+END_SRC

#+RESULTS:
: trivial setup complete

Well this is enlightening / embarassing.  I rented out the cheapo amazon (p2.xlarge) and I’m training the full model.  Buuuuut I’m using just the battlefield images.  Up to 80% accuracy after just four epochs.  I’m starting to think I should spin up more instances to check the other stages.  Presumably one of them just sucks?  Fountain?  Yoshi’s?

And then maybe a thing to try is multiclass classification?  Like how [[https://www.pyimagesearch.com/2018/05/07/multi-label-classification-with-keras/][the master salesman]] does it?

I’m a little frustrated cuz it seems my browser has lost the connection to the notebook output stream.  So I’m not getting real time updates anymore of progress.

Looks like the epochs are down to 20 minutes, too.  I guess I’ll just try to check in every hour and see if it’s done yet.  I can check the stats afterward.

--------------------------------------------------------------------------------

Real interesting.  I started a parallel process to do just fountain, and it seems like it’s gonna suck.  Too early to call it, but that’s my initial impression.

Annnnnd yeah, the battlefield ran to completion (I forgot to check the final epoch accuracy, but I’m hoping it was around 90%.  Almost certainly it was above 89%.  And I need to build the testing code tomorrow).  The fountain code never really got off the ground, though it did keep running forever.  Somehow it kept eeking out just the barest improvements on the validation loss metric … but it never really made big gains in either loss or accuracy.  I just killed it after like 35 epochs.

New test is final and story.  I’m expecting final to do well, story to do poorly.  Check in the morning.

--------------------------------------------------------------------------------

Whelp, my prediction was super wrong.  Final doing well, story doing just as well.  Ugh.

Time to try dreamland and stadium just to see if they also work well.  Wtf is wrong with fountain?  Is there something wrong with fountain?  Or did I just get unlucky that one time?

It’s kinda interesting that dreamland is doing really well and stadium is doing just okay.  Stadium is getting better and better with no signs of slowing down, but its rate of improvement is way slower than dreamland, or final, or battlefield.  How much of this is randomness at work?  I just don’t know.

--------------------------------------------------------------------------------

Well now I’m trying again with mixing stages.  Dreamland, final, and battlefield (since those were the three that went quickly by themselves).  Let’s see if this trains at all?

Yeah, looks like it’s gonna train well.

--------------------------------------------------------------------------------

Been running the model a few more times.  Just the good stages.  But now with dropout.

That definitely does help with overfitting.  The old stuff would get up to like 85% train accuracy, but never above 79 / 80% validation accuracy.  And I’m sure the losses showed something similar.  

With dropout of 20% the two stayed basically in lock step the whole time.  The training rate really suffered, though.  After like 20 odd epochs it never got much above 55% training accuracy.  Although, at the same time, I’m actually using only about half the images.  That’s the next thing I should do, try dropout with more images.  I did cut it down to 10% dropout, though, to speed training.  It’s looking like it’s diverging at about 55% accuracy again.  That’s gotta be just a function of the number of images you’re using.

I’m also trying a model with small dropout, fewer images, but a wider hidden layer.  Training is super slow again, but maybe this will get more accurate?

--------------------------------------------------------------------------------

Okay so with 10% dropout they do eventually diverge by that same 5% accuracy.  Is that just inevitable?  Or would 20% dropout help?  

And the wide hidden layer is enabling better fit, even after validation.  There’s still a 5% gap in train / valid accuracy, but it’s a higher max than the narrower layer.

So what do we do?  Let’s try 20% dropout and wide network.  And a second one that’s 20% dropout, wide layer, but two hidden layers.

--------------------------------------------------------------------------------

It’s weird.  On the one hand, the one layer got up to greater accuracy even on the validation accuracy.  But it was clearly overfitting a bit.  It got up to 66% accuracy, 60% validation accuracy.  On the other hand, the two layers petered out at 56% accuracy with no overfitting.

I forced the two layer model to keep going … it’s up to 57% now, after almost an hour, but at least it’s not overfitting.  Ugh.

Could be that I just need to run more data through at this point.

Also, I just realized I can use the caching trick on the beefy computer.  It’s got more than enough RAM.  Just don’t bother trying to persist it to disk, it’s not worth it.

*** Train Tiny Model on Big Amazon
