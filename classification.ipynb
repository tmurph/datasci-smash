{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classification Task Writeup\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- [Executive Summary](#Executive-Summary)\n",
        "- [What is Transfer Learning?](#What-is-Transfer-Learning?)\n",
        "- [Transfer Learning Code](#Transfer-Learning-Code)\n",
        " - [Review the Data](#Review-the-Data)\n",
        " - [Extract Features](#Extract-Features)\n",
        " - [Train the Network](#Train-the-Network)\n",
        " - [Test the Network](#Test-the-Network)\n",
        "- [Discussion](#Discussion)\n",
        "- [Appendix - Data Generation](#Appendix---Data-Generation)\n",
        " - [Install Dolphin](#Install-Dolphin)\n",
        " - [Make Images and Masks](#Make-Images-and-Masks)\n",
        "- [Appendix - Data Preprocessing](#Appendix---Data-Preproces",
        "sing)\n",
        " - [Set up AWS](#Set-up-AWS)\n",
        " - [Split Data into Train/Test/Valid](#Split-Data-into-Train",
        "/Test/Valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: **I am a data scientist in the Bay Area!  Feel free to",
        " contact me with questions, concerns, offers for work ;)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- [trevor.m.murphy@gmail.com](mailto:trevor.m.murphy@gmail.c",
        "om)\n",
        "- [LinkedIn](https://www.linkedin.com/in/trevor-murphy-49ba1",
        "421)\n",
        "- [GitHub](https://github.com/tmurph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Executive Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook contains the code and resources to recognize t",
        "he most common player avatars (hereafter, characters) used b",
        "y professional players of the competitive fighting game Supe",
        "r Smash Bros. Melee."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A neural network is given images captured from scripted game",
        " play and trained via transfer learning from VGG16.  In test",
        "s the network achieves ~96% classification accuracy after ~1",
        "5 training epochs.  Here is a representative confusion matri",
        "x obtained after one such test:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](jupyter_images/confusion_matrix.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# What is Transfer Learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following contains merely the briefest description of tr",
        "ansfer learning.  [Many](https://machinelearningmastery.com/",
        "transfer-learning-for-deep-learning/) [more](http://ruder.io",
        "/transfer-learning/) [thorough](https://www.analyticsvidhya.",
        "com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-",
        "pre-trained-model/) [explanations](http://ftp.cs.wisc.edu/ma",
        "chine-learning/shavlik-group/torrey.handbook09.pdf) exist an",
        "d can be found with a simple Google search.  I assume the re",
        "ader is already familiar with convolutional neural networks ",
        "(CNN)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With this project I aimed to train a neural network to class",
        "ify custom-generated data with greater than 90% classificati",
        "on accuracy.  One might construct a neural network from scra",
        "tch for this purpose, but such an approach would also raise ",
        "significant challenges and force difficult tradeoffs between",
        " depth and cost.  The desired accuracy would require a suffi",
        "ciently deep network, while the cost to train such a network",
        " would increase rapidly with depth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A common alternative to from-scratch training is transfer le",
        "arning, where pre-trained, state-of-the-art image classifica",
        "tion models are repurposed to suit a data set that may diffe",
        "r significantly from the original training data.  When this ",
        "approach works it can significantly reduce the amount of tra",
        "ining (and therefore cost) required to achieve the desired c",
        "lassification accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first step of a transfer learning solution is selecting ",
        "a model.  The [ImageNet Large Scale Visual Recognition Chall",
        "enge](http://image-net.org/challenges/LSVRC/) attracts the a",
        "ttention of brilliant researchers and the resources of unive",
        "rsities and large internet companies.  As such, winning mode",
        "ls make excellent starting points for transfer learning.  No",
        "t all models are equally easy to repurpose for any given dat",
        "a set, however.  \n",
        "  \n",
        "Following [this blog post from the Keras developers](https:/",
        "/blog.keras.io/building-powerful-image-classification-models",
        "-using-very-little-data.html) I selected VGG16 for this proj",
        "ect.  My custom-generated images differ in size from the Ima",
        "geNet training data, but VGG16’s simple convolutional archit",
        "ecture makes it uniquely capable of adapting to this differe",
        "nce.  \n",
        "  \n",
        "The last step in transfer learning is called “fine-tuning” a",
        "nd involves retraining or replacing the final layers of the ",
        "model to increase the accuracy of predictions on the new dat",
        "a set.  The Keras library makes this particularly easy to do",
        "."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transfer Learning Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Review the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, let me show some of the images used to train the CNN.",
        "  Over 100,000 images were used, so this is just a random sa",
        "mple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [16, 12]\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "DATA_DIR = \"data/keras\"\n",
        "\n",
        "all_images = []\n",
        "for root, dirs, files in os.walk(os.path.join(DATA_DIR, 'tra",
        "in', 'images')):\n",
        "    all_images.extend(files)\n",
        "\n",
        "img_re = re.compile('(?:[^/]*/)*'\n",
        "                    '(?P<character>[^_]*)_'\n",
        "                    '(?P<color>[^_]*)_'\n",
        "                    '(?P<stage>[^_]*)_'\n",
        "                    '(?P<orientation>[^_]*)_'\n",
        "                    'bg_on_'\n",
        "                    '(?P<img_number>[0-9]{3})'\n",
        "                    '.jpg')\n",
        "\n",
        "def get_image(character, color, stage, orientation, img_numb",
        "er):\n",
        "    image = os.path.join(DATA_DIR, 'train', 'images', charac",
        "ter,\n",
        "                         '{character}_{color}_{stage}_{orien",
        "tation}_bg_on_{img_number}.jpg'.format(character=character,\n",
        "                                                            ",
        "                                       color=color,\n",
        "                                                            ",
        "                                       stage=stage,\n",
        "                                                            ",
        "                                       orientation=orientati",
        "on,\n",
        "                                                            ",
        "                                       img_number=img_number",
        "))\n",
        "\n",
        "    rgb_image = plt.imread(image)\n",
        "\n",
        "    return rgb_image\n",
        "\n",
        "def get_image_by_name(name):\n",
        "    image_groups = re.search(img_re, name)\n",
        "    return get_image(**image_groups.groupdict())\n",
        "\n",
        "IMAGE_ROW_SIZE, IMAGE_COLUMN_SIZE, IMAGE_DEPTH = get_image_b",
        "y_name(all_images[0]).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "NUM_ROWS = 4\n",
        "NUM_COLS = 4\n",
        "\n",
        "fig, axarr = plt.subplots(NUM_ROWS, NUM_COLS, sharex=True, s",
        "harey=True, squeeze=False)\n",
        "\n",
        "for i, image_name in enumerate(random.sample(all_images, NUM",
        "_ROWS * NUM_COLS)):\n",
        "    ax = axarr[divmod(i, NUM_COLS)]\n",
        "    ax.tick_params(axis='both', which='both', bottom='off', ",
        "top='off', labelbottom='off', right='off', left='off', label",
        "left='off')\n",
        "    ax.imshow(get_image_by_name(image_name))\n",
        "\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These images were all captured from scripted gameplay.  The ",
        "[Data Generation](#Data-Generation) section includes technic",
        "al details on the capturing process.  \n",
        "  \n",
        "Compare these images against the images used in the ImageNet",
        " competition to train VGG16.  E.g. [these images from the pe",
        "rsian cat category](http://imagenet.stanford.edu/synset?wnid",
        "=n02123394)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Without additional metadata about the captured images, such ",
        "as character bounding boxes or pose annotation points, I can",
        "not easily compute rich statistics about the images.  Howeve",
        "r, visual inspection suggests the following differences to m",
        "e:  \n",
        "  \n",
        "- **Size within frame**\n",
        "  - Characters in captured images rarely fill more than 10% ",
        "of the frame\n",
        "  - Objects in ImageNet images often fill 25% or more of the",
        " frame\n",
        "- **Variation of pose / orientation**\n",
        "  - Characters in captured images, coming from a video game,",
        " contort into an extremely wide variety of poses\n",
        "  - Objects in ImageNet images are constrained by real biolo",
        "gy\n",
        "- **Composition within frame**\n",
        "  - Captured images frequently include visual effects like s",
        "moke and explosions, which can occlude parts of the characte",
        "rs\n",
        "  - In the video game characters frequently fall off the sid",
        "e of the frame, resulting in capture images that cut off par",
        "t of the character\n",
        "  - ImageNet images are comparatively well composed, placing",
        " the object in the front of the frame and fairly close to th",
        "e center  \n",
        "  \n",
        "Despite these differences, VGG16 is able to extract useful f",
        "eatures that lead to accurate image classification.  This is",
        " the power of award-winning neural networks and transfer lea",
        "rning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running image data through VGG16 is computationally expensiv",
        "e, so I will do it just once and cache the results for the l",
        "ater fine-tuning work.  These intermediate results I am cach",
        "ing are typically thought of as “important features” that ha",
        "ve been “extracted” from the image data.  The extracted feat",
        "ures are realized as a very large floating point array.  \n",
        "  \n",
        "This caching technique trades off increased memory requireme",
        "nts against decreased computational time.  Fortunately, memo",
        "ry is cheap relative to GPU compute, so this trade off makes",
        " sense for my problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code blocks set up the VGG model.  \n",
        "  \n",
        "I want to take advantage of all the available GPUs, which I ",
        "accomplish with the function `keras.utils.multi_gpu_model`. ",
        " This function accepts a template model (typically compiled ",
        "for a CPU-only architecture) and creates an identical model ",
        "compiled for a multi-GPU architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.utils import multi_gpu_model\n",
        "from keras.models import clone_model\n",
        "from GPUtil import getGPUs\n",
        "\n",
        "try:\n",
        "    NUM_GPU = len(getGPUs())\n",
        "except:\n",
        "    NUM_GPU = 0\n",
        "\n",
        "def gpuify_model_maybe(model):\n",
        "    result = None\n",
        "    if NUM_GPU >= 2:\n",
        "        result = multi_gpu_model(model, gpus=NUM_GPU)\n",
        "    elif NUM_GPU == 1:\n",
        "        with tf.device(\"/gpu:0\"):\n",
        "            result = clone_model(model)\n",
        "    else:\n",
        "        result = model\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with tf.device(\"/cpu:0\"):\n",
        "    template_vgg_model = VGG16(include_top=False, weights=\"i",
        "magenet\", input_shape=(IMAGE_ROW_SIZE, IMAGE_COLUMN_SIZE, IM",
        "AGE_DEPTH))\n",
        "gpu_vgg_model = gpuify_model_maybe(template_vgg_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code blocks set up the data flow into the mode",
        "l.  \n",
        "  \n",
        "As with most transfer learning work, I must preprocess the i",
        "mages to match key statistical features of the original data",
        " set.  The function `keras.applications.imagenet_utils.prepr",
        "ocess_input` does precisely this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.imagenet_utils import preprocess_inp",
        "ut\n",
        "\n",
        "BATCH_SIZE = 16 * max(NUM_GPU, 1)\n",
        "\n",
        "def make_generator(folder=\"train\",\n",
        "                   data_gen_args={\"fill_mode\": \"constant\",\n",
        "                                  \"cval\": 0,\n",
        "                                  \"width_shift_range\": 0.05,",
        "\n",
        "                                  \"height_shift_range\": 0.05",
        ",\n",
        "                                  \"zoom_range\": 0.1,\n",
        "                                  \"horizontal_flip\": True,\n",
        "                                  \"preprocessing_function\": ",
        "preprocess_input},\n",
        "                   data_flow_args={\"seed\": 1,\n",
        "                                   \"batch_size\": BATCH_SIZE}",
        "):\n",
        "\n",
        "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
        "\n",
        "    image_generator = image_datagen.flow_from_directory(\n",
        "        directory=os.path.join(DATA_DIR, folder, \"images\"),\n",
        "        target_size=(IMAGE_ROW_SIZE, IMAGE_COLUMN_SIZE),\n",
        "        color_mode='rgb',\n",
        "        **data_flow_args)\n",
        "\n",
        "    return image_generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Due to the design of the Keras library, I can’t retain the i",
        "mage labels through the image feature extraction code.  To w",
        "ork around this, I iterate over the images twice, in the sam",
        "e deterministic order both times, pulling out features one t",
        "ime and labels the other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For feature extraction, I use the `model.predict_generator` ",
        "API, which requires a generator that emits solely arrays of ",
        "images (as opposed to pairs like `(image_array, label_array)",
        "`).  To get this required generator I set the keyword parame",
        "ter `class_mode=None`.  I also set the keyword parameter `sh",
        "uffle=False` so that the features are extracted in determini",
        "stic order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_generator_train = make_generator(\"train\", data_gen_arg",
        "s={\"preprocessing_function\": preprocess_input},\n",
        "                                       data_flow_args={\"batc",
        "h_size\": BATCH_SIZE, \"class_mode\": None, \"shuffle\": False})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For label extraction, I set the keyword parameter `shuffle=F",
        "alse` again to iterate in the same order as above, but I fal",
        "l back to the default `class_mode` in order to retain image ",
        "labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "label_generator_train = make_generator(\"train\", data_gen_arg",
        "s={}, data_flow_args={\"batch_size\": BATCH_SIZE, \"shuffle\": F",
        "alse})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Keras’ image data generator API will actually generate image",
        "s indefinitely.  This supports the common use case of contin",
        "uously running a neural network for an arbitrary number of e",
        "pochs.  To end the generation early, I first define a few he",
        "lper functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_images(folder=\"train\"):\n",
        "    image_directory = os.path.join(DATA_DIR, folder, \"images",
        "\")\n",
        "    data_size = 0\n",
        "\n",
        "    for char_name in os.listdir(image_directory):\n",
        "        char_directory = os.path.join(image_directory, char_",
        "name)\n",
        "        data_size += len(os.listdir(char_directory))\n",
        "\n",
        "    return data_size\n",
        "\n",
        "def truncate_generator(gen, limit=None):\n",
        "    if limit is None:\n",
        "        return\n",
        "\n",
        "    for i, elt in enumerate(gen):\n",
        "        if i < limit:\n",
        "            yield elt\n",
        "        else:\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now I extract the training image features and labels.  Note ",
        "that the following code can take a significant amount of tim",
        "e to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_count_train = count_images(\"train\")\n",
        "image_count_train_generator_cutoff = image_count_train / BAT",
        "CH_SIZE + 1\n",
        "\n",
        "train_labels = []\n",
        "for _, labels in truncate_generator(label_generator_train, i",
        "mage_count_train_generator_cutoff):\n",
        "    train_labels.extend(labels)\n",
        "train_labels = np.array(train_labels[:image_count_train])\n",
        "\n",
        "train_data = gpu_vgg_model.predict_generator(image_generator",
        "_train, steps=image_count_train_generator_cutoff)\n",
        "train_data = train_data[:image_count_train]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now I extract the analogous pair of arrays for the validatio",
        "n images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_count_valid = count_images(\"valid\")\n",
        "image_count_valid_generator_cutoff = image_count_valid / BAT",
        "CH_SIZE + 1\n",
        "\n",
        "image_generator_valid = make_generator(\"valid\", data_gen_arg",
        "s={\"preprocessing_function\": preprocess_input},\n",
        "                                       data_flow_args={\"batc",
        "h_size\": BATCH_SIZE, \"class_mode\": None, \"shuffle\": False})\n",
        "label_generator_valid = make_generator(\"valid\", data_gen_arg",
        "s={}, data_flow_args={\"batch_size\": BATCH_SIZE, \"shuffle\": F",
        "alse})\n",
        "\n",
        "valid_labels = []\n",
        "for _, labels in truncate_generator(label_generator_valid, i",
        "mage_count_valid_generator_cutoff):\n",
        "    valid_labels.extend(labels)\n",
        "valid_labels = np.array(valid_labels[:image_count_valid])\n",
        "\n",
        "valid_data = gpu_vgg_model.predict_generator(image_generator",
        "_valid, steps=image_count_valid_generator_cutoff)\n",
        "valid_data = valid_data[:image_count_valid]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now I have training and validation arrays, but they are in n",
        "on-random order.  Before training the neural network I rando",
        "mize these arrays, being careful to preserve the pairwise co",
        "rrelation between features and labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_random_index = np.arange(train_data.shape[0])\n",
        "np.random.shuffle(train_random_index)\n",
        "train_data = train_data[train_random_index]\n",
        "train_labels = train_labels[train_random_index]\n",
        "\n",
        "valid_random_index = np.arange(valid_data.shape[0])\n",
        "np.random.shuffle(valid_random_index)\n",
        "valid_data = valid_data[valid_random_index]\n",
        "valid_labels = valid_labels[valid_random_index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the following code, I build the model in a CPU environmen",
        "t but compile it for a GPU environment.  Because the class l",
        "abels are relatively balanced in terms of number of images, ",
        "I use categorical crossentropy as the loss function and clas",
        "sification accuracy as the headline metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dropout, Flatten, Dense\n",
        "\n",
        "LEARNING_RATE = 0.0001\n",
        "NUM_CHARACTERS = len(os.listdir(os.path.join(DATA_DIR, \"trai",
        "n\", \"images\")))\n",
        "\n",
        "def build_model(input_shape, target_num=NUM_CHARACTERS, hidd",
        "en_width=128, hidden_depth=1, dropout_rate=0.2):\n",
        "    with tf.device(\"/cpu:0\"):\n",
        "        model = Sequential()\n",
        "        model.add(Flatten(input_shape=input_shape))\n",
        "        for _ in range(hidden_depth):\n",
        "            model.add(Dense(hidden_width, activation='relu',",
        " kernel_initializer='lecun_uniform'))\n",
        "            model.add(Dropout(dropout_rate))\n",
        "        model.add(Dense(target_num, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "def compile_model(model, learning_rate=LEARNING_RATE):\n",
        "    model = gpuify_model_maybe(model)\n",
        "    model.compile(optimizer=Adam(lr=learning_rate), loss='ca",
        "tegorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following callbacks will, first, ensure early stopping i",
        "n case of a bad model, and, second, improve learning as the ",
        "model reaches diminishing returns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau",
        "\n",
        "\n",
        "stop_on_val_loss = EarlyStopping(monitor='val_loss', min_del",
        "ta=0.005, patience=3, verbose=0, mode='auto')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2",
        ", patience=2, min_lr=0.00001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here’s a quick summary of the model, as a sanity check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "template_model = build_model(train_data.shape[1:], hidden_wi",
        "dth=512, hidden_depth=3, dropout_rate=0.2)\n",
        "template_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally!  Time to train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compiled_model = compile_model(template_model)\n",
        "\n",
        "training_history = compiled_model.fit(train_data, train_labe",
        "ls,\n",
        "                                      epochs=15,\n",
        "                                      batch_size=BATCH_SIZE,",
        "\n",
        "                                      validation_data=(valid",
        "_data, valid_labels),\n",
        "                                      callbacks=[stop_on_val",
        "_loss, reduce_lr])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Due to a bug in Keras, I cannot directly save the weights an",
        "d structure of a model compiled for multiple GPUs.  However,",
        " the multi-GPU model object shares both structure and weight",
        "s with the template model object, which can be saved to disk",
        ".  \n",
        "  \n",
        "The `template_model`, however, is not yet useable for testin",
        "g / production.  It expects an array of VGG16 features, but ",
        "testing will instead provide images.  \n",
        "  \n",
        "The remedy is to first stack the `template_model` on top of ",
        "the `template_vgg_model` and then save the stacked result to",
        " disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "import datetime\n",
        "\n",
        "output_layer = template_vgg_model.output\n",
        "for layer in template_model.layers:\n",
        "    output_layer = layer(output_layer)\n",
        "full_model = Model(inputs=template_vgg_model.input, outputs=",
        "output_layer)\n",
        "full_model.save(os.path.join(DATA_DIR, datetime.datetime.tod",
        "ay().strftime('%Y-%m-%d-%H-%M') + \"-model.h5\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To compare true classification labels with the predictions t",
        "he model emits, I’ll iterate twice over the test images: one",
        " pass will run the images through the model to produce predi",
        "cted labels; the other will collect the true labels for the ",
        "images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_count_test = count_images(\"test\")\n",
        "image_count_test_generator_cutoff = image_count_test / BATCH",
        "_SIZE + 1\n",
        "\n",
        "image_generator_test = make_generator(\"test\", data_gen_args=",
        "{\"preprocessing_function\": preprocess_input},\n",
        "    data_flow_args={\"batch_size\": BATCH_SIZE, \"class_mode\": ",
        "None, \"shuffle\": False})\n",
        "label_generator_test = make_generator(\"test\", data_gen_args=",
        "{}, data_flow_args={\"batch_size\": BATCH_SIZE, \"shuffle\": Fal",
        "se})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "true_labels = []\n",
        "for _, labels in truncate_generator(label_generator_test, im",
        "age_count_test_generator_cutoff):\n",
        "    true_labels.extend(labels)\n",
        "true_labels = np.array(true_labels[:image_count_test])\n",
        "true_classes = true_labels.argmax(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_labels = full_model.predict_generator(image_generator_t",
        "est, steps=image_count_test_generator_cutoff)\n",
        "pred_labels = pred_labels[:image_count_test]\n",
        "pred_classes = pred_labels.argmax(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "c_matrix = confusion_matrix(true_classes, pred_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(matrix, classes, normalize=False, ",
        "title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`",
        ".\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        matrix = matrix.astype('float') / matrix.sum(axis=1)",
        "[:, np.newaxis]\n",
        "\n",
        "    plt.imshow(matrix, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = matrix.max() / 2.\n",
        "    for i, j in itertools.product(range(matrix.shape[0]), ra",
        "nge(matrix.shape[1])):\n",
        "        plt.text(j, i, format(matrix[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if matrix[i, j] > thresh else",
        " \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By default the class labels are encoded as integers, but the",
        " actual class names used by the `label_generator` object can",
        " be used instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_names = [c for c, i in sorted(label_generator_test.cla",
        "ss_indices.items(), key=lambda pair: pair[1])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the model accuracy!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_confusion_matrix(c_matrix, class_names, normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is a truth universally acknowledged, that every data scie",
        "nce project is 99% data and 1% science.  This project is har",
        "dly an exception.  Almost a thousand hours of individual eff",
        "ort went into the data generation process, while only a few ",
        "tens of hours went into the model fitting and testing.  It i",
        "s my hope that the generated images, as well as the flexible",
        " code behind them, will be of use to future projects; I have",
        " ambitions to train an object localizer and pose recognizer ",
        "next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From my testing of restricted data sets and various network ",
        "designs, the full data set of ~150k images is necessary to t",
        "rain a model to achieve above 95% accuracy.  I attribute thi",
        "s requirement to two limitations of the model: first, I am t",
        "ransferring VGG16 to a domain almost totally unlike the orig",
        "inal training environment; and second, my use of bottleneck ",
        "feature extraction prevents data augmentation techniques.  I",
        "f I had opted to spend more money on AWS compute, I could ha",
        "ve experimented with less data and a completely novel neural",
        " network architecture.  For reasons outlined above, I chose ",
        "to use the transfer learning technique specifically to take ",
        "advantage of my large data set and to reduce AWS compute exp",
        "enses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='Data-Generation'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Appendix - Data Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For interested parties, the following describes the code use",
        "d to generate the custom images.  Not all the code will be c",
        "overed, in particular I assume the reader is familiar with [",
        "GNU Make](https://www.gnu.org/software/make/manual/make.html",
        ") and can read the [Makefile](Makefile) included in this rep",
        "ository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The images were generated with the [Dolphin emulator](https:",
        "//dolphin-emu.org/) and [ffmpeg](https://www.ffmpeg.org/) on",
        " an early 2014 Macbook Air with a stock Intel HD Graphics 50",
        "00 graphics card."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Dolphin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dolphin 5.0 or higher is required.  You can [download](https",
        "://dolphin-emu.org/download/) and install the software from ",
        "the Dolphin website, or follow the [Dolphin wiki directions]",
        "(https://wiki.dolphin-emu.org/index.php?title=Installing_Dol",
        "phin) to install Dolphin via the package manager of your cho",
        "ice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before you can generate images with Dolphin you’ll need to p",
        "lace a copy of the Super Smash Bros. Melee v1.02 ISO in the ",
        "[scripts/record_avi directory](scripts/record_avi/).  It is ",
        "my understanding that possession of a digital copy of this I",
        "SO file does not violate copyright law provided you also own",
        " a physical copy of the disc, however I am not a lawyer.  I ",
        "do not recommend breaking the law in order to recreate this ",
        "analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before you run any image generation code you will also need ",
        "to configure Dolphin for automation:  \n",
        "  \n",
        "- **Pause at the end of recording**\n",
        "  - Ensure “Pause at End of Movie” under the “Movie” menu is",
        " checked.\n",
        "- **Don’t prompt for confirmation before closing Dolphin**\n",
        "  - Select “Configure…” under the “Dolphin” menu.\n",
        "  - Ensure “Confirm on Stop” under the “Interface” tab is un",
        "checked.\n",
        "- **Enable Melee Debug Menu**\n",
        "  - Select “Configure…” under the “Dolphin” menu.\n",
        "  - Ensure “Enable Cheats” under the “General” tab is checke",
        "d.\n",
        "  - Click the “OK” button.\n",
        "  - Open the Melee ISO and select “Cheat Manager” under the ",
        "“Tools” menu.\n",
        "  - Ensure “Debug Menu” under the “AR Codes” tab is checked.",
        "\n",
        "  - Click the “Apply” button.  \n",
        "  \n",
        "Once you have configured Dolphin, close the program.  Find y",
        "our default user directory and copy it into the `scripts/rec",
        "ord_avi` directory.  Rename the copied directory to `dolphin",
        "_user` if it is not already so named."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Make Images and Masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the command line execute `make` with no arguments to se",
        "e a detailed usage message, including the documentation of c",
        "onfiguration variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The images for this analysis were generated with the followi",
        "ng commands:  \n",
        "  \n",
        "`$ make dirs`  \n",
        "`$ make images`  \n",
        "`$ make hist`  \n",
        "`$ make masks`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that proper mask generation requires a masked video tha",
        "t is frame-for-frame synchronized with a corresponding image",
        " video, and thus Dolphin must run at the same CPU speed duri",
        "ng both image and mask video recording.  For this reason I r",
        "ecorded all videos on my laptop, as I could not force EC2 in",
        "stances to run at any constant speed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Appendix - Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following describes the code used to preprocess generate",
        "d images for keras modeling.  As with the [data generation](",
        "#Data-Generation) documentation, not all code will be covere",
        "d."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Image preprocessing and keras modeling took place on an [EC2",
        " P2](https://aws.amazon.com/ec2/instance-types/p2/) instance",
        "."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up AWS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialize an instance running the [Deep Learning Base AMI (",
        "Ubuntu)](https://aws.amazon.com/marketplace/pp/B077GCZ4GR). ",
        " I used Ubuntu 16.04 LTS and Python 3 for my data processing",
        ".  All required packages are listed in the [ubuntu-packages.",
        "txt](ubuntu-packages.txt) and [requirements.txt](requirement",
        "s.txt) files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To get ready for data processing, clone this repository onto",
        " your instance and execute the following commands from the c",
        "loned directory.  \n",
        "  \n",
        "`$ sudo add-apt-repository ppa:dolphin-emu/ppa -y`  \n",
        "`$ sudo apt update`  \n",
        "`$ xargs -a ubuntu-packages.txt sudo apt install -y`  \n",
        "`$ pip3 install -r requirements.txt --user`  \n",
        "`$ touch scripts/record_avi/Super_Smash_Bros._Melee_\\(v1.02\\",
        ").iso`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I wish I could provide a docker instance for the complete se",
        "tup, however the final setup must have access to the machine",
        " GPU and I have been unable to find a GPU-capable deep learn",
        "ing docker setup that does not require more effort than the ",
        "simple commands above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that you must create a fake ISO with `touch`.  The Make",
        "file rules expect a file with this name, however as long as ",
        "you are not doing any video recording you don’t need an actu",
        "al copy of the real ISO."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split Data into Train/Test/Valid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the cloned repository directory, run `make dirs` to set",
        " up the necessary data directory structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you generated image and mask data according to [the docum",
        "entation](#Data-Generation) above, copy all images into the ",
        "`data/images` subfolder and copy all masks into the `data/ma",
        "sks` subfolder.  \n",
        "  \n",
        "If, instead of generating image and mask data, you would lik",
        "e to download what was used in this analysis, execute the fo",
        "llowing commands from the cloned repository directory.  \n",
        "  \n",
        "`$ curl -O https://s3.amazonaws.com/datasci-smash/jpg-data.t",
        "ar.gz`  \n",
        "`$ tar -xzf jpg-data.tar.gz`  \n",
        "`$ find data -name '*jpg' -execdir touch '{}' +`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that you must `touch` all the downloaded image and mask",
        " data to let Make know that the images are up-to-date and do",
        " not need to be refreshed by video recording."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, run `make keras` to create shuffled train/test/vali",
        "d image data in the `keras/train`, `keras/test`, and `keras/",
        "valid` folders.  \n",
        "  \n",
        "You may set the Make variables `KERAS_RAND_SEED`, `KERAS_TRA",
        "IN_PCT`, `KERAS_TEST_PCT`, and `KERAS_VALID_PCT` to control ",
        "the shuffling and splitting steps."
      ]
    }
  ],
  "metadata": {
    "git": {
      "suppress_outputs": true
    },
    "kernelspec": {
      "display_name": "datasci-smash",
      "language": "python",
      "name": "datasci-smash"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}